{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3a14c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os, errno\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Instseg_model import MultiLayerFastLocalGraphModelV2 as model1\n",
    "from dataset import pcloader\n",
    "from graph_generation import gen_multi_level_local_graph_v3\n",
    "\n",
    "from math import floor, ceil\n",
    "from scipy.stats import mode\n",
    "\n",
    "import open3d as o3d\n",
    "from plot_utils import add_staple_patch\n",
    "from utils import parse_labelfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c5d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UTILS\n",
    "\n",
    "## Parse txt files that list scan and label paths per row eg test.txt \n",
    "def parse_dataset_file(data_set_filepath):\n",
    "    \"\"\"\n",
    "    Looks at the given text file that list scan and label path per row eg test.txt\n",
    "    Returns:\n",
    "     - a list of dicts, where each contains the \"scan_path\", the \"label_path\" and the sample \"name\"\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = []\n",
    "    with open(data_set_filepath, 'r') as setfile:\n",
    "        lines = setfile.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().strip(\"[]\") # removing the [] bookending each line\n",
    "            scan, label = line.split(\",\")\n",
    "            scan_path = scan.strip().strip(\"'\")\n",
    "            label_path = label.strip().strip(\"'\")\n",
    "            name = label_path.split(\"/\")[-1].strip(\".txt\")\n",
    "            sample = {\"scan_path\": scan_path, \"label_path\": label_path, \"name\": name}\n",
    "            samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0908374",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION INFERENCE\n",
    "\n",
    "## Hight Level Config Settings\n",
    "\n",
    "graph_gen_kwargs = {\n",
    "\t'add_rnd3d': True,\n",
    "\t'base_voxel_size': 0.8,\n",
    "\t'downsample_method': 'random',\n",
    "\t'level_configs': [\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 64, 'radius': 0.4},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 0,\n",
    "\t\t 'graph_scale': 1},\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 192, 'radius': 1.2},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 1,\n",
    "\t\t 'graph_scale': 1}]\n",
    "}\n",
    "\n",
    "def configure_model(model_params_path, max_cls_classes=3, max_inst_classes=7, verbose=False):\n",
    "    a = time.time()\n",
    "    model = model1(num_classes=max_cls_classes, max_instance_no=max_inst_classes)\n",
    "    if os.path.isfile(model_params_path):\n",
    "        model.load_state_dict(torch.load(model_params_path))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"[ModelParamPathError] {model_params_path} does not exist\")\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), model_params_path)\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(f\"Model Setup Time (secs) : {b-a}\")\n",
    "    return model\n",
    "\n",
    "def classify_scan(model, scan_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns \n",
    "     - the x|y|z|cls|inst for all keypoints in the cloud as a Kx5 array\n",
    "     - and also as a dict\n",
    "    \"\"\"\n",
    "    a = time.time()\n",
    "    pointxyz, offset = pcloader(scan_path)\n",
    "    vertex_coord_list, keypoint_indices_list, edges_list = \\\n",
    "    gen_multi_level_local_graph_v3(pointxyz,0.6,graph_gen_kwargs['level_configs'])\n",
    "    last_layer_v = vertex_coord_list[-1]\n",
    "\n",
    "    ## conversions: type precision\n",
    "    vertex_coord_list = [p.astype(np.float32) for p in vertex_coord_list]\n",
    "    keypoint_indices_list = [e.astype(np.int32) for e in keypoint_indices_list]\n",
    "    edges_list = [e.astype(np.int32) for e in edges_list]\n",
    "\n",
    "    ## conversions: numpy array to tensor\n",
    "    vertex_coord_list = [torch.from_numpy(item) for item in vertex_coord_list]\n",
    "    keypoint_indices_list = [torch.from_numpy(item).long() for item in keypoint_indices_list]\n",
    "    edges_list = [torch.from_numpy(item).long() for item in edges_list]\n",
    "\n",
    "    ## Run graph through GNN model\n",
    "    batch = (vertex_coord_list, keypoint_indices_list, edges_list)\n",
    "    cls_seg, inst_seg = model(*batch)\n",
    "\n",
    "    ## Filter classification probabilities for the most probable\n",
    "    cls_preds = torch.argmax(cls_seg, dim=1)\n",
    "    inst_preds = torch.argmax(inst_seg, dim=1)\n",
    "    \n",
    "    ## expand the shape of the array\n",
    "    cls_preds = np.expand_dims(cls_preds, axis=1)\n",
    "    inst_preds = np.expand_dims(inst_preds, axis=1)\n",
    "\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(\"Scan Inference Time (secs): \", b-a)\n",
    "        print()\n",
    "    \n",
    "    return np.hstack((last_layer_v, cls_preds, inst_preds)),\\\n",
    "            {'vertices': last_layer_v, 'cls_preds': cls_preds, 'inst_preds': inst_preds}\n",
    "\n",
    "def filter_out_background(scan_data):\n",
    "    non_bg_idx = ~np.logical_or(scan_data[:, 3] == 0, scan_data[:, 4] == 0)\n",
    "    non_bg = scan_data[non_bg_idx]\n",
    "    return non_bg, non_bg_idx\n",
    "\n",
    "def count_cluster_by_instance_prediction(scan_data, threshold_factor=0.5):\n",
    "    \"\"\"\n",
    "    Returns a cluster_count ie the number of clusters on the cls field/col that has at least a threshold number of members\n",
    "    Also: a dict of intermediary/final counts, types and thresholds\n",
    "    \"\"\"\n",
    "    inst, inst_count = np.unique(scan_data[:,4], return_counts=True)\n",
    "    inst_count_threshold = threshold_factor * np.mean(inst_count)\n",
    "    reduced_idx = np.where(inst_count > inst_count_threshold)\n",
    "    cluster_count = np.sum(inst_count > inst_count_threshold)\n",
    "    reduced = inst[reduced_idx]\n",
    "    \n",
    "    return cluster_count, {'orig_insts': inst,\n",
    "                           'orig_inst_ct': inst_count,\n",
    "                           'inst_ct_thresh': inst_count_threshold,\n",
    "                           'reduced_insts': reduced}\n",
    "def draw_labels(welds, ax):\n",
    "    for weld in welds:\n",
    "        markers = add_staple_patch(ax ,weld['xloc'], weld['yloc'], weld[\"yaw\"], weld['cls'] )\n",
    "        \n",
    "def _plot(path, points, values, title=\"\", caption=\"\", overlay_points=None, labels_dict=None ):\n",
    "    \"\"\"\n",
    "    path - output path\n",
    "    points - matrix of x, y, etc cols\n",
    "    values - assigned value per point ie predicted/truth cls/inst\n",
    "    [optional] overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "    labels_dict - to overlay the staple patch\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    im = ax.scatter(points[:,0], points[:,1],s=0.25,c=values)\n",
    "    \n",
    "\n",
    "    if overlay_points:    \n",
    "        for pts in overlay_points:\n",
    "            _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=pts[\"ss\"], c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "    \n",
    "    if labels_dict:\n",
    "        draw_labels(labels_dict[\"welds\"], ax)\n",
    "            \n",
    "    ax.set_xlabel(\"x [mm]\")\n",
    "    ax.set_ylabel(\"y [mm]\")\n",
    "\n",
    "        \n",
    "    legend_ = ax.legend(*im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "    ax.add_artist(legend_)\n",
    "\n",
    "    ax.text(0.5, -0.5, caption, style='italic', \\\n",
    "        horizontalalignment='center', verticalalignment='top', transform=ax.transAxes)\n",
    "    axes=plt.gca()\n",
    "    axes.set_aspect(1)\n",
    "    if path:\n",
    "        plt.savefig(path, dpi=150)\n",
    "        plt.close()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show\n",
    "    \n",
    "def save_prediction_plots(non_bg_matrix, labels_dict=None, cls_tag=None, inst_tag=None, cls_col=3, inst_col=4, inst_seg_dir_path=\"./plots/inst/\", cls_seg_dir_path=\"./plots/cls/\", verbose=False):\n",
    "    \"\"\"\n",
    "    non_bg_matrix -- x|y|z|cls|inst\n",
    "    // tag -- eg A_xycls_eps0_45_50\n",
    "    tag -- eg A_xycls\n",
    "    \"\"\"\n",
    "    \n",
    "    ## [RED FLAG] - what if dir_path comes through as None\n",
    "    f_tag = \"[save_prediction_plots]\"\n",
    "    f_msg = []\n",
    "    \n",
    "    if not (cls_tag and inst_tag):\n",
    "        print(\"[ERROR] No specfied cls_tag or inst_tag args. Plots not generated\")\n",
    "    else:\n",
    "        if cls_tag:\n",
    "            cls_output_path = cls_seg_dir_path+cls_tag+\".png\" if cls_seg_dir_path else None\n",
    "            _plot(cls_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, cls_col], title=cls_tag, labels_dict=labels_dict)\n",
    "            f_msg.append(cls_output_path)\n",
    "\n",
    "        if inst_tag:\n",
    "            inst_output_path = inst_seg_dir_path+inst_tag+\".png\" if inst_seg_dir_path else None\n",
    "            _plot(inst_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, inst_col], title=inst_tag, labels_dict=labels_dict)\n",
    "            f_msg.append(inst_output_path)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "            \n",
    "\n",
    "\n",
    "###################################################\n",
    "###  EVENTUALLY BUT SKIPPING THIS FOR NOW #########\n",
    "## Need to record the model weights version\n",
    "## Need to record the number of instances identified, the instance_seg_loss and cls_loss\n",
    "def save_prediction_stats(model, scan_path, output_file):\n",
    "    \"\"\"\n",
    "    Intialize text file if non-existent with name | predictions_clusters | prediction cluster counts | cluster_count_threshold | reduced_clusters \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "###################################################\n",
    "\n",
    "def run_preclustering(model, scan_path, labels_dict=None, sample_tag=\"\", inst_seg_img_dir_path=\"./plots/inst_seg/\", cls_seg_img_dir_path=\"./plots/cls_seg/\"):\n",
    "    \"\"\"\n",
    "    Runs the scan through the model\n",
    "    Filters out the background predictions\n",
    "    Plots cls and inst predictions post filtering and saves plots to file\n",
    "    Returns the Nx5 data of non-background points --> x|y|z|cls|inst\n",
    "    \"\"\"\n",
    "    # KX5 array --> x|y|z|cls|inst\n",
    "    scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "    \n",
    "    # non bg NX5 array --> x|y|z|cls|inst\n",
    "    scan_data, _ = filter_out_background(scan_data)\n",
    "    \n",
    "    ## [RED FLAG] - No use of the cluster_count or data\n",
    "    _, cluster_data = count_cluster_by_instance_prediction(scan_data)\n",
    "    \n",
    "    ## [RED FLAG] what if dir_path specified as None. Need to catch this cas\n",
    "#     save_prediction_plots(scan_data, cls_tag=sample_tag, inst_tag=sample_tag, cls_seg_dir_path=cls_seg_img_dir_path, inst_seg_dir_path=inst_seg_img_dir_path, labels_dict=labels_dict)\n",
    "    return scan_data, {\"0_scan\": scan_path, \"0_tag\":sample_tag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4226e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLUSTERING AND CLEAN UP\n",
    "\n",
    "def scale_xyz(scan_data):\n",
    "    ## Scaling the numerical data columns ie x, y, z cols, then appending the orig cols for return\n",
    "    \n",
    "    data_numerical = scan_data[:, :3]\n",
    "    X = StandardScaler()\n",
    "    scaled_num = X.fit_transform(data_numerical)\n",
    "    return np.hstack((scaled_num, scan_data))\n",
    "\n",
    "\n",
    "def save_cluster_plots(non_bg_matrix, values, title=None, dir_path=\"./plots/\", verbose=False):\n",
    "    \"\"\"\n",
    "   ### non_bg_matrix -- scaled_x|scaled_y|scaled_z|x|y|z|cls|inst\n",
    "    non_bg_matrix -- x|y|z|cls|inst\n",
    "    values -- cluster id\n",
    "    tag -- eg A_x_eps0.05_minpts30\n",
    "    \"\"\"\n",
    "    f_tag = \"[save_cluster_plots]\"\n",
    "    f_msg = \"\"\n",
    "    \n",
    "    if not title:\n",
    "        print(\"[ERROR] No specfied title args. Plot not generated\")\n",
    "    else:\n",
    "        cls_output_path = dir_path+title+\".png\"\n",
    "        _plot(cls_output_path, non_bg_matrix[:, :2], values, title=title)\n",
    "        f_msg = cls_output_path\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "\n",
    "def processDBSCAN2(data, sample_tag, cols_tag, eps, min_samples, verbose=False):\n",
    "    \"\"\"\n",
    "    data  -- eg x|y\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "\n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "\n",
    "def run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    out, title, _ = processDBSCAN2(scan_data[:, :2], sample_tag, \"xy\", eps, min_samples, verbose=False)\n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))\n",
    "\n",
    "\n",
    "def postcluster_grouping(scan_data, clusters=None):\n",
    "    \n",
    "    meta = {}\n",
    "    \n",
    "    shortlist_cluster_idx = clusters[\"cluster_ids\"] if clusters else None\n",
    "    if shortlist_cluster_idx:\n",
    "        meta[\"2_count_pre_reduction\"] = sum(clusters[\"cluster_counts\"])\n",
    "    \n",
    "    else:\n",
    "        ## Cluster Size Check: Setting cluster count bounds to weed out noise cluster and merged clusters\n",
    "        expected_cluster_count, _ = count_cluster_by_instance_prediction(scan_data)\n",
    "        expected_mean = scan_data.shape[0]/expected_cluster_count\n",
    "        count_range = (0.5*expected_mean, 1.5*expected_mean)\n",
    "        meta[\"1_orig_nonbg_count\"] = scan_data.shape[0]\n",
    "        meta[\"4_clustering_size_range\"] = count_range\n",
    "\n",
    "        ## Cluster Size Check: Removing the noise and merged clusters\n",
    "        clusters, cluster_counts = np.unique(scan_data[:, -1], return_counts=True)\n",
    "        filtered_clusters = [] \n",
    "        for c_id, c_count in zip(clusters, cluster_counts):\n",
    "            if c_id != -1:\n",
    "                if c_count >= count_range[0] and c_count <= count_range[1]:\n",
    "                    filtered_clusters.append((int(c_id), int(c_count)))\n",
    "\n",
    "        ## [ERROR] should be storing a copy\n",
    "        meta[\"5_filtered_cluster_presort\"] = filtered_clusters\n",
    "\n",
    "        ## Cluster Size Check: Sorting into descending order of cluster size\n",
    "        filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    "\n",
    "        ## [ERROR] should be storing a copy\n",
    "        meta[\"6_filtered_cluster_sorted\"] = filtered_clusters\n",
    "        meta[\"2_filtered_by_size_count\"] = sum([c_count for c_id, c_count in filtered_clusters])\n",
    "\n",
    "        ## Cluster Size Check: Returning only expected_cluster_count or less number of clusters\n",
    "        ## Effectively relying on GNN instance predictions for number of welds detected, at least\n",
    "        if len(filtered_clusters) > expected_cluster_count:\n",
    "            filtered_clusters = filtered_clusters[:expected_cluster_count]\n",
    "        meta[\"7_filtered_cluster_final\"] = filtered_clusters    \n",
    "\n",
    "        shortlist_cluster_idx = [c_id for c_id, c_count in filtered_clusters]\n",
    "    \n",
    "    ## Reconsituting scan_data without noise, merged or extra clusters; possible size reduction\n",
    "    scan_data_idx = np.isin(scan_data[:,-1], shortlist_cluster_idx)\n",
    "    scan_data = scan_data[scan_data_idx]\n",
    "    meta[\"3_final_count\"] = scan_data.shape[0]\n",
    "    \n",
    "    return scan_data, meta\n",
    "\n",
    "def postcluster_voting(scan_data):\n",
    "\n",
    "    ## remove the -1 clusters\n",
    "    scan_data = scan_data[scan_data[:, -1] >= 0]\n",
    "    \n",
    "    ## create new voting cols: cls and inst\n",
    "    n_rows, _ = scan_data.shape\n",
    "    scan_data = np.hstack((scan_data, np.zeros((n_rows, 1)), np.zeros((n_rows, 1)))) \n",
    "\n",
    "    ## expecting col5 to be the clustering result\n",
    "    clusters = np.unique(scan_data[:, 5])\n",
    "    for c_id in clusters:\n",
    "        scan_data_subset_idx = np.where(scan_data[:, 5] == c_id)\n",
    "        \n",
    "        ## majority votes\n",
    "        cls_vote = mode(np.ravel(scan_data[scan_data_subset_idx,3]), keepdims=False)[0]\n",
    "        inst_vote = mode(np.ravel(scan_data[scan_data_subset_idx,4]), keepdims=False)[0]\n",
    "        \n",
    "        ## push to data matrix\n",
    "        scan_data[scan_data_subset_idx, 6] = cls_vote\n",
    "        scan_data[scan_data_subset_idx, 7] = inst_vote\n",
    "\n",
    "    return scan_data\n",
    "\n",
    "def run_postclustering(scan_data, sample_tag=\"\", dir_path=\"./plots/\", cls_seg_dir_path=\"./plots/\",  inst_seg_dir_path=\"./plots/\", clusters=None ):\n",
    "    scan_data, meta1 = postcluster_grouping(scan_data, clusters=None) ## ncols remains 6, nrows possibly reduced\n",
    "    scan_data = postcluster_voting(scan_data)  ## n_cols increased from 6 to 8\n",
    "    save_prediction_plots(scan_data, cls_tag=f\"{sample_tag}_xycls_clean\", inst_tag=f\"{sample_tag}_xyinst_clean\", cls_col=6, inst_col=7, cls_seg_dir_path=cls_seg_dir_path, inst_seg_dir_path=inst_seg_dir_path)\n",
    "    return scan_data, meta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9bbf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Run clustering on xy(not scaled) data for each scan and plotting results\n",
    "\n",
    "# ## Model Instantiating\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)\n",
    "\n",
    "# ## Looping through the test samples to generate and save plots post DBSCAN cleanup\n",
    "# test_txt = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_data/test.txt\"\n",
    "# test_samples = parse_dataset_file(test_txt)\n",
    "\n",
    "# clustering_img_dir = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_img_dbscan/test_set/\"\n",
    "# trash_dir =  \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_img_dbscan/test_set/trash/\"\n",
    "# inst_seg_img_directory = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_img_instance_seg/test_set/test/\"\n",
    "# cls_seg_img_directory = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_img_class_seg/test_set/test/\"\n",
    "\n",
    "# for sample in test_samples[:4]:\n",
    "#     scan_path = sample[\"scan_path\"]\n",
    "#     sample_name = sample[\"name\"]\n",
    "#     scan_data, _ = run_preclustering(model, scan_path,sample_tag=\"testingDBSCAN\", inst_seg_img_dir_path=trash_dir, cls_seg_img_dir_path=trash_dir  )\n",
    "\n",
    "# #     scan_data = scale_xyz(scan_data)\n",
    "#     scan_data = run_DBSCAN_clustering_2D(scan_data, 2, 20, sample_tag=f\"{sample_name}_\", dir_path=clustering_img_dir)\n",
    "#     scan_data, meta1 = run_postclustering(scan_data, sample_tag=sample_name, dir_path=clustering_img_dir, inst_seg_dir_path=inst_seg_img_directory, cls_seg_dir_path=cls_seg_img_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd77bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extending IPYNB script to generate weld instances and labels\n",
    "## naming convention: WELD_1.npy, WELD_1.txt\n",
    "\n",
    "locationing_dataset_dir = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/locationing_dataset/\" \n",
    "label_subdir = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/locationing_dataset/labels/\" \n",
    "pc_subdir = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/locationing_dataset/point_clouds/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdef951",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WELD EXTRACTION AND FILE GENERATION UTILS\n",
    "## IE DATASET GENERATION\n",
    "\n",
    "def extract_welds_from_cleaned_segmentation(scan_data, inst_col=7):\n",
    "    \"\"\"\n",
    "    returns a list of welds' xy point array \n",
    "    \"\"\"\n",
    "    welds=[]\n",
    "    \n",
    "    ## looking at the inst col for grouping\n",
    "    point_inst = scan_data[:, inst_col]    \n",
    "    insts, insts_counts = np.unique(point_inst, return_counts=True)\n",
    "\n",
    "    ## collect groupings into a list of xy points\n",
    "    for inst in insts:\n",
    "        weld_xy = scan_data[point_inst==inst, :2]\n",
    "        welds.append(weld_xy)\n",
    "        \n",
    "    return welds\n",
    "\n",
    "def extract_labels_from_labelfile(labelfile_path):\n",
    "    \"\"\"\n",
    "    returns a list of (x) point array \n",
    "    \"\"\"\n",
    "    label_dict = parse_labelfile(labelfile_path)\n",
    "    welds = [(weld[\"xloc\"], weld[\"yloc\"], weld[\"cls\"]) for weld in label_dict[\"welds\"]]\n",
    "    return welds\n",
    "\n",
    "def match_welds_and_centers(welds, labels):\n",
    "    \n",
    "    '''\n",
    "    returns a list of dicts each with \"points_xy\" and \"xy\" keys\n",
    "    '''\n",
    "    \n",
    "    pairings = []\n",
    "    assert(len(welds) <= len(labels))\n",
    "    \n",
    "    ## looping through each grouping of points\n",
    "    for weld in welds:\n",
    "        \n",
    "        ## checking grouping for the limits in x\n",
    "        ## labels for if within limits in x\n",
    "        xmin = np.min(weld[:, 0])\n",
    "        xmax = np.max(weld[:, 0])\n",
    "#         xy = []\n",
    "        \n",
    "        ## missing check for remaining \n",
    "        for idx, (x,y, cls) in enumerate(labels):\n",
    "            if xmin<=x<=xmax:\n",
    "                xy = labels.pop(idx)\n",
    "        \n",
    "        ## pairing points and location label\n",
    "        pairings.append({\"points_xy\": weld, \"xy\":xy, \"cls\": cls})\n",
    "        \n",
    "    return pairings\n",
    "\n",
    "def create_weld_and_label_files(points_xyinst, labels, filename_prefix=\"\", dir_path=\"./dataset/\", point_cloud_sub=\"point_clouds/\", labels_sub=\"labels/\"):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.mkdir(directory)\n",
    "        print(f\"created: {dir_path}\")\n",
    "        \n",
    "    weld_scan_path = f\"{dir_path}{point_cloud_sub}\"\n",
    "    if not os.path.isdir(weld_scan_path):\n",
    "        os.mkdir(weld_scan_path)\n",
    "        print(f\"created: {weld_scan_path}\")\n",
    "        \n",
    "    weld_label_path = f\"{dir_path}{labels_sub}\"\n",
    "    if not os.path.isdir(weld_label_path):\n",
    "        os.mkdir(weld_label_path)\n",
    "        print(f\"created: {weld_label_path}\")\n",
    "        \n",
    "    last_file_tag = 0\n",
    "    scan_filenames = os.listdir(weld_scan_path)\n",
    "    scan_filenames = [file.split(\".\")[0] for file in scan_filenames]\n",
    "    scan_suffixes = [file.split(\"_\")[-1] for file in scan_filenames if file.startswith(filename_prefix)]\n",
    "#     print(\"scan_suffixes\")\n",
    "#     print(scan_suffixes)\n",
    "    \n",
    "    if len(scan_suffixes) > 0:\n",
    "        scan_suffixes = [int(suffix) for suffix in scan_suffixes]\n",
    "        last_file_tag = max(scan_suffixes)\n",
    "#         print(\"last_file_tag\")\n",
    "#         print(last_file_tag)\n",
    "     \n",
    "    welds = extract_welds_from_cleaned_segmentation(points_xyinst)\n",
    "#     print(\"welds: \", len(welds) )\n",
    "#     print(\"labels: \", labels )\n",
    "    \n",
    "    welds_and_labels = match_welds_and_centers(welds, labels)\n",
    "    for w_l in welds_and_labels:\n",
    "        last_file_tag+=1\n",
    "        np.save(f\"{weld_scan_path}{filename_prefix}{last_file_tag}\", w_l[\"points_xy\"])\n",
    "        np.savetxt(f\"{weld_label_path}{filename_prefix}{last_file_tag}.txt\", np.array(w_l[\"xy\"]).reshape((1,3)),\"%1.2f %1.2f %1d\" )\n",
    "#         print(f\"created scan: {weld_scan_path}{filename_prefix}{last_file_tag}\")\n",
    "#         print(f\"created label: {weld_label_path}{filename_prefix}{last_file_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58b7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runItemizing(model_params_path, dataset_txt, clustering_img_dir, trash_dir, inst_seg_img_directory, cls_seg_img_directory):\n",
    "\n",
    "    ## Model Instantiating\n",
    "    model = configure_model(model_params_path, verbose=True)\n",
    "\n",
    "    ## Looping through the samples list to generate and save plots\n",
    "    samples = parse_dataset_file(dataset_txt)\n",
    "    print(\"# samples in set: \", len(samples))\n",
    "\n",
    "    for idx, sample in enumerate(samples):\n",
    "        scan_path = sample[\"scan_path\"]\n",
    "        label_path = sample[\"label_path\"]\n",
    "        sample_name = sample[\"name\"]\n",
    "\n",
    "        scan_data, _ = run_preclustering(model, scan_path, sample_tag=\"testingDBSCAN\", inst_seg_img_dir_path=trash_dir, cls_seg_img_dir_path=trash_dir  )\n",
    "        scan_data = run_DBSCAN_clustering_2D(scan_data, 2, 20, sample_tag=f\"{sample_name}_\", dir_path=clustering_img_dir)\n",
    "        scan_data, meta1 = run_postclustering(scan_data, sample_tag=sample_name, dir_path=clustering_img_dir, inst_seg_dir_path=inst_seg_img_directory, cls_seg_dir_path=cls_seg_img_directory)\n",
    "\n",
    "        print(idx, \"label_path\", label_path)\n",
    "        labels = extract_labels_from_labelfile(label_path)\n",
    "        create_weld_and_label_files(scan_data, labels, filename_prefix=f\"WELD_{sample_name}_\", dir_path=locationing_dataset_dir, point_cloud_sub=\"point_clouds/\", labels_sub=\"labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e3e1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## Itemizing Welds from the Training Set\n",
    "#################################################\n",
    "\n",
    "# ## settings\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# dataset_txt = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_data/train.txt\"\n",
    "\n",
    "# root_dir = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/\"\n",
    "# clustering_img_dir = root_dir+ \"_img_dbscan/train_set/\"\n",
    "# trash_dir =  root_dir+ \"/_img_dbscan/train_set/trash/\"\n",
    "# inst_seg_img_directory = root_dir+ \"_img_instance_seg/train_set/\"\n",
    "# cls_seg_img_directory = root_dir+ \"_img_class_seg/train_set/\"\n",
    "\n",
    "# locationing_dataset_dir = root_dir+\"locationing_dataset/train/\"\n",
    "# runItemizing(model_params_path, dataset_txt, clustering_img_dir, trash_dir, inst_seg_img_directory, cls_seg_img_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7caada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup Time (secs) : 0.07054829597473145\n",
      "# samples in set:  121\n",
      "Scan Inference Time (secs):  10.323866605758667\n",
      "\n",
      "0 label_path ./_data/labels/RH-5-231201627-Pass-2023_06_13-9-19-08-357.txt\n",
      "Scan Inference Time (secs):  10.128194332122803\n",
      "\n",
      "1 label_path ./_data/labels/LH-5-231201608-Pass-2023_06_12-10-56-33-911.txt\n",
      "Scan Inference Time (secs):  9.975475549697876\n",
      "\n",
      "2 label_path ./_data/labels/LH-5-231201605-Pass-2023_06_12-10-46-15-717.txt\n",
      "Scan Inference Time (secs):  9.687974691390991\n",
      "\n",
      "3 label_path ./_data/labels/LH-5-231201596-Pass-2023_06_12-9-22-54-270.txt\n",
      "Scan Inference Time (secs):  9.312412738800049\n",
      "\n",
      "4 label_path ./_data/labels/RH-10-231201591-Pass-2023_06_13-11-42-25-737.txt\n",
      "Scan Inference Time (secs):  10.10603952407837\n",
      "\n",
      "5 label_path ./_data/labels/RH-7-231201627-Fail-2023_06_13-9-19-17-681.txt\n",
      "Scan Inference Time (secs):  9.6724374294281\n",
      "\n",
      "6 label_path ./_data/labels/RH-9-231201590-Pass-2023_06_13-11-39-05-753.txt\n",
      "Scan Inference Time (secs):  9.764238357543945\n",
      "\n",
      "7 label_path ./_data/labels/RH-7-231201626-Fail-2023_06_13-9-13-27-075.txt\n",
      "Scan Inference Time (secs):  10.055442571640015\n",
      "\n",
      "8 label_path ./_data/labels/LH-3-231201605-Pass-2023_06_12-10-46-06-374.txt\n",
      "Scan Inference Time (secs):  9.329961061477661\n",
      "\n",
      "9 label_path ./_data/labels/LH-1--2312014-NoResult-l-2023_06_02-7-56-18-48.txt\n",
      "Scan Inference Time (secs):  10.299638271331787\n",
      "\n",
      "10 label_path ./_data/labels/RH-6-231201597-Pass-2023_06_13-12-22-18-478.txt\n",
      "Scan Inference Time (secs):  9.496577262878418\n",
      "\n",
      "11 label_path ./_data/labels/LH-2-231201596-Pass-2023_06_12-9-22-40-303.txt\n",
      "Scan Inference Time (secs):  10.027687549591064\n",
      "\n",
      "12 label_path ./_data/labels/LH-6-231201595-Pass-2023_06_12-9-21-22-515.txt\n",
      "Scan Inference Time (secs):  10.073584079742432\n",
      "\n",
      "13 label_path ./_data/labels/LH-7-231201595-Pass-2023_06_12-9-21-27-188.txt\n",
      "Scan Inference Time (secs):  10.773317098617554\n",
      "\n",
      "14 label_path ./_data/labels/RH-6-231201590-Pass-2023_06_13-11-38-41-053.txt\n",
      "Scan Inference Time (secs):  11.171236753463745\n",
      "\n",
      "15 label_path ./_data/labels/RH-8-231201598-Pass-2023_06_13-12-39-49-771.txt\n",
      "Scan Inference Time (secs):  10.11626386642456\n",
      "\n",
      "16 label_path ./_data/labels/LH-3-231201600-Pass-2023_06_12-9-38-37-588.txt\n",
      "Scan Inference Time (secs):  9.813693046569824\n",
      "\n",
      "17 label_path ./_data/labels/LH-4-231201595-Pass-2023_06_12-9-21-13-183.txt\n",
      "Scan Inference Time (secs):  10.623604774475098\n",
      "\n",
      "18 label_path ./_data/labels/LH-3-231201598-Pass-2023_06_12-9-30-26-590.txt\n",
      "Scan Inference Time (secs):  10.084757089614868\n",
      "\n",
      "19 label_path ./_data/labels/RH-8-231201592-Pass-2023_06_13-11-45-02-878.txt\n",
      "Scan Inference Time (secs):  9.979097604751587\n",
      "\n",
      "20 label_path ./_data/labels/LH-6-231201596-Pass-2023_06_12-9-22-58-919.txt\n",
      "Scan Inference Time (secs):  9.946147203445435\n",
      "\n",
      "21 label_path ./_data/labels/RH-3-231201626-Pass-2023_06_13-9-13-08-435.txt\n",
      "Scan Inference Time (secs):  9.972500324249268\n",
      "\n",
      "22 label_path ./_data/labels/LH-5-231201604-Pass-2023_06_12-10-01-59-188.txt\n",
      "Scan Inference Time (secs):  9.197142839431763\n",
      "\n",
      "23 label_path ./_data/labels/RH-10-231201626-Pass-2023_06_13-9-13-41-018.txt\n",
      "Scan Inference Time (secs):  9.058796882629395\n",
      "\n",
      "24 label_path ./_data/labels/RH-10-231201592-Pass-2023_06_13-11-45-19-145.txt\n",
      "Scan Inference Time (secs):  10.539861679077148\n",
      "\n",
      "25 label_path ./_data/labels/LH-1-231201598-Pass-2023_06_12-9-30-17-240.txt\n",
      "Scan Inference Time (secs):  9.681898355484009\n",
      "\n",
      "26 label_path ./_data/labels/LH-6-231201600-Pass-2023_06_12-9-38-51-617.txt\n",
      "Scan Inference Time (secs):  9.549375534057617\n",
      "\n",
      "27 label_path ./_data/labels/LH-3-231201597-Pass-2023_06_12-9-27-19-422.txt\n",
      "Scan Inference Time (secs):  9.560643911361694\n",
      "\n",
      "28 label_path ./_data/labels/RH-7-231201596-Fail-2023_06_13-12-19-11-036.txt\n",
      "Scan Inference Time (secs):  9.883094310760498\n",
      "\n",
      "29 label_path ./_data/labels/LH-6-231201598-Pass-2023_06_12-9-30-40-570.txt\n",
      "Scan Inference Time (secs):  9.902008771896362\n",
      "\n",
      "30 label_path ./_data/labels/RH-4-231201595-Pass-2023_06_13-12-14-53-295.txt\n",
      "Scan Inference Time (secs):  10.084307432174683\n",
      "\n",
      "31 label_path ./_data/labels/RH-2-231201591-Pass-2023_06_13-11-41-20-814.txt\n",
      "Scan Inference Time (secs):  9.846367359161377\n",
      "\n",
      "32 label_path ./_data/labels/RH-11-231201594-Pass-2023_06_13-12-08-56-692.txt\n",
      "Scan Inference Time (secs):  9.897791862487793\n",
      "\n",
      "33 label_path ./_data/labels/LH-4-231201597-Pass-2023_06_12-9-27-24-065.txt\n",
      "Scan Inference Time (secs):  9.844788074493408\n",
      "\n",
      "34 label_path ./_data/labels/LH-4-231201605-Pass-2023_06_12-10-46-11-096.txt\n",
      "Scan Inference Time (secs):  10.381158113479614\n",
      "\n",
      "35 label_path ./_data/labels/LH-3-231201599-Pass-2023_06_12-9-34-38-894.txt\n",
      "Scan Inference Time (secs):  10.566952228546143\n",
      "\n",
      "36 label_path ./_data/labels/LH-2-231201599-Pass-2023_06_12-9-34-34-210.txt\n",
      "Scan Inference Time (secs):  10.40673279762268\n",
      "\n",
      "37 label_path ./_data/labels/RH-3-231201595-Pass-2023_06_13-12-14-44-634.txt\n",
      "Scan Inference Time (secs):  10.091077089309692\n",
      "\n",
      "38 label_path ./_data/labels/RH-9-231201594-Fail-2023_06_13-12-08-40-959.txt\n",
      "Scan Inference Time (secs):  10.188334941864014\n",
      "\n",
      "39 label_path ./_data/labels/LH-3-231201596-Pass-2023_06_12-9-22-44-921.txt\n",
      "Scan Inference Time (secs):  10.052832126617432\n",
      "\n",
      "40 label_path ./_data/labels/RH-9-231201592-Fail-2023_06_13-11-45-10-478.txt\n",
      "Scan Inference Time (secs):  10.263283491134644\n",
      "\n",
      "41 label_path ./_data/labels/LH-9-231201605-Fail-2023_06_12-10-46-34-327.txt\n",
      "Scan Inference Time (secs):  10.036397695541382\n",
      "\n",
      "42 label_path ./_data/labels/LH-4-231201603-Pass-2023_06_12-9-58-00-685.txt\n",
      "Scan Inference Time (secs):  10.330565690994263\n",
      "\n",
      "43 label_path ./_data/labels/RH-9-231201588-Fail-2023_06_13-10-19-58-456.txt\n",
      "Scan Inference Time (secs):  11.533349752426147\n",
      "\n",
      "44 label_path ./_data/labels/LH-3-231201595-Pass-2023_06_12-9-21-08-519.txt\n",
      "Scan Inference Time (secs):  9.925302743911743\n",
      "\n",
      "45 label_path ./_data/labels/RH-10-231201596-Pass-2023_06_13-12-19-39-257.txt\n",
      "Scan Inference Time (secs):  10.002604007720947\n",
      "\n",
      "46 label_path ./_data/labels/RH-1-231201627-Pass-2023_06_13-9-18-49-630.txt\n",
      "Scan Inference Time (secs):  10.018248319625854\n",
      "\n",
      "47 label_path ./_data/labels/RH-7-231201590-Fail-2023_06_13-11-38-49-513.txt\n",
      "Scan Inference Time (secs):  10.609432220458984\n",
      "\n",
      "48 label_path ./_data/labels/RH-3-231201627-Pass-2023_06_13-9-18-59-073.txt\n",
      "Scan Inference Time (secs):  10.293573379516602\n",
      "\n",
      "49 label_path ./_data/labels/RH-5-231201595-Pass-2023_06_13-12-15-01-555.txt\n",
      "Scan Inference Time (secs):  11.798558473587036\n",
      "\n",
      "50 label_path ./_data/labels/RH-3-231201599-Pass-2023_06_13-12-43-30-651.txt\n",
      "Scan Inference Time (secs):  10.17324709892273\n",
      "\n",
      "51 label_path ./_data/labels/LH-8-231201603-Fail-2023_06_12-9-58-19-339.txt\n",
      "Scan Inference Time (secs):  10.544455289840698\n",
      "\n",
      "52 label_path ./_data/labels/LH-5-231201603-Fail-2023_06_12-9-58-05-361.txt\n",
      "Scan Inference Time (secs):  9.84865951538086\n",
      "\n",
      "53 label_path ./_data/labels/LH-6-231201602-Pass-2023_06_12-9-53-25-942.txt\n",
      "Scan Inference Time (secs):  10.69725513458252\n",
      "\n",
      "54 label_path ./_data/labels/RH-5-231201626-Pass-2023_06_13-9-13-17-760.txt\n",
      "Scan Inference Time (secs):  10.18821668624878\n",
      "\n",
      "55 label_path ./_data/labels/RH-9-231201596-Fail-2023_06_13-12-19-28-965.txt\n",
      "Scan Inference Time (secs):  10.72292447090149\n",
      "\n",
      "56 label_path ./_data/labels/LH-8-231201601-Pass-2023_06_12-9-43-14-007.txt\n",
      "Scan Inference Time (secs):  10.958828210830688\n",
      "\n",
      "57 label_path ./_data/labels/RH-1-231201596-Pass-2023_06_13-12-18-19-465.txt\n",
      "Scan Inference Time (secs):  11.113059997558594\n",
      "\n",
      "58 label_path ./_data/labels/LH-7-231201604-Pass-2023_06_12-10-02-08-485.txt\n",
      "Scan Inference Time (secs):  10.617967367172241\n",
      "\n",
      "59 label_path ./_data/labels/LH-2-231201595-Pass-2023_06_12-9-21-03-886.txt\n",
      "Scan Inference Time (secs):  11.695319890975952\n",
      "\n",
      "60 label_path ./_data/labels/RH-9-231201591-Pass-2023_06_09-10-51-11-577.txt\n",
      "Scan Inference Time (secs):  10.794097661972046\n",
      "\n",
      "61 label_path ./_data/labels/RH-5-231201585-Pass-2023_06_09-9-22-35-051.txt\n",
      "Scan Inference Time (secs):  12.403300046920776\n",
      "\n",
      "62 label_path ./_data/labels/RH-2-231201593-Pass-2023_06_09-10-54-31-162.txt\n",
      "Scan Inference Time (secs):  12.13508915901184\n",
      "\n",
      "63 label_path ./_data/labels/RH-1-231201588-Fail-2023_06_09-10-44-27-895.txt\n",
      "Scan Inference Time (secs):  10.118866443634033\n",
      "\n",
      "64 label_path ./_data/labels/RH-11-231201251-Pass-2023_06_09-9-15-24-431.txt\n",
      "Scan Inference Time (secs):  11.626881122589111\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 label_path ./_data/labels/RH-4-231201590-Fail-2023_06_09-10-48-34-421.txt\n",
      "Scan Inference Time (secs):  10.473971128463745\n",
      "\n",
      "66 label_path ./_data/labels/RH-8-231201585-Pass-2023_06_09-9-22-49-024.txt\n",
      "Scan Inference Time (secs):  10.86557126045227\n",
      "\n",
      "67 label_path ./_data/labels/RH-5-231201587-Fail-2023_06_09-10-42-44-797.txt\n",
      "Scan Inference Time (secs):  9.890031814575195\n",
      "\n",
      "68 label_path ./_data/labels/RH-6-231201625-Pass-2023_06_12-14-17-48-219.txt\n",
      "Scan Inference Time (secs):  9.78389024734497\n",
      "\n",
      "69 label_path ./_data/labels/RH-8-231201624-Pass-2023_06_12-14-14-41-990.txt\n",
      "Scan Inference Time (secs):  9.684354543685913\n",
      "\n",
      "70 label_path ./_data/labels/RH-10-231201625-Pass-2023_06_12-14-18-06-818.txt\n",
      "Scan Inference Time (secs):  9.738842010498047\n",
      "\n",
      "71 label_path ./_data/labels/RH-9-231201583-Fail-2023_06_09-9-17-11-772.txt\n",
      "Scan Inference Time (secs):  10.548301219940186\n",
      "\n",
      "72 label_path ./_data/labels/RH-6-231201250-Pass-2023_06_09-9-09-12-098.txt\n",
      "Scan Inference Time (secs):  9.803160190582275\n",
      "\n",
      "73 label_path ./_data/labels/RH-1-231201612-Pass-2023_06_12-11-45-07-296.txt\n",
      "Scan Inference Time (secs):  10.254488945007324\n",
      "\n",
      "74 label_path ./_data/labels/RH-5-231201584-Pass-2023_06_09-9-19-57-787.txt\n",
      "Scan Inference Time (secs):  9.825440883636475\n",
      "\n",
      "75 label_path ./_data/labels/RH-2-231201584-Pass-2023_06_09-9-19-43-815.txt\n",
      "Scan Inference Time (secs):  9.879463911056519\n",
      "\n",
      "76 label_path ./_data/labels/RH-9-231201251-Fail-2023_06_09-9-15-15-123.txt\n",
      "Scan Inference Time (secs):  12.204201459884644\n",
      "\n",
      "77 label_path ./_data/labels/LH-1-231201597-Fail-2023_06_13-12-21-34-579.txt\n",
      "Scan Inference Time (secs):  10.642369747161865\n",
      "\n",
      "78 label_path ./_data/labels/RH-7-231201616-Fail-2023_06_12-12-14-28-799.txt\n",
      "Scan Inference Time (secs):  11.602855682373047\n",
      "\n",
      "79 label_path ./_data/labels/RH-2-231201588-Pass-2023_06_09-10-44-32-644.txt\n",
      "Scan Inference Time (secs):  10.287773847579956\n",
      "\n",
      "80 label_path ./_data/labels/RH-4-231201613-Pass-2023_06_12-11-49-07-871.txt\n",
      "Scan Inference Time (secs):  10.272625207901001\n",
      "\n",
      "81 label_path ./_data/labels/RH-1-231201251-Pass-2023_06_09-9-14-37-782.txt\n",
      "Scan Inference Time (secs):  9.93037724494934\n",
      "\n",
      "82 label_path ./_data/labels/RH-3-231201584-Pass-2023_06_09-9-19-48-487.txt\n",
      "Scan Inference Time (secs):  10.217954158782959\n",
      "\n",
      "83 label_path ./_data/labels/RH-7-231201585-Fail-2023_06_09-9-22-44-389.txt\n",
      "Scan Inference Time (secs):  10.708479404449463\n",
      "\n",
      "84 label_path ./_data/labels/RH-8-231201592-Pass-2023_06_09-10-53-26-003.txt\n",
      "Scan Inference Time (secs):  9.313071489334106\n",
      "\n",
      "85 label_path ./_data/labels/RH-10-231201249-Pass-2023_06_09-8-04-18-518.txt\n",
      "Scan Inference Time (secs):  9.968442916870117\n",
      "\n",
      "86 label_path ./_data/labels/RH-3-231201250-Pass-2023_06_09-9-08-58-142.txt\n",
      "Scan Inference Time (secs):  10.931675434112549\n",
      "\n",
      "87 label_path ./_data/labels/RH-5-231201251-Pass-2023_06_09-9-14-56-512.txt\n",
      "Scan Inference Time (secs):  10.162243843078613\n",
      "\n",
      "88 label_path ./_data/labels/RH-7-231201623-Fail-2023_06_12-14-11-19-597.txt\n",
      "Scan Inference Time (secs):  8.927365064620972\n",
      "\n",
      "89 label_path ./_data/labels/RH-10-231201584-Pass-2023_06_09-9-20-21-084.txt\n",
      "Scan Inference Time (secs):  9.417199611663818\n",
      "\n",
      "90 label_path ./_data/labels/RH-3-231201613-Fail-2023_06_12-11-49-03-245.txt\n",
      "Scan Inference Time (secs):  10.07236909866333\n",
      "\n",
      "91 label_path ./_data/labels/RH-4-231201622-Pass-2023_06_12-14-07-08-024.txt\n",
      "Scan Inference Time (secs):  9.972094058990479\n",
      "\n",
      "92 label_path ./_data/labels/RH-5-231201622-Fail-2023_06_12-14-07-12-706.txt\n",
      "Scan Inference Time (secs):  9.628670692443848\n",
      "\n",
      "93 label_path ./_data/labels/RH-5-231201583-Pass-2023_06_09-9-16-53-153.txt\n",
      "Scan Inference Time (secs):  10.481619596481323\n",
      "\n",
      "94 label_path ./_data/labels/RH-6-231201624-Pass-2023_06_12-14-14-32-709.txt\n",
      "Scan Inference Time (secs):  11.61043095588684\n",
      "\n",
      "95 label_path ./_data/labels/LH-1-231201599-Fail-2023_06_13-12-43-13-933.txt\n",
      "Scan Inference Time (secs):  11.293543577194214\n",
      "\n",
      "96 label_path ./_data/labels/RH-1-231201619-Pass-2023_06_12-12-21-51-539.txt\n",
      "Scan Inference Time (secs):  11.050904512405396\n",
      "\n",
      "97 label_path ./_data/labels/RH-10-231201622-Pass-2023_06_12-14-07-35-975.txt\n",
      "Scan Inference Time (secs):  10.96199655532837\n",
      "\n",
      "98 label_path ./_data/labels/RH-9-231201621-Fail-2023_06_12-13-55-00-115.txt\n",
      "Scan Inference Time (secs):  10.387196063995361\n",
      "\n",
      "99 label_path ./_data/labels/RH-5-231201625-Pass-2023_06_12-14-17-43-530.txt\n",
      "Scan Inference Time (secs):  10.975446224212646\n",
      "\n",
      "100 label_path ./_data/labels/RH-11-231201593-Pass-2023_06_09-10-55-13-000.txt\n",
      "Scan Inference Time (secs):  13.004415273666382\n",
      "\n",
      "101 label_path ./_data/labels/LH-6-231201603-Fail-2023_06_13-13-30-45-710.txt\n",
      "Scan Inference Time (secs):  12.412911653518677\n",
      "\n",
      "102 label_path ./_data/labels/LH-1-231201607-Fail-2023_06_13-13-44-20-591.txt\n",
      "Scan Inference Time (secs):  10.784142255783081\n",
      "\n",
      "103 label_path ./_data/labels/LH-5-231201597-Fail-2023_06_13-12-22-08-360.txt\n",
      "Scan Inference Time (secs):  11.206111907958984\n",
      "\n",
      "104 label_path ./_data/labels/RH-1-231201594-Fail-2023_06_09-10-57-27-505.txt\n",
      "Scan Inference Time (secs):  10.720975160598755\n",
      "\n",
      "105 label_path ./_data/labels/LH-5-231201604-Fail-2023_06_13-13-34-23-901.txt\n",
      "Scan Inference Time (secs):  10.126788139343262\n",
      "\n",
      "106 label_path ./_data/labels/RH-9-231201612-Fail-2023_06_12-11-45-44-597.txt\n",
      "Scan Inference Time (secs):  9.687007188796997\n",
      "\n",
      "107 label_path ./_data/labels/RH-7-231201621-Fail-2023_06_12-13-54-50-814.txt\n",
      "Scan Inference Time (secs):  11.024928331375122\n",
      "\n",
      "108 label_path ./_data/labels/LH-10-231201595-Fail-2023_06_13-12-15-48-417.txt\n",
      "Scan Inference Time (secs):  10.333160877227783\n",
      "\n",
      "109 label_path ./_data/labels/RH-5-231201592-Fail-2023_06_09-10-53-12-049.txt\n",
      "Scan Inference Time (secs):  9.708873987197876\n",
      "\n",
      "110 label_path ./_data/labels/RH-8-231201587-Pass-2023_06_09-10-42-58-781.txt\n",
      "Scan Inference Time (secs):  9.811912059783936\n",
      "\n",
      "111 label_path ./_data/labels/RH-8-231201251-Pass-2023_06_09-9-15-10-481.txt\n",
      "Scan Inference Time (secs):  10.02832818031311\n",
      "\n",
      "112 label_path ./_data/labels/RH-3-231201621-Pass-2023_06_12-13-54-32-191.txt\n",
      "Scan Inference Time (secs):  10.496248483657837\n",
      "\n",
      "113 label_path ./_data/labels/LH-5-231201595-Fail-2023_06_13-12-15-01-556.txt\n",
      "Scan Inference Time (secs):  9.803092241287231\n",
      "\n",
      "114 label_path ./_data/labels/LH-5-231201607-Fail-2023_06_13-13-44-55-051.txt\n",
      "Scan Inference Time (secs):  10.219972133636475\n",
      "\n",
      "115 label_path ./_data/labels/RH-7-231201617-Fail-2023_06_12-12-18-26-677.txt\n",
      "Scan Inference Time (secs):  10.741333484649658\n",
      "\n",
      "116 label_path ./_data/labels/RH-5-231201589-Fail-2023_06_09-10-46-42-686.txt\n",
      "Scan Inference Time (secs):  9.74793553352356\n",
      "\n",
      "117 label_path ./_data/labels/LH-9-231201600-Fail-2023_06_13-13-12-23-510.txt\n",
      "Scan Inference Time (secs):  9.644004106521606\n",
      "\n",
      "118 label_path ./_data/labels/RH-4-231201251-Pass-2023_06_09-9-14-51-848.txt\n",
      "Scan Inference Time (secs):  10.648511171340942\n",
      "\n",
      "119 label_path ./_data/labels/LH-5-231201601-Fail-2023_06_13-13-16-14-093.txt\n",
      "Scan Inference Time (secs):  10.345823287963867\n",
      "\n",
      "120 label_path ./_data/labels/RH-9-231201584-Fail-2023_06_09-9-20-16-429.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################\n",
    "## Itemizing Welds from the Test Set\n",
    "#################################################\n",
    "\n",
    "## settings\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "dataset_txt = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_data/test.txt\"\n",
    "\n",
    "root_dir = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/\"\n",
    "clustering_img_dir = root_dir+ \"_img_dbscan/test_set/\"\n",
    "trash_dir =  root_dir+ \"/_img_dbscan/test_set/trash/\"\n",
    "inst_seg_img_directory = root_dir+ \"_img_instance_seg/test_set/\"\n",
    "cls_seg_img_directory = root_dir+ \"_img_class_seg/test_set/\"\n",
    "\n",
    "locationing_dataset_dir = root_dir+\"locationing_dataset/test/\" \n",
    "runItemizing(model_params_path, dataset_txt, clustering_img_dir, trash_dir, inst_seg_img_directory, cls_seg_img_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012d5453",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'repeat' from 'throwaway' (C:\\Users\\KZTYLF\\Documents\\playground\\GNN UIs\\GNN InstanceSegmentation\\throwaway.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthrowaway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m repeat, repeat_print\n\u001b[0;32m      3\u001b[0m repeat()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'repeat' from 'throwaway' (C:\\Users\\KZTYLF\\Documents\\playground\\GNN UIs\\GNN InstanceSegmentation\\throwaway.py)"
     ]
    }
   ],
   "source": [
    "from throwaway import repeat, repeat_print\n",
    "\n",
    "repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f4e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
