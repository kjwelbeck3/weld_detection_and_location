{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3df53167",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Instseg_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mInstseg_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLayerFastLocalGraphModelV2 \u001b[38;5;28;01mas\u001b[39;00m model1\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pcloader\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraph_generation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_multi_level_local_graph_v3\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Instseg_model'"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os, errno\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Instseg_model import MultiLayerFastLocalGraphModelV2 as model1\n",
    "from dataset import pcloader\n",
    "from graph_generation import gen_multi_level_local_graph_v3\n",
    "\n",
    "from math import floor, ceil\n",
    "from scipy.stats import mode\n",
    "\n",
    "import open3d as o3d\n",
    "from plot_utils import add_staple_patch\n",
    "from utils import parse_labelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99029836",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UTILS\n",
    "\n",
    "## Parse txt files that list scan and label paths per row eg test.txt \n",
    "def parse_dataset_file(data_set_filepath):\n",
    "    \"\"\"\n",
    "    Looks at the given text file that list scan and label path per row eg test.txt\n",
    "    Returns:\n",
    "     - a list of dicts, where each contains the \"scan_path\", the \"label_path\" and the sample \"name\"\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = []\n",
    "    with open(data_set_filepath, 'r') as setfile:\n",
    "        lines = setfile.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().strip(\"[]\") # removing the [] bookending each line\n",
    "            scan, label = line.split(\",\")\n",
    "            scan_path = scan.strip().strip(\"'\")\n",
    "            label_path = label.strip().strip(\"'\")\n",
    "            name = label_path.split(\"/\")[-1].strip(\".txt\")\n",
    "            sample = {\"scan_path\": scan_path, \"label_path\": label_path, \"name\": name}\n",
    "            samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "def _plot(points, values, out_path=None, title=\"\", caption=\"\", overlay_points=None, labels_dict=None ):\n",
    "    \"\"\"\n",
    "    path - output path\n",
    "    points - matrix of x, y, etc cols\n",
    "    values - assigned value per point ie predicted/truth cls/inst\n",
    "    [optional] overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "    labels_dict - to overlay the staple patch\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    im = ax.scatter(points[:,0], points[:,1],s=0.25,c=values)\n",
    "    \n",
    "\n",
    "    if overlay_points:    \n",
    "        for pts in overlay_points:\n",
    "            _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=pts[\"ss\"], c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "    \n",
    "    if labels_dict:\n",
    "        draw_labels(labels_dict[\"welds\"], ax)\n",
    "            \n",
    "    ax.set_xlabel(\"x [mm]\")\n",
    "    ax.set_ylabel(\"y [mm]\")\n",
    "\n",
    "        \n",
    "    legend_ = ax.legend(*im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "    ax.add_artist(legend_)\n",
    "\n",
    "    ax.text(0.5, -0.5, caption, style='italic', \\\n",
    "        horizontalalignment='center', verticalalignment='top', transform=ax.transAxes)\n",
    "    axes=plt.gca()\n",
    "    axes.set_aspect(1)\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.close()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEGMENTATION INFERENCE\n",
    "## Run Segmentation\n",
    "\n",
    "## High Level Graph Generation Config Settings\n",
    "graph_gen_kwargs = {\n",
    "\t'add_rnd3d': True,\n",
    "\t'base_voxel_size': 0.8,\n",
    "\t'downsample_method': 'random',\n",
    "\t'level_configs': [\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 64, 'radius': 0.4},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 0,\n",
    "\t\t 'graph_scale': 1},\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 192, 'radius': 1.2},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 1,\n",
    "\t\t 'graph_scale': 1}]\n",
    "}\n",
    "\n",
    "\n",
    "def configure_model(model_params_path, max_cls_classes=3, max_inst_classes=7, verbose=False):\n",
    "    a = time.time()\n",
    "    model = model1(num_classes=max_cls_classes, max_instance_no=max_inst_classes)\n",
    "    if os.path.isfile(model_params_path):\n",
    "        model.load_state_dict(torch.load(model_params_path))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"[ModelParamPathError] {model_params_path} does not exist\")\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), model_params_path)\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(f\"Model Setup Time (secs) : {b-a}\")\n",
    "    return model\n",
    "\n",
    "def classify_scan(model, scan_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns \n",
    "     - the x|y|z|cls|inst for all keypoints in the cloud as a Kx5 array\n",
    "     - and also as a dict\n",
    "    \"\"\"\n",
    "    a = time.time()\n",
    "    pointxyz, offset = pcloader(scan_path)\n",
    "    vertex_coord_list, keypoint_indices_list, edges_list = \\\n",
    "    gen_multi_level_local_graph_v3(pointxyz,0.6,graph_gen_kwargs['level_configs'])\n",
    "    last_layer_v = vertex_coord_list[-1]\n",
    "\n",
    "    ## conversions: type precision\n",
    "    vertex_coord_list = [p.astype(np.float32) for p in vertex_coord_list]\n",
    "    keypoint_indices_list = [e.astype(np.int32) for e in keypoint_indices_list]\n",
    "    edges_list = [e.astype(np.int32) for e in edges_list]\n",
    "\n",
    "    ## conversions: numpy array to tensor\n",
    "    vertex_coord_list = [torch.from_numpy(item) for item in vertex_coord_list]\n",
    "    keypoint_indices_list = [torch.from_numpy(item).long() for item in keypoint_indices_list]\n",
    "    edges_list = [torch.from_numpy(item).long() for item in edges_list]\n",
    "\n",
    "    ## Run graph through GNN model\n",
    "    batch = (vertex_coord_list, keypoint_indices_list, edges_list)\n",
    "    cls_seg, inst_seg = model(*batch)\n",
    "\n",
    "    ## Filter classification probabilities for the most probable\n",
    "    cls_preds = torch.argmax(cls_seg, dim=1)\n",
    "    inst_preds = torch.argmax(inst_seg, dim=1)\n",
    "    \n",
    "    ## expand the shape of the array\n",
    "    cls_preds = np.expand_dims(cls_preds, axis=1)\n",
    "    inst_preds = np.expand_dims(inst_preds, axis=1)\n",
    "\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(\"Scan Inference Time (secs): \", b-a)\n",
    "        print()\n",
    "    \n",
    "    return np.hstack((last_layer_v, cls_preds, inst_preds)),\\\n",
    "            {'vertices': last_layer_v, 'cls_preds': cls_preds, 'inst_preds': inst_preds}\n",
    "\n",
    "def filter_out_background(scan_data):\n",
    "    non_bg_idx = ~np.logical_or(scan_data[:, 3] == 0, scan_data[:, 4] == 0)\n",
    "    non_bg = scan_data[non_bg_idx]\n",
    "    return non_bg, non_bg_idx\n",
    "\n",
    "# def count_cluster_by_instance_prediction(scan_data, threshold_factor=0.5):\n",
    "#     \"\"\"\n",
    "#     Returns a cluster_count ie the number of clusters on the cls field/col that has at least a threshold number of members\n",
    "#     Also: a dict of intermediary/final counts, types and thresholds\n",
    "#     \"\"\"\n",
    "#     inst, inst_count = np.unique(scan_data[:,4], return_counts=True)\n",
    "#     inst_count_threshold = threshold_factor * np.mean(inst_count)\n",
    "#     reduced_idx = np.where(inst_count > inst_count_threshold)\n",
    "#     cluster_count = np.sum(inst_count > inst_count_threshold)\n",
    "#     reduced = inst[reduced_idx]\n",
    "    \n",
    "#     return cluster_count, {'orig_insts': inst,\n",
    "#                            'orig_inst_ct': inst_count,\n",
    "#                            'inst_ct_thresh': inst_count_threshold,\n",
    "#                            'reduced_insts': reduced}\n",
    "# def draw_labels(welds, ax):\n",
    "#     for weld in welds:\n",
    "#         markers = add_staple_patch(ax ,weld['xloc'], weld['yloc'], weld[\"yaw\"], weld['cls'] )\n",
    "        \n",
    "# def _plot(path, points, values, title=\"\", caption=\"\", overlay_points=None, labels_dict=None ):\n",
    "#     \"\"\"\n",
    "#     path - output path\n",
    "#     points - matrix of x, y, etc cols\n",
    "#     values - assigned value per point ie predicted/truth cls/inst\n",
    "#     [optional] overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "#     labels_dict - to overlay the staple patch\n",
    "#     \"\"\"\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.set_title(title)\n",
    "#     im = ax.scatter(points[:,0], points[:,1],s=0.25,c=values)\n",
    "    \n",
    "\n",
    "#     if overlay_points:    \n",
    "#         for pts in overlay_points:\n",
    "#             _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=pts[\"ss\"], c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "    \n",
    "#     if labels_dict:\n",
    "#         draw_labels(labels_dict[\"welds\"], ax)\n",
    "            \n",
    "#     ax.set_xlabel(\"x [mm]\")\n",
    "#     ax.set_ylabel(\"y [mm]\")\n",
    "\n",
    "        \n",
    "#     legend_ = ax.legend(*im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "#     ax.add_artist(legend_)\n",
    "\n",
    "#     ax.text(0.5, -0.5, caption, style='italic', \\\n",
    "#         horizontalalignment='center', verticalalignment='top', transform=ax.transAxes)\n",
    "#     axes=plt.gca()\n",
    "#     axes.set_aspect(1)\n",
    "#     if path:\n",
    "#         plt.savefig(path, dpi=150)\n",
    "#         plt.close()\n",
    "#         plt.cla()\n",
    "#         plt.clf()\n",
    "#     else:\n",
    "#         plt.show\n",
    "    \n",
    "# def save_prediction_plots(non_bg_matrix, labels_dict=None, cls_tag=None, inst_tag=None, cls_col=3, inst_col=4, inst_seg_dir_path=\"./plots/inst/\", cls_seg_dir_path=\"./plots/cls/\", verbose=False):\n",
    "#     \"\"\"\n",
    "#     non_bg_matrix -- x|y|z|cls|inst\n",
    "#     // tag -- eg A_xycls_eps0_45_50\n",
    "#     tag -- eg A_xycls\n",
    "#     \"\"\"\n",
    "    \n",
    "#     ## [RED FLAG] - what if dir_path comes through as None\n",
    "#     f_tag = \"[save_prediction_plots]\"\n",
    "#     f_msg = []\n",
    "    \n",
    "#     if not (cls_tag and inst_tag):\n",
    "#         print(\"[ERROR] No specfied cls_tag or inst_tag args. Plots not generated\")\n",
    "#     else:\n",
    "#         if cls_tag:\n",
    "#             cls_output_path = cls_seg_dir_path+cls_tag+\".png\" if cls_seg_dir_path else None\n",
    "#             _plot(cls_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, cls_col], title=cls_tag, labels_dict=labels_dict)\n",
    "#             f_msg.append(cls_output_path)\n",
    "\n",
    "#         if inst_tag:\n",
    "#             inst_output_path = inst_seg_dir_path+inst_tag+\".png\" if inst_seg_dir_path else None\n",
    "#             _plot(inst_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, inst_col], title=inst_tag, labels_dict=labels_dict)\n",
    "#             f_msg.append(inst_output_path)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "            \n",
    "\n",
    "\n",
    "# ###################################################\n",
    "# ###  EVENTUALLY BUT SKIPPING THIS FOR NOW #########\n",
    "# ## Need to record the model weights version\n",
    "# ## Need to record the number of instances identified, the instance_seg_loss and cls_loss\n",
    "# def save_prediction_stats(model, scan_path, output_file):\n",
    "#     \"\"\"\n",
    "#     Intialize text file if non-existent with name | predictions_clusters | prediction cluster counts | cluster_count_threshold | reduced_clusters \n",
    "#     \"\"\"\n",
    "#     pass\n",
    "\n",
    "# ###################################################\n",
    "\n",
    "def run_preclustering(model, scan_path, labels_dict=None, sample_tag=\"\", inst_seg_img_dir_path=\"./plots/inst_seg/\", cls_seg_img_dir_path=\"./plots/cls_seg/\"):\n",
    "    \"\"\"\n",
    "    Runs the scan through the model\n",
    "    Filters out the background predictions\n",
    "    [OMITTED - Plots cls and inst predictions post filtering and saves plots to file]\n",
    "    Returns the Nx5 data of non-background points --> x|y|z|cls|inst\n",
    "    \"\"\"\n",
    "    # KX5 array --> x|y|z|cls|inst\n",
    "    scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "    \n",
    "    # non bg NX5 array --> x|y|z|cls|inst\n",
    "    scan_data, _ = filter_out_background(scan_data)\n",
    "    \n",
    "    return scan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLUSTERING INSTANCE SEGMENTATION RESULTS INTO INDIVIDUAL WELDS\n",
    "\n",
    "## dbscan params\n",
    "eps = 0\n",
    "min_samples= 0\n",
    "\n",
    "## \n",
    "def partial_isolate_and_cluster(scan_data, inst_seg_col, inst_seg_value, dbscan_model, view_per_segment=False):\n",
    "    partial = scan_data[scan_data[:, inst_seg_col] == inst_seg_value]\n",
    "    DBSCAN_result = dbscan_model.fit_predict(partial[:, :2])\n",
    "    \n",
    "    ## Visualizing raw dbscan result\n",
    "    if view_per_segment:\n",
    "        _plot(partial, DBSCAN_result, caption=f\"Raw DBSCAN clusters for inst_segment={inst_seg_value}\")\n",
    "\n",
    "    ## Clean up routine, then visualizing\n",
    "    # Get only those in the largest cluster\n",
    "    cluster_vals, cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "    idx = np.argmax(cluster_counts)\n",
    "    val = cluster_vals[idx]\n",
    "#     print(\"cluster_vals\", cluster_vals)\n",
    "#     print(\"cluster_counts\", cluster_counts)\n",
    "#     print(\"idx:\", idx)\n",
    "    partial = partial[DBSCAN_result == val]\n",
    "    values = DBSCAN_result[DBSCAN_result == val]\n",
    "    \n",
    "    if view_per_segment:\n",
    "        _plot(partial, values, caption=f\"Filtered DBSCAN clusters for inst_segment={inst_seg_value}\")\n",
    "    \n",
    "    return partial\n",
    "    \n",
    "    \n",
    "    \n",
    "def full_isolate_and_cluster(scan_data, tag, inst_seg_col=4, eps=2, min_samples=20, view_per_segment=False, view_full=False, out_dir=None):\n",
    "    \n",
    "    ## permutations on the min_samples and \n",
    "    if not isinstance(eps, (tuple, list)):\n",
    "        eps = (eps,)\n",
    "        \n",
    "    if not isinstance(min_samples, (tuple, list)):\n",
    "        min_samples = (min_samples,)\n",
    "    \n",
    "    combos = []\n",
    "    \n",
    "    for i in eps:\n",
    "        for j in min_samples:\n",
    "            combos.append((i, j))\n",
    "    print(combos)\n",
    "    \n",
    "    for eps, min_samples in combos:\n",
    "        ## Construct the DBSCAN model\n",
    "        DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "        ## Loop through all instance\n",
    "        insts = np.unique(scan_data[:, inst_seg_col])\n",
    "        partials = []\n",
    "\n",
    "        for inst_seg_value in insts:\n",
    "            print(\"inst_seg_value\", inst_seg_value)\n",
    "            partial = partial_isolate_and_cluster(scan_data, inst_seg_col, inst_seg_value, DBSCAN_model, view_per_segment=view_per_segment)\n",
    "            partials.append(partial)\n",
    "\n",
    "        ## Aggregating all partials\n",
    "        full = np.vstack(partials)\n",
    "\n",
    "        if view_full:\n",
    "            if out_dir:\n",
    "                _plot(scan_data, scan_data[:, inst_seg_col], caption=f\"PreDBSCAN\", out_path=out_dir+tag+\"_rawSeg.png\")\n",
    "                _plot(full, full[:, inst_seg_col], caption=f\"Filtered FULL DBSCAN Result_eps{eps}_min{min_samples}\", out_path=out_dir+tag+f\"_eps{eps}_minsamples{min_samples}.png\",)\n",
    "            else:\n",
    "                _plot(scan_data, scan_data[:, inst_seg_col], caption=f\"PreDBSCAN\")\n",
    "                _plot(full, full[:, inst_seg_col], caption=f\"Filtered FULL DBSCAN Result_eps{eps}_min{min_samples}\")\n",
    "\n",
    "    return full\n",
    "\n",
    "\n",
    "## RECOMBINING INTO A POST-CLUSTERING AGGREGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca121b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Testing New DBSCAN sequence\n",
    "\n",
    "##########\n",
    "## TestSet\n",
    "##########\n",
    "\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "dataset_txt = \"./Recreating Dataset/_data/test.txt\"\n",
    "img_dir = \"./Recreating Dataset/_img_dbscan_on_insts/test/\"\n",
    "trash_dir = \"./Recreating Dataset/_img_dbscan_on_insts/trash/\"\n",
    "eps = (2,3,4,5)\n",
    "min_samples = (20, 30, 40, 50)\n",
    "\n",
    "# ##########\n",
    "# ## TrainSet\n",
    "# ##########\n",
    "# dataset_txt = \"./Recreating Dataset/_data/train.txt\"\n",
    "# img_dir = \"./Recreating Dataset/_img_dbscan_on_insts/train/\"\n",
    "\n",
    "\n",
    "def test_DBSCAN_onInstSegResults(model_params_path, dataset_txt, out_dir, trash_dir, eps=2, min_samples=20):\n",
    "    \n",
    "    ## Instantiating Model\n",
    "    seg_model = configure_model(model_params_path, verbose=True)\n",
    "    \n",
    "    ## Looping through datset\n",
    "    samples = parse_dataset_file(dataset_txt)[]\n",
    "    for sample in samples[:2]:\n",
    "        scan_path = sample[\"scan_path\"]\n",
    "        sample_name = sample[\"name\"]\n",
    "        print(scan_path, sample_name)\n",
    "        \n",
    "        ## Segmentation\n",
    "        scan_data = run_preclustering(seg_model, scan_path, sample_tag=sample_name, inst_seg_img_dir_path=trash_dir, cls_seg_img_dir_path=trash_dir)\n",
    "        \n",
    "        ## DBSCAN on Instance Seg\n",
    "        scan_data = full_isolate_and_cluster(scan_data, sample_name, 4, eps=eps, min_samples=min_samples, view_per_segment=True, view_full=True, out_dir=out_dir)\n",
    "\n",
    "test_DBSCAN_onInstSegResults(model_params_path, dataset_txt, img_dir, trash_dir, eps=eps, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49763748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
