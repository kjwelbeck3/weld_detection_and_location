{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04da5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring sklearn's DBSCAN\n",
    "## Goal: For multiple samples, Across different param combos, focussing on different data slices \n",
    "##       --> Compile splits, compile plots\n",
    "## Specify reasoned parameter ranges, for each data slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b4fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os, errno\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Instseg_model import MultiLayerFastLocalGraphModelV2 as model1\n",
    "from dataset import pcloader\n",
    "from graph_generation import gen_multi_level_local_graph_v3\n",
    "\n",
    "from math import floor, ceil\n",
    "from scipy.stats import mode\n",
    "\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION INFERENCE\n",
    "##  - High Level Config Settings\n",
    "##  - Singular Function to get predictions\n",
    "##  - Post Processing for Non Background points\n",
    "##  - Plot and Save Both Predictions\n",
    "\n",
    "## LOOPING THROUGH DIFFERENT CLUSTERING RUNS FOR EACH DATA SLICE\n",
    "##  - High Level Config Setting\n",
    "##  - Single Function for Run through param matrix --> log and Plots for all samples\n",
    "##     - Plots in the Same Folder\n",
    "##     - CSV of Cluster and Cluster Count\n",
    "\n",
    "## ORGANIZING OUTPUT DATA\n",
    "## Single CSV : FileName, SampleTag, slice, param1, param2, paramX,-1_count, 0_count, etc\n",
    "## Directory for Plots prefixed with SampleTag + DataColHeaders + paramCombo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba0e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION INFERENCE\n",
    "\n",
    "## Hight Level Config Settings\n",
    "\n",
    "graph_gen_kwargs = {\n",
    "\t'add_rnd3d': True,\n",
    "\t'base_voxel_size': 0.8,\n",
    "\t'downsample_method': 'random',\n",
    "\t'level_configs': [\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 64, 'radius': 0.4},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 0,\n",
    "\t\t 'graph_scale': 1},\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 192, 'radius': 1.2},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 1,\n",
    "\t\t 'graph_scale': 1}]\n",
    "}\n",
    "\n",
    "def configure_model(model_params_path, max_cls_classes=3, max_inst_classes=7, verbose=False):\n",
    "    a = time.time()\n",
    "    model = model1(num_classes=max_cls_classes, max_instance_no=max_inst_classes)\n",
    "    if os.path.isfile(model_params_path):\n",
    "        model.load_state_dict(torch.load(model_params_path))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"[ModelParamPathError] {model_params_path} does not exist\")\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), model_params_path)\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(f\"Model Setup Time (secs) : {b-a}\")\n",
    "    return model\n",
    "\n",
    "def classify_scan(model, scan_path, verbose=False):\n",
    "    a = time.time()\n",
    "    pointxyz, offset = pcloader(scan_path)\n",
    "    vertex_coord_list, keypoint_indices_list, edges_list = \\\n",
    "    gen_multi_level_local_graph_v3(pointxyz,0.6,graph_gen_kwargs['level_configs'])\n",
    "    last_layer_v = vertex_coord_list[-1]\n",
    "\n",
    "    ## conversions: type precision\n",
    "    vertex_coord_list = [p.astype(np.float32) for p in vertex_coord_list]\n",
    "    keypoint_indices_list = [e.astype(np.int32) for e in keypoint_indices_list]\n",
    "    edges_list = [e.astype(np.int32) for e in edges_list]\n",
    "\n",
    "    ## conversions: numpy array to tensor\n",
    "    vertex_coord_list = [torch.from_numpy(item) for item in vertex_coord_list]\n",
    "    keypoint_indices_list = [torch.from_numpy(item).long() for item in keypoint_indices_list]\n",
    "    edges_list = [torch.from_numpy(item).long() for item in edges_list]\n",
    "\n",
    "    batch = (vertex_coord_list, keypoint_indices_list, edges_list)\n",
    "    cls_seg, inst_seg = model(*batch)\n",
    "\n",
    "    cls_preds = torch.argmax(cls_seg, dim=1)\n",
    "    inst_preds = torch.argmax(inst_seg, dim=1)\n",
    "    \n",
    "    cls_preds = np.expand_dims(cls_preds, axis=1)\n",
    "    inst_preds = np.expand_dims(inst_preds, axis=1)\n",
    "\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(\"Scan Inference Time (secs): \", b-a)\n",
    "        print()\n",
    "    \n",
    "    return np.hstack((last_layer_v, cls_preds, inst_preds)),\\\n",
    "            {'vertices': last_layer_v, 'cls_preds': cls_preds, 'inst_preds': inst_preds}\n",
    "\n",
    "def filter_out_background(scan_data):\n",
    "    non_bg_idx = ~np.logical_or(scan_data[:, 3] == 0, scan_data[:, 4] == 0)\n",
    "    non_bg = scan_data[non_bg_idx]\n",
    "    return non_bg, non_bg_idx\n",
    "\n",
    "def count_cluster_by_instance_prediction(scan_data, threshold_factor=0.5):\n",
    "    inst, inst_count = np.unique(scan_data[:,4], return_counts=True)\n",
    "    inst_count_threshold = threshold_factor * np.mean(inst_count)\n",
    "    reduced_idx = np.where(inst_count > inst_count_threshold)\n",
    "    cluster_count = np.sum(inst_count > inst_count_threshold)\n",
    "    reduced = inst[reduced_idx]\n",
    "    \n",
    "    return cluster_count, {'orig_insts': inst,\n",
    "                           'orig_inst_ct': inst_count,\n",
    "                           'inst_ct_thresh': inst_count_threshold,\n",
    "                           'reduced_insts': reduced}\n",
    "\n",
    "def _plot(path, points, values, title=\"\", caption=\"\", overlay_points=None ):\n",
    "    \"\"\"\n",
    "    path - output path\n",
    "    points - matrix of x, y, etc cols\n",
    "    values - assigned value per point ie predicted/truth cls/inst\n",
    "    [optional] overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    im = ax.scatter(points[:,0], points[:,1],s=0.25,c=values)\n",
    "    \n",
    "    ims = []\n",
    "    if overlay_points:    \n",
    "        for pts in overlay_points:\n",
    "            _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=20, c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "#             ims.append(_im)\n",
    "            \n",
    "        \n",
    "\n",
    "    ax.set_xlabel(\"x [mm]\")\n",
    "    ax.set_ylabel(\"y [mm]\")\n",
    "    legend_ = ax.legend(*im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "    ax.add_artist(legend_)\n",
    "#     for _im in ims:\n",
    "#         _legend = ax.legend(*_im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "#         ax.add_artist(_legend)\n",
    "\n",
    "    ax.text(0.5, -0.5, caption, style='italic', \\\n",
    "        horizontalalignment='center', verticalalignment='top', transform=ax.transAxes)\n",
    "    axes=plt.gca()\n",
    "    axes.set_aspect(1)\n",
    "    if path:\n",
    "        plt.savefig(path, dpi=150)\n",
    "        plt.close()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show\n",
    "    \n",
    "def save_prediction_plots(non_bg_matrix, cls_tag=None, inst_tag=None, cls_col=3, inst_col=4, dir_path=\"./plots/\", verbose=False):\n",
    "    \"\"\"\n",
    "    non_bg_matrix -- x|y|z|cls|inst\n",
    "    // tag -- eg A_xycls_eps0_45_50\n",
    "    tag -- eg A_xycls\n",
    "    \"\"\"\n",
    "    f_tag = \"[save_prediction_plots]\"\n",
    "    f_msg = []\n",
    "    \n",
    "    if not (cls_tag and inst_tag):\n",
    "        print(\"[ERROR] No specfied cls_tag or inst_tag args. Plots not generated\")\n",
    "    else:\n",
    "        if cls_tag:\n",
    "            cls_output_path = dir_path+cls_tag+\".png\" if dir_path else None\n",
    "            _plot(cls_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, cls_col], title=cls_tag)\n",
    "            f_msg.append(cls_output_path)\n",
    "\n",
    "        if inst_tag:\n",
    "            inst_output_path = dir_path+inst_tag+\".png\" if dir_path else None\n",
    "            _plot(inst_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, inst_col], title=inst_tag)\n",
    "            f_msg.append(inst_output_path)\n",
    "\n",
    "        if verbose:\n",
    "#             f_msg = \", \".join(f_msg)\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "    \n",
    "\n",
    "def run_preclustering(model, scan_path, sample_tag=\"\", dir_path=\"./plots\"):\n",
    "    scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "    scan_data, _ = filter_out_background(scan_data)\n",
    "    _, cluster_data = count_cluster_by_instance_prediction(scan_data)\n",
    "    save_prediction_plots(scan_data, cls_tag=sample_tag+\"_xycls\", inst_tag=sample_tag+\"_xyinst\", dir_path=dir_path)\n",
    "    return scan_data, {\"0_scan\": scan_path, \"0_tag\":sample_tag}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8eac280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Test Running a sample\n",
    "\n",
    "# ## Run Settings\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "\n",
    "# ## Instantiating model and running scan through for predictions\n",
    "# model = configure_model(model_params_path, verbose=True)\n",
    "# scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "# print(\"scan_data.shape :\")\n",
    "# print(scan_data.shape)\n",
    "# print()\n",
    "\n",
    "# ## Filter out background points\n",
    "# scan_data, _ = filter_out_background(scan_data)\n",
    "# print(\"scan_data.shape :\")\n",
    "# print(scan_data.shape)\n",
    "# print()\n",
    "\n",
    "# ## Clustering by the instance predictions\n",
    "# cluster_count, cluster_data = count_cluster_by_instance_prediction(scan_data)\n",
    "# print(\"cluster_count: \")\n",
    "# print(cluster_count)\n",
    "# print()\n",
    "# print(\"cluster_data: \")\n",
    "# print(cluster_data)\n",
    "# print()\n",
    "\n",
    "# ## Plotting class predictions and instance predictions\n",
    "# save_prediction_plots(scan_data, cls_tag='egA_xycls', inst_tag='egA_xyinst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55fd0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_xyz(scan_data):\n",
    "    ## Scaling the numerical data columns ie x, y, z cols, then appending the orig cols for return\n",
    "    \n",
    "    data_numerical = scan_data[:, :3]\n",
    "    X = StandardScaler()\n",
    "    scaled_num = X.fit_transform(data_numerical)\n",
    "    return np.hstack((scaled_num, scan_data))\n",
    "\n",
    "\n",
    "\n",
    "def save_cluster_plots(non_bg_matrix, values, title=None, dir_path=\"./plots/\", verbose=False):\n",
    "    \"\"\"\n",
    "   ### non_bg_matrix -- scaled_x|scaled_y|scaled_z|x|y|z|cls|inst\n",
    "    non_bg_matrix -- x|y|z|cls|inst\n",
    "    values -- cluster id\n",
    "    tag -- eg A_x_eps0.05_minpts30\n",
    "    \"\"\"\n",
    "    f_tag = \"[save_cluster_plots]\"\n",
    "    f_msg = \"\"\n",
    "    \n",
    "    if not title:\n",
    "        print(\"[ERROR] No specfied title args. Plot not generated\")\n",
    "    else:\n",
    "        cls_output_path = dir_path+title+\".png\"\n",
    "        _plot(cls_output_path, non_bg_matrix[:, :2], values, title=title)\n",
    "        f_msg = cls_output_path\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "            \n",
    "def processDBSCAN(data, sample_tag, cols_tag, eps, min_samples, plot_sample=False, verbose=False):\n",
    "    \"\"\"\n",
    "    data  -- scaled_x|scaled_y|scaled_z|x|y|z|cls|inst\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data[:, :1])\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "        \n",
    "#         if plot_sample:\n",
    "#             plt.scatter(scan_data[idx, 3], scan_data[idx, 4])\n",
    "    \n",
    "#     if plot_sample:\n",
    "#         plt.show()\n",
    "\n",
    "    \n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def processDBSCAN2(data, sample_tag, cols_tag, eps, min_samples, verbose=False):\n",
    "    \"\"\"\n",
    "    data  -- eg x|y\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "\n",
    "    \n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def processDBSCAN3_with_backups(data, instance_count, sample_tag, cols_tag, eps_seq, min_samples_seq, verbose=False):\n",
    "    assert type(eps_seq) == type(min_samples_seq), \"eps_seq and min_samples_seq must be of the same type, either a float or a list of float\"\n",
    "    assert len(eps_seq) == len(min_samples_seq), \"eps_seq and min_samples_seq must be of the same length\"\n",
    "    \n",
    "    expected_mean = data.shape[0]/instance_count\n",
    "    count_range = (0.75*expected_mean, 1.25*expected_mean)\n",
    "    \n",
    "#     pairs = zip(eps_seq, min_samples_seq)\n",
    "    \n",
    "    cluster_count = 0\n",
    "    pair_idx = 0\n",
    "    max_cluster_count = 0\n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    \n",
    "    while cluster_count != instance_count:\n",
    "        if pair_idx >= len(eps_seq):\n",
    "            break\n",
    "            \n",
    "        eps = eps_seq[pair_idx]\n",
    "        min_samples = min_samples_seq[pair_idx]\n",
    "        DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "        DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "        ## grouping and overriding DBSCAN_clusters and _cluster_count with clean version\n",
    "        DBSCAN_clusters, DBSCAN_cluster_counts = \\\n",
    "                pre_clean_cluster(DBSCAN_clusters, DBSCAN_cluster_counts, count_range, instance_count)\n",
    "\n",
    "        pair_idx+=1\n",
    "        cluster_count = len(DBSCAN_cluster_counts)\n",
    "        if cluster_count > max_cluster_count:\n",
    "            max_cluster_count = cluster_count\n",
    "            for cluster in DBSCAN_clusters:\n",
    "                idx = np.where(DBSCAN_result == cluster)\n",
    "                out[idx] = cluster\n",
    "            title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "        \n",
    "    return out, title, {\"cluster_ids\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def pre_clean_cluster(clusters, cluster_counts, count_range, instance_count):\n",
    "    \n",
    "    ## Cluster Size Check: Removing the noise and merged clusters\n",
    "    filtered_clusters = []\n",
    "    for c_id, c_count in zip(clusters, cluster_counts):\n",
    "        if c_id != -1:\n",
    "            if c_count >= count_range[0] and c_count <= count_range[1]:\n",
    "                filtered_clusters.append((int(c_id), int(c_count)))\n",
    "                \n",
    "    ## Cluster Size Check: Sorting into descending order of cluster size\n",
    "    filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    " \n",
    "    ## Cluster Size Check: Returning only expected_cluster_count or less number of clusters\n",
    "    ## Effectively relying on GNN instance predictions for number of welds detected, at least\n",
    "    if len(filtered_clusters) > instance_count:\n",
    "        filtered_clusters = filtered_clusters[:instance_count]\n",
    "    \n",
    "    print(\"filtered_clusters\")\n",
    "    print(filtered_clusters)\n",
    "    \n",
    "#     clusters, cluster_counts = [], [] if filtered_clusters == [] else zip(*filtered_clusters) \n",
    "    if filtered_clusters == []:\n",
    "        clusters, cluster_counts = [0],[0]\n",
    "    else:\n",
    "        clusters, cluster_counts = zip(*filtered_clusters) \n",
    "    print(\"clusters, cluster_counts\")\n",
    "    print(clusters, cluster_counts)\n",
    "    \n",
    "    return clusters, cluster_counts\n",
    "    \n",
    "def clean_cluster(scan_data, clusters):\n",
    "    ## Reconsituting scan_data without noise, merged or extra clusters; \n",
    "    ## Reorders into continuguous clusters; possible size reduction\n",
    "    shortlist_cluster_idx = [c_id for c_id, c_count in filtered_clusters]\n",
    "    scan_data_idx = np.isin(scan_data[:,-1], shortlist_cluster_idx)\n",
    "    scan_data = scan_data[scan_data_idx]\n",
    "    meta[\"3_final_count\"] = scan_data.shape[0]\n",
    "    \n",
    "    \n",
    "def run_DBSCAN_clustering(scan_data, sample_tag, cols_tag, eps, min_samples, dir_path):\n",
    "    scan_data = scale_xyz(scan_data)\n",
    "    out, title, _ = processDBSCAN(scan_data, sample_tag, cols_tag, eps, min_samples, verbose=True)\n",
    "    save_cluster_plots(scan_data[:, 3:], out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))\n",
    "    \n",
    "def run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    out, title, _ = processDBSCAN2(scan_data[:, :2], sample_tag, \"xy\", eps, min_samples, verbose=False)\n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))\n",
    "\n",
    "def run_DBSCAN_clustering_2D_with_backups(scan_data, eps_seq, min_samples_seq, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    instance_count, _ = count_cluster_by_instance_prediction(scan_data)\n",
    "    out, title, clusters = processDBSCAN3_with_backups(scan_data[:,:2], instance_count, sample_tag, \"xy\", eps_seq, min_samples_seq, verbose=False)\n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out)), clusters\n",
    "\n",
    "## POSTProcessing after clustering to remove the low numbers (clusters of noise) and the high numbers (merges)\n",
    "## Check for non-clusters (-1 tagged) and remove from the scan_data\n",
    "## For each cluster, if count greater than a min and less than a max\n",
    "## Return \"cleaned\" cluster index in sorted order\n",
    "\n",
    "def postcluster_grouping(scan_data, clusters=None):\n",
    "    \n",
    "    meta = {}\n",
    "    \n",
    "    shortlist_cluster_idx = clusters[\"cluster_ids\"] if clusters else None\n",
    "    if shortlist_cluster_idx:\n",
    "        meta[\"2_count_pre_reduction\"] = sum(clusters[\"cluster_counts\"])\n",
    "    \n",
    "    else:\n",
    "        ## Cluster Size Check: Setting cluster count bounds to weed out noise cluster and merged clusters\n",
    "        expected_cluster_count, _ = count_cluster_by_instance_prediction(scan_data)\n",
    "        expected_mean = scan_data.shape[0]/expected_cluster_count\n",
    "        count_range = (0.5*expected_mean, 1.5*expected_mean)\n",
    "        meta[\"1_orig_nonbg_count\"] = scan_data.shape[0]\n",
    "        meta[\"4_clustering_size_range\"] = count_range\n",
    "\n",
    "        ## Cluster Size Check: Removing the noise and merged clusters\n",
    "        clusters, cluster_counts = np.unique(scan_data[:, -1], return_counts=True)\n",
    "        filtered_clusters = [] \n",
    "        for c_id, c_count in zip(clusters, cluster_counts):\n",
    "            if c_id != -1:\n",
    "                if c_count >= count_range[0] and c_count <= count_range[1]:\n",
    "                    filtered_clusters.append((int(c_id), int(c_count)))\n",
    "\n",
    "        ## [ERROR] should be storing a copy\n",
    "        meta[\"5_filtered_cluster_presort\"] = filtered_clusters\n",
    "\n",
    "        ## Cluster Size Check: Sorting into descending order of cluster size\n",
    "        filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    "\n",
    "        ## [ERROR] should be storing a copy\n",
    "        meta[\"6_filtered_cluster_sorted\"] = filtered_clusters\n",
    "        meta[\"2_filtered_by_size_count\"] = sum([c_count for c_id, c_count in filtered_clusters])\n",
    "\n",
    "        ## Cluster Size Check: Returning only expected_cluster_count or less number of clusters\n",
    "        ## Effectively relying on GNN instance predictions for number of welds detected, at least\n",
    "        if len(filtered_clusters) > expected_cluster_count:\n",
    "            filtered_clusters = filtered_clusters[:expected_cluster_count]\n",
    "        meta[\"7_filtered_cluster_final\"] = filtered_clusters    \n",
    "\n",
    "        shortlist_cluster_idx = [c_id for c_id, c_count in filtered_clusters]\n",
    "    \n",
    "    ## Reconsituting scan_data without noise, merged or extra clusters; possible size reduction\n",
    "    scan_data_idx = np.isin(scan_data[:,-1], shortlist_cluster_idx)\n",
    "    scan_data = scan_data[scan_data_idx]\n",
    "    meta[\"3_final_count\"] = scan_data.shape[0]\n",
    "    \n",
    "    return scan_data, meta\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45094672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2083eecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## CLUSTERING EXPERIMENTS\n",
    "\n",
    "# scan_path_options = [{\"scan_path\":\"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\", \"tag\": \"A\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1--2312014-NoResult-l-2023_06_02-7-56-18-48.ply\", \"tag\": \"B\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-10-231201626-Pass-2023_06_13-9-13-41-018.ply\", \"tag\": \"C\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-4-231201605-Pass-2023_06_12-10-46-11-096.ply\", \"tag\": \"D\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-1-231201596-Pass-2023_06_13-12-18-19-465.ply\", \"tag\": \"E\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-2-231201584-Pass-2023_06_09-9-19-43-815.ply\", \"tag\": \"F\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-5-231201583-Pass-2023_06_09-9-16-53-153.ply\", \"tag\": \"G\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-11-231201594-Pass-2023_06_13-12-08-56-692.ply\", \"tag\": \"H\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201598-Pass-2023_06_12-9-30-17-240.ply\", \"tag\": \"I\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201597-Fail-2023_06_13-12-21-34-579.ply\", \"tag\": \"J\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201599-Fail-2023_06_13-12-43-13-933.ply\", \"tag\": \"K\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201607-Fail-2023_06_13-13-44-20-591.ply\", \"tag\": \"L\"},\n",
    "#                     ]\n",
    "# eps_options = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4,\\\n",
    "#                0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "# min_samples_options = [3, 10, 30, 50, 75, 100, 150, 200, 250, 300, 350, 400]\n",
    "# dir_path = \"./plots1_/\"\n",
    "\n",
    "# # eps_options = [0.01, 0.02]\n",
    "# # min_samples_options = [3, 10]\n",
    "\n",
    "# scan_data = None\n",
    "# for scan in scan_path_options:\n",
    "#     scan_data = run_preclustering(model, scan[\"scan_path\"], scan[\"tag\"], dir_path=dir_path)\n",
    "\n",
    "#     for eps in eps_options:\n",
    "#         for min_samples in min_samples_options:\n",
    "#             run_DBSCAN_clustering(scan_data, scan[\"tag\"], \"scaled_x\", eps, min_samples, dir_path=dir_path)\n",
    "# print(\"Done\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5c6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fdf5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Testing the Clustering run\n",
    "\n",
    "# print(\"scan_data.shape: \")\n",
    "# print(scan_data.shape)\n",
    "# print()\n",
    "\n",
    "# scan_data = scale_xyz(scan_data)\n",
    "# print(\"scan_data.shape: \")\n",
    "# print(scan_data.shape)\n",
    "# print()\n",
    "\n",
    "# result, data = processDBSCAN(scan_data[:, :1], 'testA', 'xy', 0.02, 3)\n",
    "# print(\"result\")\n",
    "# print(result)\n",
    "# print()\n",
    "\n",
    "# print(\"data\")\n",
    "# print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1afb4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "# scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "# scan_data, _ = filter_out_background(scan_data)\n",
    "# count_cluster_by_instance_prediction(scan_data, threshold_factor=0.5)\n",
    "# scan_data = scale_xyz(scan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ab7341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### IDEA2\n",
    "# ## LOOPING THROUGH EACH INSTANCE SUBSET AND CLUSTERING SINGULAR HIGH DENSITY REGION\n",
    "\n",
    "# def filter_and_cluster(scan_data, inst_col_no, inst_id, eps, min_samples, verbose=False):\n",
    "# #     print(scan_data[inst_col_no])\n",
    "#     inst_subset_idx = np.where(scan_data[:, inst_col_no] == inst_id)\n",
    "# #     if verbose:\n",
    "# #         print(inst_subset_idx)\n",
    "#     inst_subset = scan_data[inst_subset_idx]\n",
    "#     if verbose:\n",
    "#         print(\"inst_subset.shape\")\n",
    "#         print(inst_subset.shape)\n",
    "        \n",
    "#     DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#     DBSCAN_result = DBSCAN_model.fit_predict(inst_subset[:, :1])\n",
    "#     DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "#     if verbose:\n",
    "#         print(\"DBSCAN_clusters\")\n",
    "#         print(DBSCAN_clusters)\n",
    "#         print()\n",
    "        \n",
    "#         print(\"DBSCAN_cluster_counts\")\n",
    "#         print(DBSCAN_cluster_counts)\n",
    "#         print()\n",
    "        \n",
    "#     for cluster in DBSCAN_clusters:\n",
    "#         idx = np.where(DBSCAN_result == cluster)\n",
    "#         plt.scatter(scan_data[idx, 0], scan_data[idx, 1])\n",
    "    \n",
    "#     plt.show\n",
    "#     return inst_subset\n",
    "\n",
    "\n",
    "# filter_and_cluster(scan_data, 7, 2, 0.01, 50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d31e3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Testing DBSCAN Clustering on 2D ie  x and y Locs\n",
    "\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)\n",
    "\n",
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "\n",
    "\n",
    "# scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "# scan_data, _ = filter_out_background(scan_data)\n",
    "# _, cluster_data = count_cluster_by_instance_prediction(scan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35c1b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 2.5\n",
    "# min_samples = 30\n",
    "# out, title, _ = processDBSCAN2(scan_data[:, :2], \"\", \"\", eps, min_samples, verbose=True)\n",
    "# print(\"title:\", title)\n",
    "# _plot(None, scan_data[:, :2], out, title=\"\", caption=\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "179c787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbb72bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_path_options = [{\"scan_path\":\"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\", \"tag\": \"A\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1--2312014-NoResult-l-2023_06_02-7-56-18-48.ply\", \"tag\": \"B\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-10-231201626-Pass-2023_06_13-9-13-41-018.ply\", \"tag\": \"C\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-4-231201605-Pass-2023_06_12-10-46-11-096.ply\", \"tag\": \"D\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-1-231201596-Pass-2023_06_13-12-18-19-465.ply\", \"tag\": \"E\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-2-231201584-Pass-2023_06_09-9-19-43-815.ply\", \"tag\": \"F\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-5-231201583-Pass-2023_06_09-9-16-53-153.ply\", \"tag\": \"G\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-11-231201594-Pass-2023_06_13-12-08-56-692.ply\", \"tag\": \"H\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201598-Pass-2023_06_12-9-30-17-240.ply\", \"tag\": \"I\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201597-Fail-2023_06_13-12-21-34-579.ply\", \"tag\": \"J\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201599-Fail-2023_06_13-12-43-13-933.ply\", \"tag\": \"K\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201607-Fail-2023_06_13-13-44-20-591.ply\", \"tag\": \"L\"},\n",
    "#                     ]\n",
    "# eps_options = [1.0, 1.5, 2, 2.5, 3]\n",
    "# min_samples_options = [3, 10, 20, 30, 40, 50]\n",
    "# dir_path = \"./plots2_/\"\n",
    "\n",
    "\n",
    "# scan_data = None\n",
    "# for scan in scan_path_options:\n",
    "#     scan_data = run_preclustering(model, scan[\"scan_path\"], scan[\"tag\"], dir_path=dir_path)\n",
    "\n",
    "#     for eps in eps_options:\n",
    "#         for min_samples in min_samples_options:\n",
    "#             run_DBSCAN_clustering_2D(scan_data, scan[\"tag\"], \"xy\", eps, min_samples, dir_path=dir_path)\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b756f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "# eps = 2\n",
    "# min_samples = 20\n",
    "# sample_tag = \"T1\"\n",
    "# dir_path = \"./plots/\"\n",
    "# scan_data = run_preclustering(model, scan_path, sample_tag=sample_tag, dir_path=dir_path)\n",
    "# scan_data = run_DBSCAN_clustering_2D(scan_data, sample_tag, \"xy\", eps, min_samples, dir_path=dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51c5b340",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# postcluster_grouping(scan_data)\n",
    "\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90222d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.random.randint(3, size=(10,2))\n",
    "# d = []\n",
    "# b = np.where(a[:,-1] == 2)[0].tolist()\n",
    "# d+=b\n",
    "# c = np.where(a[:,-1] == 0)[0].tolist()\n",
    "# d+=c\n",
    "\n",
    "# print(\"a\")\n",
    "# print(a)\n",
    "# print(\"b\")\n",
    "# print(b)\n",
    "# print(\"c\")\n",
    "# print(c)\n",
    "# print(\"d\")\n",
    "# print(d)\n",
    "\n",
    "# np.isin(a[:, 0], [1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72287c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1691a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing through a file itemizing a list of scans_path, label_path\n",
    "## Tag each as A-Z,AA-ZZ,AAA-ZZZ,etc\n",
    "## Parse Each and Plot\n",
    "## Run through the DBSCAN_2D and plot clustering\n",
    "## Run through the post_clustering_grouping\n",
    "## RUn through the post_clustering_voting\n",
    "## Plot final cls and inst, for demo sake\n",
    "## Compile Spreadsheet of plotting results\n",
    "## Compile meta data: prediction point count, clustering point count, filtered clustering point \n",
    "## compile weld data: centroid, x/y min/max, type --> ordered by x,\n",
    "##                    bincounts --> mode_x, mode_y\n",
    "##                    median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fb37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scan_label_file(file_path):\n",
    "    \"\"\"\n",
    "    Read file at spec'd file for line count and for scan and label path lists\n",
    "    \"\"\"\n",
    "    scan_list, label_list = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            scan_path, label_path = eval(line)\n",
    "            scan_list.append(scan_path)\n",
    "            label_list.append(label_path)\n",
    "    return len(scan_list), {\"scans\": scan_list, \"labels\": label_list}\n",
    "    \n",
    "\n",
    "# sample_file_path = \"./_data/selection.txt\"\n",
    "# count, data = parse_scan_label_file(sample_file_path)\n",
    "# print(\"parse count\", count)\n",
    "# print(\"\\nscans\")\n",
    "# print(data[\"scans\"])\n",
    "# print(\"\\nlabels\")\n",
    "# print(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ddc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tags_list(item_count):\n",
    "    \"\"\"\n",
    "    Generating tag list for X number of items up to max count of 26*27=702\n",
    "    level1: A to Z --> 1 to 26\n",
    "    level2: AA to ZZ --> 27 to 26*(26+1)\n",
    "    \"\"\"\n",
    "    chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    \n",
    "    ## Dictionary to capture each Level\n",
    "    n_levels = 1 if item_count <=26 else 2\n",
    "    level_els = {}\n",
    "    for i in range(1, n_levels+1):\n",
    "        level_els[i] = []\n",
    "\n",
    "    ## Incrementing up to item_count and creating a tag each time\n",
    "    idx = 0\n",
    "    while idx < item_count:\n",
    "        if idx < 26:\n",
    "            level_els[1].append(chars[idx])\n",
    "        elif idx < 26*27:\n",
    "            first = int(idx/26) - 1\n",
    "            second = idx % 26\n",
    "            level_els[2].append(level_els[1][first]+chars[second])\n",
    "        idx+=1\n",
    "    \n",
    "    ## Combining dictionary's levels into output list\n",
    "    _list = level_els[1] + level_els[2] if n_levels==2 else level_els[1]\n",
    "    \n",
    "    return _list\n",
    "\n",
    "# test_tags1 = gen_tags_list(26*27)\n",
    "# print(test_tags1[:3])\n",
    "# print(test_tags1[-3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3511157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postcluster_voting(scan_data):\n",
    "    \n",
    "    ## create new voting cols: cls and inst\n",
    "    n_rows, _ = scan_data.shape\n",
    "    scan_data = np.hstack((scan_data, np.zeros((n_rows, 1)), np.zeros((n_rows, 1)))) \n",
    "\n",
    "    ## expecting col5 to be the clustering result\n",
    "    clusters = np.unique(scan_data[:, 5])\n",
    "    for c_id in clusters:\n",
    "        scan_data_subset_idx = np.where(scan_data[:, 5] == c_id)\n",
    "        \n",
    "        ## majority votes\n",
    "        cls_vote = mode(np.ravel(scan_data[scan_data_subset_idx,3]), keepdims=False)[0]\n",
    "        inst_vote = mode(np.ravel(scan_data[scan_data_subset_idx,4]), keepdims=False)[0]\n",
    "        \n",
    "        ## push to data matrix\n",
    "        scan_data[scan_data_subset_idx, 6] = cls_vote\n",
    "        scan_data[scan_data_subset_idx, 7] = inst_vote\n",
    "\n",
    "    return scan_data\n",
    "\n",
    "# scan_data_test = postcluster_voting(scan_data)\n",
    "# df = pd.DataFrame(scan_data_test, columns=['x', 'y', 'z', 'cls_preds' ,'inst_pred', 'cluster', 'cls', 'inst'])\n",
    "# df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "086d92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial version\n",
    "def run_postclustering(scan_data, sample_tag=\"\", dir_path=\"./plots/\", clusters=None ):\n",
    "    scan_data, meta1 = postcluster_grouping(scan_data, clusters=None) ## ncols remains 6\n",
    "    scan_data = postcluster_voting(scan_data)  ## n_cols increased from 6 to 8\n",
    "    save_prediction_plots(scan_data, cls_tag=f\"{sample_tag}_xycls_clean\", inst_tag=f\"{sample_tag}_xyinst_clean\", cls_col=6, inst_col=7, dir_path=dir_path)\n",
    "    return scan_data, meta1\n",
    "\n",
    "# ## refactored version\n",
    "# def run_clustervoting(scan_data, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "#     scan_data = postcluster_voting(scan_data)  ## n_cols increased from 6 to 8\n",
    "#     save_prediction_plots(scan_data, cls_tag=f\"{sample_tag}_xycls_clean\", inst_tag=f\"{sample_tag}_xyinst_clean\", cls_col=6, inst_col=7, dir_path=dir_path)\n",
    "#     return scan_data, meta1\n",
    "\n",
    "def save_data_dictionary(data, dir_path=\"./plots/\"):\n",
    "    file_path = f\"{dir_path}log.txt\"\n",
    "    pairs = [(k, v) for k, v in data.items()]\n",
    "    pairs.sort()\n",
    "    with open(file_path, \"a+\") as f_out:\n",
    "        for k,v in pairs:\n",
    "            f_out.write(f\"{k} \\t {v}\\n\")\n",
    "        f_out.write(\"\\n\")\n",
    "    \n",
    "# scan_data_test, meta_test = run_postclustering(scan_data, \"testA\", dir_path=None)\n",
    "# save_data_dictionary(meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc6ed417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29966dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eps = 2\n",
    "# min_samples = 20\n",
    "# sample_tag = \"T1\"\n",
    "# dir_path = \"./plots/\"\n",
    "# scan_data = run_preclustering(model, scan_path, sample_tag=sample_tag, dir_path=dir_path)\n",
    "# scan_data = run_DBSCAN_clustering_2D(scan_data, sample_tag, \"xy\", eps, min_samples, dir_path=dir_path)\n",
    "# run_postclustering(scan_data, \"testA\", None)\n",
    "\n",
    "def run(model_params_path, file_path, dir_path, eps=2, min_samples=20):\n",
    "    \n",
    "    ## [TODO] Check path exists\n",
    "    \n",
    "    ## [TODO] Delete Log file if exist\n",
    "    \n",
    "    ## Generate model, load parameters\n",
    "    model = configure_model(model_params_path, verbose=True)\n",
    "    \n",
    "    ## Parse the spec'd file for count and scans' paths\n",
    "    file_count, file_data = parse_scan_label_file(file_path)\n",
    "    scans = file_data[\"scans\"]\n",
    "    \n",
    "    ## Generate sample tags for each scan\n",
    "    tags = gen_tags_list(file_count)\n",
    "    \n",
    "    ## Zip scans and tags and loop through each pair\n",
    "    for scan, tag in zip(scans, tags):\n",
    "        \n",
    "        ## preclustering: inference, filter out background, save plots\n",
    "        scan_data, metaA = run_preclustering(model, scan, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "        ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6]\n",
    "#         scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "        ## ALTERNATIVELY for exploration: cluster and plot diff combinations of eps and min_samples\n",
    "        if isinstance(eps, list) and isinstance(min_samples, list):\n",
    "            for e in eps:\n",
    "                for ms in min_samples:\n",
    "                    run_DBSCAN_clustering_2D(scan_data, e, ms, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "        elif isinstance(eps, list) and not isinstance(min_samples):\n",
    "            for e in eps:\n",
    "                run_DBSCAN_clustering_2D(scan_data, e, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "                \n",
    "        elif not isinstance(eps,list) and isinstance(min_samples, list):\n",
    "            for ms in min_samples:\n",
    "                run_DBSCAN_clustering_2D(scan_data, eps, ms, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "        else:\n",
    "            scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "            \n",
    "        \n",
    "            ## postclustering: filter out noise and merged data, limit cluster \n",
    "            ## count to count from instance predictions, majority votes and \n",
    "            ## add to scan_data [cols: 6->8], save plots\n",
    "            scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "\n",
    "            ## combine meta dictionaries and print to file\n",
    "            meta = metaA | metaB\n",
    "            save_data_dictionary(meta, dir_path=dir_path)\n",
    "\n",
    "            ## locationing: collect welds with mean/median/mode, type, id, and tag; save plot and csv\n",
    "            welds = run_locationing(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "    print(\"[Run]: Done!\")\n",
    "        \n",
    "def run2(model_params_path, file_path, dir_path, eps_seq=[2], min_samples_seq=[20]):\n",
    "    \n",
    "    ## [TODO] Check path exists\n",
    "    \n",
    "    ## [TODO] Delete Log file if exist\n",
    "    \n",
    "    ## Generate model, load parameters\n",
    "    model = configure_model(model_params_path, verbose=True)\n",
    "    \n",
    "    ## Parse the spec'd file for count and scans' paths\n",
    "    file_count, file_data = parse_scan_label_file(file_path)\n",
    "    scans = file_data[\"scans\"]\n",
    "    \n",
    "    ## Generate sample tags for each scan\n",
    "    tags = gen_tags_list(file_count)\n",
    "    \n",
    "    ## Zip scans and tags and loop through each pair\n",
    "    for scan, tag in zip(scans, tags):\n",
    "        \n",
    "        ## preclustering: inference, filter out background, save plots\n",
    "        scan_data, metaA = run_preclustering(model, scan, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "        ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6]\n",
    "        scan_data, clusters = \\\n",
    "                run_DBSCAN_clustering_2D_with_backups(scan_data, eps_seq, min_samples_seq, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "        \n",
    "        ## postclustering: filter out noise and merged data, limit cluster \n",
    "        ## count to count from instance predictions, majority votes and \n",
    "        ## add to scan_data [cols: 6->8], save plots\n",
    "        scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path, clusters=clusters)\n",
    "\n",
    "        ## combine meta dictionaries and print to file\n",
    "        meta = metaA | metaB\n",
    "        save_data_dictionary(meta, dir_path=dir_path)\n",
    "\n",
    "        ## locationing: collect welds with mean/median/mode, type, id, and tag; save plot and csv\n",
    "        welds = run_locationing(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "    print(\"[Run]: Done!\")\n",
    "        \n",
    "## [TODO] need a prodcution version without the plotting and/or saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8717412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemize_welds(scan_data, cls_col, inst_col, sample_tag=\"\"):\n",
    "    \"\"\"\n",
    "    For \n",
    "     - Instance id, and type\n",
    "     - min and max x and y\n",
    "     - Centroid by mean - x and y means\n",
    "     - Centroid by median - x and y medians\n",
    "     - Bin splits then highest count --> effectively the x and y modes  (bins to decrease ganularity of the widely sparsed floats)\n",
    "     \n",
    "    Output options: ==> Another Function\n",
    "     -- each weld to a record in a csv file\n",
    "     -- an imaged saved showing center estimations\n",
    "    \n",
    "    \"\"\"\n",
    "    ## A incrementing list of welds\n",
    "    welds = []\n",
    "    \n",
    "    ## Loop through each option in the inst_col ie each weld\n",
    "    for weld_inst in np.unique(scan_data[:, inst_col]):     \n",
    "        \n",
    "    ## For each weld identified, \n",
    "            \n",
    "        ## dict, label, and id \n",
    "        weld = {}\n",
    "        weld['tag'] = f\"{sample_tag}_weld{int(weld_inst)}\"\n",
    "        weld['id'] = int(weld_inst)\n",
    "\n",
    "        ## weld members\n",
    "        sub = scan_data[scan_data[:, inst_col] == weld_inst]\n",
    "        weld[\"sub\"] = sub\n",
    "\n",
    "        ## type\n",
    "        weld['type'] = \"[\" if sub[0, cls_col] == 1 else \"]\"\n",
    "\n",
    "        ## ranges\n",
    "        weld[\"x_min\"], weld[\"x_max\"] = np.min(sub[:, 0]), np.max(sub[:,0])\n",
    "        weld[\"y_min\"], weld[\"y_max\"] = np.min(sub[:, 1]), np.max(sub[:,1])\n",
    "\n",
    "        ## means\n",
    "        weld['x_mean'] = np.mean(sub[:,0])\n",
    "        weld['y_mean'] = np.mean(sub[:,1])\n",
    "\n",
    "        ## medians\n",
    "        weld['x_median'] = np.median(sub[:,0])\n",
    "        weld['y_median'] = np.median(sub[:,1])\n",
    "\n",
    "        ## modes\n",
    "        weld['x_modeA'], weld['x_modeB'] = modes(sub, 0, 0.2)\n",
    "        weld['y_modeA'], weld['y_modeB'] = modes(sub, 1, 0.2)\n",
    "\n",
    "        welds.append(weld)\n",
    "    \n",
    "    return welds\n",
    "            \n",
    "            \n",
    "def modes(arr, col, bin_step):\n",
    "    \"\"\"\n",
    "    bin_step determines resolution\n",
    "    \"\"\"\n",
    "    col_min, col_max = np.min(arr[:,col]), np.max(arr[:,col])\n",
    "    step = int(np.ceil((col_max - col_min)/bin_step)) ## ideally should recenter the cols instead of shrinking the divs\n",
    "    bin_bounds = np.linspace(col_min, col_max, step)\n",
    "    bin_splits = bin_bounds[1:-1]\n",
    "\n",
    "    splits = [np.where(np.logical_and(bin_bounds[i-1] <= arr[:,col], arr[:,col] <bin_bounds[i]))[0] for i in range(1, len(bin_bounds))]\n",
    "    split_counts = [len(split) for split in splits]\n",
    "    highest_count = max(split_counts)\n",
    "    \n",
    "    mode_by_mean = [np.mean(arr[splits[i], col]) for i, s in enumerate(split_counts) if s == highest_count]\n",
    "    mode_by_range_mid = [(bin_bounds[i] + bin_bounds[i+1])/2 for i, s in enumerate(split_counts) if s == highest_count]\n",
    "    \n",
    "    return mode_by_mean, mode_by_range_mid\n",
    "    \n",
    "def save_weld_location_plots(non_bg_matrix, values, welds, title=None, dir_path=\"./plots\", verbose=False):\n",
    "\n",
    "    f_tag = \"[save_weld_location_plots]\"\n",
    "    f_msg = \"\"\n",
    "    \n",
    "    if not title:\n",
    "        print(\"[ERROR] No specfied title args. Plot not generated\")\n",
    "    else:\n",
    "        ## weld_inst \n",
    "        ## --> x_/y_min/ max;\n",
    "        ## --> x_/y_mean;\n",
    "        ## --> x_/y_median;\n",
    "        ## --> x_/y_modeA;\n",
    "        ## --> x_/y_modeB;\n",
    "        \n",
    "        ## overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "        overlay_points, _ = parse_weld_centers(welds) \n",
    "        \n",
    "#         print()\n",
    "#         print(\"overlay_points\")\n",
    "#         print(overlay_points)\n",
    "        \n",
    "        \n",
    "        weld_loc_output_path = dir_path+title+\".png\"\n",
    "        _plot(weld_loc_output_path, non_bg_matrix[:, :2], values, title=title, overlay_points=overlay_points)\n",
    "        f_msg = weld_loc_output_path\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "        \n",
    "#         return f_msg \n",
    "\n",
    "def save_welds_csv(welds, dir_path=\"./plots/\", verbose=False):\n",
    "    filename = f\"{dir_path}log.csv\"\n",
    "    _keys = [\"tag\", \"id\", \"type\", \\\n",
    "             \"x_min\", \"x_max\", \\\n",
    "             \"y_min\", \"y_max\", \\\n",
    "             \"x_mean\", \"y_mean\", \\\n",
    "             \"x_median\", \"y_median\", \\\n",
    "             \"x_modeA\", \"y_modeA\", \\\n",
    "             \"x_modeB\", \"y_modeB\"]\n",
    "    \n",
    "    add_header = False\n",
    "    if not os.path.isfile(filename):\n",
    "        add_header = True\n",
    "        \n",
    "    with open(filename, 'a+') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        if add_header:\n",
    "            csvwriter.writerow(_keys)\n",
    "            add_header=False\n",
    "    \n",
    "        for weld in welds:\n",
    "            row = []\n",
    "            for k in _keys:\n",
    "                row.append(weld[k])\n",
    "            csvwriter.writerow(row)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[save_welds_csv]: Done --> {filename}\")\n",
    "        \n",
    "    pass\n",
    "\n",
    "def parse_weld_centers(welds_list):\n",
    "    \n",
    "    ## overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "    \n",
    "    means = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'+', \"label\":\"x_/y_mean\" }\n",
    "    medians = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'s', \"label\":\"x_/y_median\" }\n",
    "    modeAs = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'x', \"label\":\"x_/y_mode[mean]\" }\n",
    "    modeBs = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'*', \"label\":\"x_/y_mode[range_mid]\" }\n",
    "    \n",
    "    for weld in welds_list:\n",
    "        ## means\n",
    "        means[\"xs\"].append(weld[\"x_mean\"])\n",
    "        means[\"ys\"].append(weld[\"y_mean\"])\n",
    "        means[\"cs\"].append('black')\n",
    "        \n",
    "        ## medians\n",
    "        medians[\"xs\"].append(weld[\"x_median\"])\n",
    "        medians[\"ys\"].append(weld[\"y_median\"])\n",
    "        medians[\"cs\"].append('grey')\n",
    "        \n",
    "        ## modeAs\n",
    "        xs = weld[\"x_modeA\"]\n",
    "        ys = weld[\"y_modeA\"]\n",
    "        for x_mA in xs:\n",
    "            for y_mA in ys:\n",
    "                modeAs[\"xs\"].append(x_mA)\n",
    "                modeAs[\"ys\"].append(y_mA)\n",
    "                modeAs[\"cs\"].append('red')\n",
    "        \n",
    "        ## modeBs\n",
    "        xs = weld[\"x_modeB\"]\n",
    "        ys = weld[\"y_modeB\"]\n",
    "        for x_mB in xs:\n",
    "            for y_mB in ys:\n",
    "                modeBs[\"xs\"].append(x_mB)\n",
    "                modeBs[\"ys\"].append(y_mB)\n",
    "                modeBs[\"cs\"].append('orange')\n",
    "                \n",
    "                \n",
    "    return [means, medians, modeAs, modeBs]   \n",
    "\n",
    "\n",
    "def run_locationing(scan_data, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=sample_tag)\n",
    "    title = f\"{sample_tag}_xyinst_weld_locs\"\n",
    "    save_weld_location_plots(scan_data, scan_data[:,7], welds, title=title, dir_path=dir_path)\n",
    "    save_welds_csv(welds, dir_path=dir_path)\n",
    "    return welds\n",
    "\n",
    "# def \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b7c298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup Time (secs) : 0.09599614143371582\n",
      "Scan Inference Time (secs):  9.68393087387085\n",
      "\n",
      "Scan Inference Time (secs):  9.631904363632202\n",
      "\n",
      "Scan Inference Time (secs):  10.195894241333008\n",
      "\n",
      "Scan Inference Time (secs):  10.716588020324707\n",
      "\n",
      "Scan Inference Time (secs):  9.997865438461304\n",
      "\n",
      "Scan Inference Time (secs):  10.084866762161255\n",
      "\n",
      "Scan Inference Time (secs):  10.643692970275879\n",
      "\n",
      "Scan Inference Time (secs):  11.065883874893188\n",
      "\n",
      "Scan Inference Time (secs):  10.098860025405884\n",
      "\n",
      "Scan Inference Time (secs):  10.005257368087769\n",
      "\n",
      "Scan Inference Time (secs):  10.447861194610596\n",
      "\n",
      "Scan Inference Time (secs):  11.028852701187134\n",
      "\n",
      "Scan Inference Time (secs):  12.03091287612915\n",
      "\n",
      "Scan Inference Time (secs):  11.640886783599854\n",
      "\n",
      "Scan Inference Time (secs):  10.948853969573975\n",
      "\n",
      "Scan Inference Time (secs):  10.764853954315186\n",
      "\n",
      "[Run]: Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## VERSION 1: Single eps and min_samples OR eps and min_sample permutation exploration\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "file_path = \"./_data/good_and_bad2.txt\"\n",
    "dir_path = \"./plots5_/\"\n",
    "eps = [2, 1.5, 1]\n",
    "min_samples = [20, 10]\n",
    "run(model_params_path, file_path, dir_path, eps=eps, min_samples=min_samples)\n",
    "\n",
    "# ## VERSION2: indexwise eps and min_samples combinations\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# file_path = \"./_data/good_and_bad.txt\"\n",
    "# dir_path = \"./plots4_/\"\n",
    "# eps_seq = [2, 1.5, 1]\n",
    "# min_samples_seq = [20, 20, 10]\n",
    "# run2(model_params_path, file_path, dir_path, eps_seq=eps_seq, min_samples_seq=min_samples_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1d6d61b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  9.871661901473999\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Denoising single instances\n",
    "\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)\n",
    "\n",
    "# #easys\n",
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"  \n",
    "# sample_tag = \"_T_\"\n",
    "\n",
    "\n",
    "# #hards\n",
    "scan_path = './_data/scans/LH-6-231201595-Pass-2023_06_12-9-21-22-515.ply'\n",
    "sample_tag = '_H1_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-7-231201596-Fail-2023_06_13-12-19-11-036.ply'\n",
    "# sample_tag = '_H2_'\n",
    "\n",
    "# scan_path = './_data/scans/LH-6-231201598-Pass-2023_06_12-9-30-40-570.ply'\n",
    "# sample_tag = '_H3_'\n",
    "\n",
    "# scan_path = './_data/scans/LH-2-231201599-Pass-2023_06_12-9-34-34-210.ply'\n",
    "# sample_tag = '_H4_'\n",
    "\n",
    "# scan_path = './_data/scans/LH-2-231201595-Pass-2023_06_12-9-21-03-886.ply'\n",
    "# sample_tag = '_H5_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-9-231201591-Pass-2023_06_09-10-51-11-577.ply'\n",
    "# sample_tag = '_H6_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-2-231201593-Pass-2023_06_09-10-54-31-162.ply'\n",
    "# sample_tag = '_H7_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-4-231201590-Fail-2023_06_09-10-48-34-421.ply'\n",
    "# sample_tag = '_H8_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-7-231201616-Fail-2023_06_12-12-14-28-799.ply'\n",
    "# sample_tag = '_H9_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-8-231201592-Pass-2023_06_09-10-53-26-003.ply'\n",
    "# sample_tag = '_H10_'\n",
    "\n",
    "dir_path = \"./plots_tests/\"\n",
    "\n",
    "scan_data = run_preclustering(model, scan_path, sample_tag=sample_tag, dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3702b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processInstancePredictions(scan_data, sample_tag=\"\", verbose=False): # x|y|z|cls|inst\n",
    "    out = np.zeros((scan_data.shape[0], 1))\n",
    "    instances = np.unique(scan_data[:,4])\n",
    "    DBSCAN_model = DBSCAN(eps=2, min_samples=20)\n",
    "    global_clusters, global_cluster_counts = [], []\n",
    "    for inst in instances:\n",
    "        subset_idx = np.where(scan_data[:, 4] == inst)[0]\n",
    "        if verbose:\n",
    "            print(\"subset_idx\")\n",
    "            print(subset_idx)\n",
    "        \n",
    "        temp = np.zeros((subset_idx.shape[0], 4))\n",
    "        temp[:,0] = subset_idx\n",
    "        temp[:, 1:3] = scan_data[subset_idx, 0:2]\n",
    "        temp[:,3] = DBSCAN_model.fit_predict(temp[:, 1:3])\n",
    "        clusters, cluster_counts = np.unique(temp[:,3], return_counts=True)\n",
    "        if verbose:\n",
    "            print('temp')\n",
    "            print(temp)\n",
    "            print(\"clusters, cluster_counts\")\n",
    "            print(clusters, cluster_counts)\n",
    "        \n",
    "        ## Removing the noise clusters\n",
    "        filtered_clusters = []\n",
    "        for c_id, c_count in zip(clusters, cluster_counts):\n",
    "            if c_id != -1:\n",
    "                filtered_clusters.append((int(c_id), int(c_count)))\n",
    "                \n",
    "        ## Sorting into descending order of cluster size\n",
    "        filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    "        largest_clust = filtered_clusters[0][0]\n",
    "        if verbose:\n",
    "            print(\"largest_clust\")\n",
    "            print(largest_clust)\n",
    "        \n",
    "        ## Reconstituting cluster columns\n",
    "        idx = temp[temp[:,3] == largest_clust, 0].astype(int)\n",
    "        if verbose:\n",
    "            print(\"idx\", idx)\n",
    "        out[idx] = inst\n",
    "        global_clusters.append(int(inst))\n",
    "        global_cluster_counts.append(len(idx))\n",
    "\n",
    "    ## Removing the misclassed points\n",
    "    \n",
    "    \n",
    "    title = f\"{sample_tag}_xy_per_pred_inst\"\n",
    "    return out, title, {\"cluster_vals\": global_clusters, \"cluster_counts\": global_cluster_counts}\n",
    "\n",
    "def processDBSCAN3D(data, sample_tag, cols_tag, eps, min_samples, verbose=False):\n",
    "    \"\"\"\n",
    "    data  -- eg x|y\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "\n",
    "    \n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def run_denoising_predictions(scan_data, sample_tag=\"\", dir_path=\"./plots/\", verbose=False): # x|y|z|cls|inst\n",
    "    out, title, clusters = processInstancePredictions(scan_data, sample_tag)\n",
    "    scan_data = np.hstack((scan_data, out))\n",
    "    print(clusters)\n",
    "    cluster_idx = clusters[\"cluster_vals\"]\n",
    "    scan_data = scan_data[np.isin(scan_data[:,-1],  cluster_idx)]\n",
    "    save_cluster_plots(scan_data, scan_data[:,-1], title=title, dir_path=dir_path )\n",
    "    return scan_data\n",
    "\n",
    "\n",
    "def run_DBSCAN_clustering_3D(scan_data, eps, min_samples, instance_weighting=1, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    scan_data[:,4] *= instance_weighting\n",
    "    out, title, _ = processDBSCAN3D(scan_data[:, [0,1,4]], sample_tag, f\"xyinst_w{instance_weighting}\", eps, min_samples)\n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d0907e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scan_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m min_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      5\u001b[0m instance_weighting \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 6\u001b[0m soln \u001b[38;5;241m=\u001b[39m run_DBSCAN_clustering_3D(\u001b[43mscan_data\u001b[49m[\u001b[38;5;241m0\u001b[39m], eps, min_samples, instance_weighting, sample_tag\u001b[38;5;241m=\u001b[39msample_tag, dir_path\u001b[38;5;241m=\u001b[39mdir_path)\n\u001b[0;32m      7\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m run_postclustering(soln, sample_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mw\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_weighting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, dir_path\u001b[38;5;241m=\u001b[39mdir_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scan_data' is not defined"
     ]
    }
   ],
   "source": [
    "# run_denoising_predictions(scan_data[0], sample_tag=sample_tag, dir_path=dir_path)\n",
    "\n",
    "eps = 2\n",
    "min_samples = 20\n",
    "instance_weighting = 2\n",
    "soln = run_DBSCAN_clustering_3D(scan_data[0], eps, min_samples, instance_weighting, sample_tag=sample_tag, dir_path=dir_path)\n",
    "_, _ = run_postclustering(soln, sample_tag=f\"{sample_tag}w{instance_weighting}_\", dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aad6506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative to Isolating the Weld Centers\n",
    "## Centroiding\n",
    "## ICP\n",
    "## Pattern Matching\n",
    "## Manual: straight edges --> offset\n",
    "## GNN?\n",
    "\n",
    "def run_locationing_by_icp():\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe0988aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing ICP\n",
    "\n",
    "## 1. Create Template for source, 2D --> Desire the x,y and theta for each weld and weld are already \n",
    "## Need a good scan with multiple complete weld segments (emphasize quality) --> visualize\n",
    "## Combine those of like type by mean, median, mids, limits; Visualize the different options together for comparison\n",
    "## Offsets (x,y,theta) for combination to position the origin where desired\n",
    "## Alternatively offset each before combining\n",
    "\n",
    "\n",
    "def create_template(model, scan_path, tag=\"\", weld_selections=None, dir_path='./plots/'):\n",
    "    \n",
    "    ## preclustering: inference, filter out background, save plots\n",
    "    scan_data, metaA = run_preclustering(model, scan_path, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6] \n",
    "    print(\"create_template::scan_data.shape\")\n",
    "    print(scan_data.shape)\n",
    "    scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "    ## postclustering: filter out noise and merged data, limit cluster \n",
    "    ## count to count from instance predictions, majority votes and \n",
    "    ## add to scan_data [cols: 6->8], save plots\n",
    "    scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ### splitting for weld instances\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=tag)\n",
    "    \n",
    "    \n",
    "    ## collecting into type groups\n",
    "    \n",
    "    type1, type2 = [], []\n",
    "    if weld_selections:\n",
    "        if isinstance(weld_selections, list):\n",
    "            type1_insts = [int(sel) for sel in weld_selections]\n",
    "        else:\n",
    "            type1_insts = [int(weld_selections)]\n",
    "        type1 = [weld for weld in welds if weld[\"id\"] in type1_insts]\n",
    "    else:\n",
    "        for weld in welds:\n",
    "            if weld[\"type\"] == \"[\":\n",
    "                type1.append(weld)\n",
    "            else:\n",
    "                type2.append(weld)\n",
    "\n",
    "    ## counting for largest group, but starting with type1 for now\n",
    "    ## combining by subtracting the mean | median | mids\n",
    "    welds_at_means = []\n",
    "    welds_at_medians = []\n",
    "    welds_at_mids = []\n",
    "    welds_at_SW_corner = []\n",
    "    \n",
    "    for weld in type1:\n",
    "        \n",
    "        ## subtracting means\n",
    "        weld_sub_means = np.copy(weld[\"sub\"])\n",
    "        weld_sub_means[:, :2] -= np.array([weld[\"x_mean\"], weld[\"y_mean\"]])\n",
    "        welds_at_means.append(weld_sub_means)\n",
    "        \n",
    "        ## subtracting medians\n",
    "        weld_sub_medians = np.copy(weld[\"sub\"])\n",
    "        weld_sub_medians[:, :2] -= np.array([weld[\"x_median\"], weld[\"y_median\"]])\n",
    "        welds_at_medians.append(weld_sub_medians)\n",
    "        \n",
    "        ## subtracting mids\n",
    "        x_mid = (weld[\"x_max\"] + weld[\"x_min\"])/2\n",
    "        y_mid = (weld[\"y_max\"] + weld[\"y_min\"])/2\n",
    "        weld_sub_mids = np.copy(weld[\"sub\"])\n",
    "        weld_sub_mids[:, :2] -= np.array([x_mid, y_mid])\n",
    "        welds_at_mids.append(weld_sub_mids)\n",
    "        \n",
    "        ## subtracting SW corner\n",
    "        weld_sub_corner = np.copy(weld[\"sub\"])\n",
    "        weld_sub_corner[:, :2] -= np.array([weld[\"x_min\"], weld[\"y_min\"]])\n",
    "        welds_at_SW_corner.append(weld_sub_corner)\n",
    "        \n",
    "    template_at_mean = np.vstack(tuple(welds_at_means))\n",
    "    save_cluster_plots(template_at_mean, template_at_mean[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_means_then_merged\", dir_path=dir_path)\n",
    "        \n",
    "    template_at_median = np.vstack(tuple(welds_at_medians))\n",
    "    save_cluster_plots(template_at_median, template_at_median[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_medians_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "    template_at_mid = np.vstack(tuple(welds_at_mids))\n",
    "    save_cluster_plots(template_at_mid, template_at_mid[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_mids_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "    template_at_corner = np.vstack(tuple(welds_at_SW_corner))\n",
    "    save_cluster_plots(template_at_corner, template_at_corner[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_corner_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "    templateA = {\"at_mean\": template_at_mean,\n",
    "                \"at_median\": template_at_median,\n",
    "                \"at_mid\": template_at_mid,\n",
    "                \"at_corner\": template_at_corner}\n",
    "    if type2:\n",
    "    \n",
    "        welds_at_means = []\n",
    "        welds_at_medians = []\n",
    "        welds_at_mids = []\n",
    "        welds_at_SW_corner = []\n",
    "\n",
    "        for weld in type2:\n",
    "\n",
    "            ## subtracting means\n",
    "            weld_sub_means = np.copy(weld[\"sub\"])\n",
    "            weld_sub_means[:, :2] -= np.array([weld[\"x_mean\"], weld[\"y_mean\"]])\n",
    "            welds_at_means.append(weld_sub_means)\n",
    "\n",
    "            ## subtracting medians\n",
    "            weld_sub_medians = np.copy(weld[\"sub\"])\n",
    "            weld_sub_medians[:, :2] -= np.array([weld[\"x_median\"], weld[\"y_median\"]])\n",
    "            welds_at_medians.append(weld_sub_medians)\n",
    "\n",
    "            ## subtracting mids\n",
    "            x_mid = (weld[\"x_max\"] + weld[\"x_min\"])/2\n",
    "            y_mid = (weld[\"y_max\"] + weld[\"y_min\"])/2\n",
    "            weld_sub_mids = np.copy(weld[\"sub\"])\n",
    "            weld_sub_mids[:, :2] -= np.array([x_mid, y_mid])\n",
    "            welds_at_mids.append(weld_sub_mids)\n",
    "\n",
    "            ## subtracting SW corner\n",
    "            weld_sub_corner = np.copy(weld[\"sub\"])\n",
    "            weld_sub_corner[:, :2] -= np.array([weld[\"x_min\"], weld[\"y_min\"]])\n",
    "            welds_at_SW_corner.append(weld_sub_corner)\n",
    "\n",
    "        template_at_mean = np.vstack(tuple(welds_at_means))\n",
    "        save_cluster_plots(template_at_mean, template_at_mean[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_means_then_merged\", dir_path=dir_path)\n",
    "\n",
    "        template_at_median = np.vstack(tuple(welds_at_medians))\n",
    "        save_cluster_plots(template_at_median, template_at_median[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_medians_then_merged\", dir_path=dir_path)\n",
    "\n",
    "        template_at_mid = np.vstack(tuple(welds_at_mids))\n",
    "        save_cluster_plots(template_at_mid, template_at_mid[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_mids_then_merged\", dir_path=dir_path)\n",
    "\n",
    "        template_at_corner = np.vstack(tuple(welds_at_SW_corner))\n",
    "        save_cluster_plots(template_at_corner, template_at_corner[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_corner_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "        \n",
    "        templateB = {\"at_mean\": template_at_mean,\n",
    "                \"at_median\": template_at_median,\n",
    "                \"at_mid\": template_at_mid,\n",
    "                \"at_corner\": template_at_corner}\n",
    "        \n",
    "        return templateA, templateB\n",
    "    \n",
    "#     TODO: DRAW THE ORIGIN CENTERS\n",
    "    \n",
    "    return templateA\n",
    "\n",
    "def save_template_to_file(template, path, weld_type=\"[\"):\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_template_from_file(path, weld_type=\"[\"):\n",
    "    \"\"\" Saved as Type-[ (default)\"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "def use_template(template, weld_type):\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "def offset_template(template, weld_type, offset=(0,0)):\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "## 2. Segment out each weld\n",
    "\n",
    "## 3. Adapt Template for weld instance type\n",
    "\n",
    "## 3.5 Compute FPFH Features\n",
    "\n",
    "## 4. Global Registration (Mean | Median | Box Centers | RANSAC Global Registration | Fast Global Registration)\n",
    "\n",
    "## 5. Local Registration \n",
    "\n",
    "## EXTRA_1: Refactor weld instance from dictionary to class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a96912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0.exe: Initializing Model\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "model = configure_model(model_params_path)\n",
    "eps = 2\n",
    "min_samples = 20\n",
    "instance_weighting = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a2cc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  10.664317846298218\n",
      "\n",
      "create_template::scan_data.shape\n",
      "(2876, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 1.exe: Creating Template\n",
    "scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "templates = create_template(model, scan_path, tag=\"TEMPLATE\", dir_path=\"./plots_tests_2/\")\n",
    "\n",
    "if len(templates) == 2:\n",
    "    weld_source_right = templates[0][\"at_mid\"][:, :2]\n",
    "else:\n",
    "    weld_source_right = templates[\"at_mid\"][:, :2]\n",
    "weld_source_left = weld_source_right * -1\n",
    "\n",
    "weld_source_left = np.hstack((weld_source_left, np.zeros((weld_source_left.shape[0], 1))))\n",
    "weld_source_right = np.hstack((weld_source_right, np.zeros((weld_source_right.shape[0], 1))))\n",
    "\n",
    "## Checking by visualizing\n",
    "save_cluster_plots(weld_source_left, weld_source_left[:, 2], \\\n",
    "                       title=\"left_facing_weld_template\", dir_path=\"./plots_tests_2/\")\n",
    "                   \n",
    "save_cluster_plots(weld_source_right, weld_source_right[:, 2], \\\n",
    "                       title=\"right_facing_weld_template\", dir_path=\"./plots_tests_2/\")\n",
    "\n",
    "templates_dict = {\n",
    "    \"[\": {\"xyz\": weld_source_right}, \n",
    "    \"]\": {\"xyz\": weld_source_left}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ed81ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Segment out each weld\n",
    "## Generate a list of welds after processing a scan\n",
    "def get_weld_instances(model, scan_path, tag=\"\", dir_path=None):\n",
    "    \n",
    "    ## preclustering: inference, filter out background, save plots\n",
    "    scan_data, metaA = run_preclustering(model, scan_path, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6] \n",
    "    print(\"get_weld_instances::scan_data.shape\")\n",
    "    print(scan_data.shape)\n",
    "    scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "    ## postclustering: filter out noise and merged data, limit cluster \n",
    "    ## count to count from instance predictions, majority votes and \n",
    "    ## add to scan_data [cols: 6->8], save plots\n",
    "    scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ### splitting for weld instances\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=tag)\n",
    "    \n",
    "    \n",
    "    ## Visualizing each weld separately if a dir_path is spec'd\n",
    "    if dir_path:\n",
    "        for weld in welds:\n",
    "            save_cluster_plots(weld[\"sub\"], np.zeros((weld[\"sub\"].shape[0], 1)), \\\n",
    "                       title=weld[\"tag\"], dir_path=dir_path)\n",
    "            \n",
    "    return welds\n",
    "    \n",
    "## 3. Match template to weld instance by type\n",
    "# templates_dict = {\"[\": weld_source_right, \"]\": weld_source_left}\n",
    "def get_matching_template(weld, templates_dict):\n",
    "    return templates_dict[weld[\"type\"]]\n",
    "\n",
    "## 3.5 Compute FPFH Features as part of weld instance\n",
    "## Transforming the points of cloud to projections on the same 0-plane\n",
    "def compute_FPFH_for_weld_points_2D(weld, voxel_size):\n",
    "    \"\"\"\n",
    "    Downsamples, by specified. Units: mm\n",
    "    Computes a 33-d vector for each downsampling that describes the local geometry of the point\n",
    "    \"\"\"\n",
    "    ## Projecting points to the z=0 plane\n",
    "    xyz = np.zeros((weld[\"sub\"].shape[0], 3))\n",
    "    xyz[:,:2 ] = weld[\"sub\"][:,:2 ]\n",
    "    weld[\"xyz\"] = xyz\n",
    "    \n",
    "    ## Computing Downsampling and FastPointFeatureHistogram of weld\n",
    "    weld[\"down\"], weld[\"fpfh\"] = compute_FPFH(xyz, voxel_size)\n",
    "    \n",
    "    \n",
    "def compute_FPFH(xyz, voxel_size):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    \n",
    "    ## Downsampling by specified voxel-size, mm\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "    \n",
    "    ## Computing normals of downsampling\n",
    "    radius_normal = voxel_size*2\n",
    "    pcd_down.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "    \n",
    "    ## Computing Fast Point Feature Histograms\n",
    "    radius_feature = voxel_size*5\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(pcd_down, \n",
    "                                o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    \n",
    "    return pcd_down, pcd_fpfh\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59bbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0585ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Global Registration (Mean | Median | Box Centers ie mids | RANSAC Global Registration | Fast Global Registration)\n",
    "## Getting and Initial Transfrom to feed into and initialize the local registration\n",
    "## Mean, Median, Mids are already precalculated as entries in the welds' dictionaries \n",
    "##   via x_mean, y_mean; x_median, y_median; x_min, x_max, y_min, y_max\n",
    "## Functions for RANdom SAmpling Concensus and Fast Global Registration by FPFH \n",
    "##   to return a 4x4 transfrom that roughly moves the re-zeroed source to the given weld instance \n",
    "\n",
    "# def global_reg_by_RANSAC(template_down, weld_down, template_fpfh, weld_fpfh, voxel_size):\n",
    "def global_reg_by_RANSAC(template, weld, voxel_size):\n",
    "    tmpl_down, tmpl_fpfh = template[\"down\"], template[\"fpfh\"]\n",
    "    weld_down, weld_fpfh = weld[\"down\"], weld[\"fpfh\"]\n",
    "    \n",
    "    dist_threshold = voxel_size*1.5\n",
    "    \n",
    "    ## Args: source, target, source_feature, target_feature, mutual_filter, \n",
    "    ##     max_correspondence_dist, estimation, ransac_n ie sampling size, \n",
    "    ##     correspondence_checkers ie pruning algos, convergence_criteria\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        tmpl_down, weld_down, tmpl_fpfh, weld_fpfh, True, \n",
    "        dist_threshold, o3d.pipelines.registration.TransformationEstimationPointToPoint(False), 3, \n",
    "        [o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.9),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                dist_threshold)], o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999))\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "\n",
    "def global_reg_by_FGR(template, weld, voxel_size):\n",
    "    tmpl_down, tmpl_fpfh = template[\"down\"], template[\"fpfh\"]\n",
    "    weld_down, weld_fpfh = weld[\"down\"], weld[\"fpfh\"]\n",
    "    \n",
    "    dist_threshold = voxel_size*1.5\n",
    "    \n",
    "    ## Args: source, target, source_feature, target_feature, fgr_options\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "                tmpl_down, weld_down, tmpl_fpfh, weld_fpfh, \n",
    "                o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "                    maximum_correspondence_distance=dist_threshold))\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "        \n",
    "def global_reg_by_mean(template, weld):\n",
    "    transform = np.identity(4)\n",
    "    transform[:3, -1] = np.array([weld[\"x_mean\"], weld[\"y_mean\"], 0])\n",
    "    return transform, None, None, None\n",
    "\n",
    "def global_reg_by_median(template, weld):\n",
    "    transform = np.identity(4)\n",
    "    transform[:3, -1] = np.array([weld[\"x_median\"], weld[\"y_median\"], 0])\n",
    "    return transform, None, None, None\n",
    "    \n",
    "def global_reg_by_mid(template, weld):\n",
    "    transform = np.identity(4)\n",
    "    x_mid = (weld[\"x_min\"] + weld[\"x_max\"])/2\n",
    "    y_mid = (weld[\"y_min\"] + weld[\"y_max\"])/2\n",
    "    transform[:3, -1] = np.array([x_mid, y_mid, 0])\n",
    "    return transform, None, None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af4ee2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Local Registration (Point-to-Point ICP vs Point-to-Plane ICP)\n",
    "## With an initial transform guess, refine guess until within stopping criteria\n",
    "## The refined transform is to be deconstructed for the finer x and y and yaw potentially \n",
    "##  of the center of each weld, which are to collected for output to a temp file\n",
    "## To be verified by the GNN UI\n",
    "\n",
    "def point_to_point_ICP(template_xyz, weld_xyz, init_transform, voxel_size):\n",
    "    dist_threshold = voxel_size*0.4\n",
    "    \n",
    "    ## Args: source, target, max_correspondence_distance, init_transform, \n",
    "    ##     estimation method, convergence criteria\n",
    "    result = o3d.pipelines.registration.registration_icp(template_xyz, weld_xyz, dist_threshold, init_transform,\n",
    "                                                        o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "\n",
    "def point_to_plane_ICP(template_xyz, weld_xyz, init_transform, voxel_size):\n",
    "    dist_threshold = voxel_size*0.4\n",
    "    \n",
    "    ## Args: source, target, max_correspondence_distance, init_transform, \n",
    "    ##     estimation method, convergence criteria\n",
    "    result = o3d.pipelines.registration.registration_icp(template_xyz, weld_xyz, dist_threshold, init_transform,\n",
    "                                                        o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "\n",
    "def transform_to_xyztheta(transform):\n",
    "    \n",
    "    ## X, y, theta from transform\n",
    "    x = transform[0, -1]\n",
    "    y = transform[1, -1]\n",
    "    z = transform[2, -1]\n",
    "    theta = np.arccos(transform[0,0])\n",
    "    \n",
    "    return x,y,z,theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f0bf0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  9.50768494606018\n",
      "\n",
      "get_weld_instances::scan_data.shape\n",
      "(2899, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2.exe: Collecting welds to loop through\n",
    "scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "welds = get_weld_instances(model, scan_path, tag=\"TEST\", dir_path=\"./plots_tests_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e56af82a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weld ID:  1\n",
      "Weld type:  [\n",
      "Weld tag:  TEST_weld1\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.5301837270341208 0.19672737004831298\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.5021872265966754 0.20083992012417337\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (-75.45001602172852, 12.424999376758933, 0.0, 0.0)\n",
      "-- -- FGR:  (-76.59411868778508, 13.496627766971184, -0.0, 0.18501002694000118)\n",
      "-- -- mean:  (-76.70888387974229, 11.858549511060076, 0.0, 0.0)\n",
      "-- -- median:  (-76.70000457763672, 11.549999713897705, 0.0, 0.0)\n",
      "-- -- mid:  (-76.40000915527344, 11.87499962002039, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.05653170359052712 0.05142112301461699\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.04583651642475172 0.05400742973371009\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.051098731406921276\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05357055503310873\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.0748663101604278 0.05305645324772217\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (-75.43996660924583, 12.439386945059965, 0.0, 0.0006688827382727644)\n",
      "-- -- FGR:  (-76.58842978697113, 13.49385440422537, 0.0, 0.18405940504368032)\n",
      "-- -- mean:  (-76.63408824246494, 11.862620168082591, 0.0, 0.00035699788005462397)\n",
      "-- -- median:  (-76.6989157690891, 11.552046547860346, 0.0, 0.0010146407944573057)\n",
      "-- -- mid:  (-76.42277214183724, 11.889519636161547, 0.0, 0.0011980810462876052)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.05653170359052712 0.05142112301461696\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.04583651642475172 0.05400742973371015\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05109873140692078\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05357055503310855\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.0748663101604278 0.0530564532477236\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (-75.43996660924583, 12.439386945059967, 0.0, 0.0006688827382727644)\n",
      "-- -- FGR:  (-76.58842978697115, 13.493854404225365, 0.0, 0.1840594050436797)\n",
      "-- -- mean:  (-76.63408824246497, 11.862620168082593, 0.0, 0.00035699788005462397)\n",
      "-- -- median:  (-76.69891576908908, 11.552046547860344, 0.0, 0.0010146407944573057)\n",
      "-- -- mid:  (-76.42277214183726, 11.889519636161545, 0.0, 0.0011980810458242712)\n",
      "\n",
      "Weld ID:  2\n",
      "Weld type:  [\n",
      "Weld tag:  TEST_weld2\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.600174978127734 0.1944567610210616\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.657917760279965 0.19885350645826075\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (16.824483766794657, 12.574719799390973, 0.0, 0.08410540625655188)\n",
      "-- -- FGR:  (16.973724961590932, 13.061283494972642, -0.0, 0.0)\n",
      "-- -- mean:  (16.671554852907477, 12.78057075621417, 0.0, 0.0)\n",
      "-- -- median:  (16.49999237060547, 12.337499976158142, 0.0, 0.0)\n",
      "-- -- mid:  (17.149993896484375, 12.874999612569809, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.052614641488350246\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07944996180290298 0.050208857942080326\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.06340718105423988 0.04958382041375862\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061115355233002294 0.054983793934947574\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.04355504448922343\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (16.849469432221117, 12.556750864174917, 0.0, 0.07714087613185858)\n",
      "-- -- FGR:  (16.969657553014414, 12.97026654409567, 0.0, 0.0036855921794863994)\n",
      "-- -- mean:  (16.692367315112786, 12.7606167122272, 0.0, 0.0005644868118809546)\n",
      "-- -- median:  (16.52232450411378, 12.338674700908584, 0.0, 0.0021623135594923157)\n",
      "-- -- mid:  (17.14011695922101, 12.951869118614445, 0.0, 8.487226064488343e-05)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.052614641488349614\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07944996180290298 0.05020885794208041\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.06340718105423988 0.04958382041375935\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061115355233002294 0.05498379393494659\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.043555044489222525\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (16.84946943222112, 12.556750864174917, 0.0, 0.07714087613186145)\n",
      "-- -- FGR:  (16.969657553014418, 12.97026654409567, 0.0, 0.0036855921794863994)\n",
      "-- -- mean:  (16.692367315112776, 12.760616712227202, 0.0, 0.0005644868118809546)\n",
      "-- -- median:  (16.52232450411378, 12.338674700908582, 0.0, 0.0021623135594409713)\n",
      "-- -- mid:  (17.140116959221018, 12.951869118614448, 0.0, 8.487226326110456e-05)\n",
      "\n",
      "Weld ID:  3\n",
      "Weld type:  [\n",
      "Weld tag:  TEST_weld3\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.6124234470691163 0.19542132982949448\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.7069116360454943 0.19575472730534949\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (27.80436679392755, 11.705985116905547, 0.0, 0.03131120281064451)\n",
      "-- -- FGR:  (28.631504433617344, 11.312664834777326, -0.0, 0.0)\n",
      "-- -- mean:  (28.34323322847954, 10.96906514373809, 0.0, 0.0)\n",
      "-- -- median:  (28.199996948242188, 9.349999904632568, 0.0, 0.0)\n",
      "-- -- mid:  (28.50999221801758, 11.774999616667628, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.06799083269671505 0.05706802681493617\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.05394384749990139\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08097784568372804 0.054013419707839676\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.060351413292589765 0.048809877785348864\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.054298270084972525\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (27.82497310417815, 11.698031663571522, 0.0, 0.03098750249256388)\n",
      "-- -- FGR:  (28.682332926983545, 11.338269367699226, 0.0, 0.003517458400574025)\n",
      "-- -- mean:  (28.335598633382915, 10.981644700195433, 0.0, 0.003292993718726247)\n",
      "-- -- median:  (28.1449951455654, 9.34716859024786, 0.0, 0.0008451531269451214)\n",
      "-- -- mid:  (28.470659621154848, 11.714265462657147, 0.0, 0.008242266344548364)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.06799083269671505 0.057068026814935724\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.053943847499901346\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08097784568372804 0.05401341970783947\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.060351413292589765 0.0488098777853489\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.05429827008497165\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (27.82497310417816, 11.69803166357152, 0.0, 0.030987502492571047)\n",
      "-- -- FGR:  (28.68233292698355, 11.338269367699231, 0.0, 0.003517458400542462)\n",
      "-- -- mean:  (28.335598633382922, 10.981644700195428, 0.0, 0.003292993718726247)\n",
      "-- -- median:  (28.144995145565403, 9.347168590247858, 0.0, 0.000845153125894213)\n",
      "-- -- mid:  (28.470659621154844, 11.714265462657146, 0.0, 0.008242266344561834)\n",
      "\n",
      "Weld ID:  4\n",
      "Weld type:  ]\n",
      "Weld tag:  TEST_weld4\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.5701754385964912 0.19711524715052586\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.5166666666666667 0.19541327494607993\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (71.67499351501465, 13.625000268220901, 0.0, 0.0)\n",
      "-- -- FGR:  (71.20431826477912, 11.697374964755028, -0.0, 3.0099966977802444)\n",
      "-- -- mean:  (70.96786276086586, 12.194696806943892, 0.0, 0.0)\n",
      "-- -- median:  (71.09999338785808, 11.75, 0.0, 0.0)\n",
      "-- -- mid:  (70.64999389648438, 12.583333214124043, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.054239877769289534 0.05447898044243861\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05547527460527819\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.09014514896867838 0.05215179761573007\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.07104660045836517 0.05186404339114506\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.08326967150496563 0.05472166278894629\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (71.63742372452569, 13.585887943137637, 0.0, 0.0009124890012384704)\n",
      "-- -- FGR:  (71.2027642076742, 11.70310773962994, 0.0, 3.0100695987564063)\n",
      "-- -- mean:  (70.98964435716057, 12.188150122187245, 0.0, 0.0029450297122656736)\n",
      "-- -- median:  (71.03628551341774, 11.768254256651236, 0.0, 0.00047213977667327723)\n",
      "-- -- mid:  (70.64437483252466, 12.58036940018812, 0.0, 0.000134721399279547)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.054239877769289534 0.05447898044243791\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.055475274605278284\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.09014514896867838 0.05215179761573036\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.07104660045836517 0.051864043391145226\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.08326967150496563 0.05472166278894613\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (71.63742372452569, 13.585887943137639, 0.0, 0.0009124890017251495)\n",
      "-- -- FGR:  (71.20276420767422, 11.703107739629942, 0.0, 3.0100695987564063)\n",
      "-- -- mean:  (70.9896443571606, 12.188150122187245, 0.0, 0.002945029712227975)\n",
      "-- -- median:  (71.03628551341778, 11.768254256651234, 0.0, 0.00047213977784901285)\n",
      "-- -- mid:  (70.64437483252468, 12.580369400188117, 0.0, 0.000134721399279547)\n",
      "\n",
      "Weld ID:  5\n",
      "Weld type:  ]\n",
      "Weld tag:  TEST_weld5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.6236842105263158 0.186049182945404\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.5447368421052632 0.1942277448735041\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (82.25587864390668, 12.872363823858846, 0.0, 0.23955024844256692)\n",
      "-- -- FGR:  (82.31270094346942, 15.762220305855932, -0.0, 3.1075408738070633)\n",
      "-- -- mean:  (82.30348323382707, 14.210766498392717, 0.0, 0.0)\n",
      "-- -- median:  (82.44999694824219, 14.47499966621399, 0.0, 0.0)\n",
      "-- -- mid:  (81.87499364217123, 14.149999856948853, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.05618690162437299\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.06417112299465241 0.052814018116671364\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08403361344537816 0.05562431414638576\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.09472880061115355 0.05446143087022958\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.10236822001527884 0.054711404415762305\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (82.22157309608335, 12.864001009479368, 0.0, 0.23192201929794887)\n",
      "-- -- FGR:  (82.31837413810189, 15.669245638107292, 0.0, 3.111927098476774)\n",
      "-- -- mean:  (82.29202583722454, 14.209322029732808, 0.0, 0.0014139411819621035)\n",
      "-- -- median:  (82.41699957600899, 14.384563372233718, 0.0, 0.0025974131461868736)\n",
      "-- -- mid:  (81.83958117037615, 14.1860332322698, 0.0, 0.0006773802274928631)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.056186901624374355\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.06417112299465241 0.052814018116671725\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08403361344537816 0.055624314146385814\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.09472880061115355 0.05446143087023\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.10236822001527884 0.054711404415762686\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (82.22157309608338, 12.864001009479365, 0.0, 0.2319220192979479)\n",
      "-- -- FGR:  (82.31837413810192, 15.669245638107292, 0.0, 3.111927098476774)\n",
      "-- -- mean:  (82.29202583722454, 14.209322029732812, 0.0, 0.0014139411819621035)\n",
      "-- -- median:  (82.41699957600898, 14.384563372233714, 0.0, 0.0025974131458876694)\n",
      "-- -- mid:  (81.83958117037614, 14.1860332322698, 0.0, 0.0006773802266733654)\n",
      "\n",
      "Weld ID:  6\n",
      "Weld type:  ]\n",
      "Weld tag:  TEST_weld6\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.6412280701754386 0.19218865308466643\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.6938596491228071 0.19328465254457688\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (92.53526896720311, 15.618580888585516, 0.0, 0.008261546666379679)\n",
      "-- -- FGR:  (92.47648903524241, 14.154685745482137, -0.0, 0.0)\n",
      "-- -- mean:  (92.7885320432354, 14.57595016570143, 0.0, 0.0)\n",
      "-- -- median:  (93.29999542236328, 15.25, 0.0, 0.0)\n",
      "-- -- mid:  (92.19166056315103, 13.949999928474426, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.0718105423987777 0.050338712047594915\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.08938120702826585 0.05088631292693545\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.07868601986249045 0.055870921328772895\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.05576776165011459 0.057238398227506225\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.054988682094888594\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (92.53681601847202, 15.598588116384985, 0.0, 0.005750083264880892)\n",
      "-- -- FGR:  (92.44292923828375, 14.180012164465744, 0.0, 0.0010932542779134146)\n",
      "-- -- mean:  (92.81893836284716, 14.579020290573085, 0.0, 0.0032352800821497303)\n",
      "-- -- median:  (93.29155606662519, 15.235808504655145, 0.0, 0.00010435138389263829)\n",
      "-- -- mid:  (92.19970258518362, 13.966947640421056, 0.0, 0.0014368700623996313)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.0718105423987777 0.05033871204759545\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.08938120702826585 0.05088631292693499\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.07868601986249045 0.0558709213287744\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.05576776165011459 0.05723839822750642\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.054988682094888774\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (92.53681601847204, 15.598588116384986, 0.0, 0.005750083264842275)\n",
      "-- -- FGR:  (92.44292923828368, 14.180012164465753, 0.0, 0.0010932542779134146)\n",
      "-- -- mean:  (92.81893836284715, 14.579020290573085, 0.0, 0.003235280081978149)\n",
      "-- -- median:  (93.29155606662519, 15.235808504655152, 0.0, 0.00010435138602049319)\n",
      "-- -- mid:  (92.19970258518366, 13.966947640421054, 0.0, 0.0014368700623996313)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3,4,5.exe : Looping through collected welds\n",
    "\n",
    "voxel_size = 0.2\n",
    "\n",
    "for idx, weld in enumerate(welds, start=1):\n",
    "    print(\"Weld ID: \", weld[\"id\"])\n",
    "    print(\"Weld type: \", weld[\"type\"])\n",
    "    print(\"Weld tag: \", weld[\"tag\"]) \n",
    "    print()\n",
    "    \n",
    "    ## 3. Adapt Template for weld instance type\n",
    "    template = get_matching_template(weld, templates_dict)\n",
    "    \n",
    "    ## 3.5 Compute FPFH Features\n",
    "    ## For weld\n",
    "    compute_FPFH_for_weld_points_2D(weld, voxel_size)\n",
    "    ## For Template\n",
    "    template_down, template_fpfh = compute_FPFH(template[\"xyz\"], voxel_size)\n",
    "    template[\"down\"], template[\"fpfh\"] = template_down, template_fpfh\n",
    "    \n",
    "    ## 4. Global Registration (Mean | Median | Box Centers | RANSAC Global Registration | Fast Global Registration)\n",
    "    T_RANSAC, fitness_RANSAC, inlier_rmse_RANSAC, _ = global_reg_by_RANSAC(template, weld, voxel_size)\n",
    "    T_FGR, fitness_FGR, inlier_rmse_FGR, _ = global_reg_by_FGR(template, weld, voxel_size)\n",
    "    T_mean, _, _, _ = global_reg_by_mean(template, weld)\n",
    "    T_median, _, _, _ = global_reg_by_median(template, weld)\n",
    "    T_mid, _, _, _ = global_reg_by_mid(template, weld)\n",
    "    \n",
    "    print(\"Global Registration\")\n",
    "    print(\"-- RANSAC\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", fitness_RANSAC, inlier_rmse_RANSAC)\n",
    "    print(\"-- FGR\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", fitness_FGR, inlier_rmse_FGR)\n",
    "    print()\n",
    "    print(\"-- X|Y|Z|theta\")\n",
    "    print(\"-- -- RANSAC: \", transform_to_xyztheta(T_RANSAC))\n",
    "    print(\"-- -- FGR: \", transform_to_xyztheta(T_FGR))\n",
    "    print(\"-- -- mean: \", transform_to_xyztheta(T_mean))\n",
    "    print(\"-- -- median: \", transform_to_xyztheta(T_median))\n",
    "    print(\"-- -- mid: \", transform_to_xyztheta(T_mid))\n",
    "    print()\n",
    "    \n",
    "    ## 5. Local Registration: Refining\n",
    "    \n",
    "    ## Represent as Point Cloud\n",
    "    \n",
    "    weld_xyz = o3d.geometry.PointCloud()\n",
    "    weld_xyz.points = o3d.utility.Vector3dVector(weld[\"xyz\"])\n",
    "    \n",
    "    template_xyz = o3d.geometry.PointCloud()\n",
    "    template_xyz.points = o3d.utility.Vector3dVector(template[\"xyz\"])\n",
    "    \n",
    "    ## Point to Point ICP using T_RANSAC, T_FGR, T_mean, F_median, F_mid respectively as Initial Transforms \n",
    "    \n",
    "    f_T_p2p_RANSAC, f_fitness_p2p_RANSAC, f_inlier_rmse_p2p_RANSAC, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_RANSAC, voxel_size)\n",
    "    \n",
    "    f_T_p2p_FGR, f_fitness_p2p_FGR, f_inlier_rmse_p2p_FGR, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_FGR, voxel_size)\n",
    "    \n",
    "    f_T_p2p_mean, f_fitness_p2p_mean, f_inlier_rmse_p2p_mean, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mean, voxel_size)\n",
    "    \n",
    "    f_T_p2p_median, f_fitness_p2p_median, f_inlier_rmse_p2p_median, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_median, voxel_size)\n",
    "    \n",
    "    f_T_p2p_mid, f_fitness_p2p_mid, f_inlier_rmse_p2p_mid, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mid, voxel_size)\n",
    "    \n",
    "    print(\"Local Registration: Point 2 Point \")\n",
    "    print(\"-- RANSAC\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_RANSAC, f_inlier_rmse_p2p_RANSAC)\n",
    "    print(\"-- FGR\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_FGR, f_inlier_rmse_p2p_FGR)\n",
    "    print(\"-- mean\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_mean, f_inlier_rmse_p2p_mean)\n",
    "    print(\"-- median\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_median, f_inlier_rmse_p2p_median)\n",
    "    print(\"-- mid\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_mid, f_inlier_rmse_p2p_mid)\n",
    "    print()\n",
    "    print(\"-- X|Y|Z|theta\")\n",
    "    print(\"-- -- RANSAC: \", transform_to_xyztheta(f_T_p2p_RANSAC))\n",
    "    print(\"-- -- FGR: \", transform_to_xyztheta(f_T_p2p_FGR))\n",
    "    print(\"-- -- mean: \", transform_to_xyztheta(f_T_p2p_mean))\n",
    "    print(\"-- -- median: \", transform_to_xyztheta(f_T_p2p_median))\n",
    "    print(\"-- -- mid: \", transform_to_xyztheta(f_T_p2p_mid))\n",
    "    print()\n",
    "    \n",
    "    ## Point to Point ICP using T_RANSAC, T_FGR, T_mean, F_median, F_mid respectively as Initial Transforms \n",
    "    \n",
    "    f_T_p2pl_RANSAC, f_fitness_p2pl_RANSAC, f_inlier_rmse_p2pl_RANSAC, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_RANSAC, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_FGR, f_fitness_p2pl_FGR, f_inlier_rmse_p2pl_FGR, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_FGR, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_mean, f_fitness_p2pl_mean, f_inlier_rmse_p2pl_mean, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mean, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_median, f_fitness_p2pl_median, f_inlier_rmse_p2pl_median, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_median, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_mid, f_fitness_p2pl_mid, f_inlier_rmse_p2pl_mid, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mid, voxel_size)\n",
    "    \n",
    "    print(\"Local Registration: Point 2 Plane \")\n",
    "    print(\"-- RANSAC\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_RANSAC, f_inlier_rmse_p2pl_RANSAC)\n",
    "    print(\"-- FGR\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_FGR, f_inlier_rmse_p2pl_FGR)\n",
    "    print(\"-- mean\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_mean, f_inlier_rmse_p2pl_mean)\n",
    "    print(\"-- median\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_median, f_inlier_rmse_p2pl_median)\n",
    "    print(\"-- mid\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_mid, f_inlier_rmse_p2pl_mid)\n",
    "    print()\n",
    "    print(\"-- X|Y|Z|theta\")\n",
    "    print(\"-- -- RANSAC: \", transform_to_xyztheta(f_T_p2pl_RANSAC))\n",
    "    print(\"-- -- FGR: \", transform_to_xyztheta(f_T_p2pl_FGR))\n",
    "    print(\"-- -- mean: \", transform_to_xyztheta(f_T_p2pl_mean))\n",
    "    print(\"-- -- median: \", transform_to_xyztheta(f_T_p2pl_median))\n",
    "    print(\"-- -- mid: \", transform_to_xyztheta(f_T_p2pl_mid))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "805a57c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'at_mean': array([[ 0.47552498, -4.87945503,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 2.92552574, -4.22945493,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.72552116, -9.029455  ,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 3.48632034,  8.00074502,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 4.13631805,  8.25074534,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [-0.96368042, 11.30074521,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]]),\n",
       "  'at_median': array([[ 0.54999924, -4.30000019,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.        , -3.6500001 ,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.79999542, -8.45000017,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 3.59999847, 10.4000001 ,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 4.24999619, 10.65000041,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [-0.85000229, 13.70000029,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]]),\n",
       "  'at_mid': array([[ 0.25000381, -5.05000029,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 2.70000458, -4.40000019,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.5       , -9.20000026,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 3.35000229,  7.27499994,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 4.        ,  7.52500026,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [-1.09999847, 10.57500013,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]]),\n",
       "  'at_corner': array([[ 4.35000229,  6.55      ,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 6.80000305,  7.20000009,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 7.59999847,  2.40000002,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 7.35000229, 18.80000019,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 8.        , 19.05000051,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 2.90000153, 22.10000039,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]])},\n",
       " {'at_mean': array([[-1.99669032,  5.67663618,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.30331273, -5.47336392,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 0.85331578, -2.07336414,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [-2.17007287, 10.02996473,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.97006829, 10.17996435,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.52007135, 10.37996416,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]]),\n",
       "  'at_median': array([[-2.30000305,  6.4000001 ,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.        , -4.75      ,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 0.55000305, -1.35000022,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [-2.65000153,  9.40000057,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-1.44999695,  9.55000019,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-1.        ,  9.75      ,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]]),\n",
       "  'at_mid': array([[-1.67499924,  5.20000029,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.62500381, -5.94999981,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.17500687, -2.55000003,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [-1.72500229, 10.55000067,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.52499771, 10.70000029,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.07500076, 10.9000001 ,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]]),\n",
       "  'at_corner': array([[ 2.34999847, 16.60000038,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 5.65000153,  5.45000029,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 5.20000458,  8.85000006,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [ 2.34999847, 22.45000076,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [ 3.55000305, 22.60000038,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [ 4.        , 22.80000019,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]])})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc4252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
