{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04da5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring sklearn's DBSCAN\n",
    "## Goal: For multiple samples, Across different param combos, focussing on different data slices \n",
    "##       --> Compile splits, compile plots\n",
    "## Specify reasoned parameter ranges, for each data slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b4fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os, errno\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Instseg_model import MultiLayerFastLocalGraphModelV2 as model1\n",
    "from dataset import pcloader\n",
    "from graph_generation import gen_multi_level_local_graph_v3\n",
    "\n",
    "from math import floor, ceil\n",
    "from scipy.stats import mode\n",
    "\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfaac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import add_staple_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION INFERENCE\n",
    "##  - High Level Config Settings\n",
    "##  - Singular Function to get predictions\n",
    "##  - Post Processing for Non Background points\n",
    "##  - Plot and Save Both Predictions\n",
    "\n",
    "## LOOPING THROUGH DIFFERENT CLUSTERING RUNS FOR EACH DATA SLICE\n",
    "##  - High Level Config Setting\n",
    "##  - Single Function for Run through param matrix --> log and Plots for all samples\n",
    "##     - Plots in the Same Folder\n",
    "##     - CSV of Cluster and Cluster Count\n",
    "\n",
    "## ORGANIZING OUTPUT DATA\n",
    "## Single CSV : FileName, SampleTag, slice, param1, param2, paramX,-1_count, 0_count, etc\n",
    "## Directory for Plots prefixed with SampleTag + DataColHeaders + paramCombo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba0e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFICATION INFERENCE\n",
    "\n",
    "## Hight Level Config Settings\n",
    "\n",
    "graph_gen_kwargs = {\n",
    "\t'add_rnd3d': True,\n",
    "\t'base_voxel_size': 0.8,\n",
    "\t'downsample_method': 'random',\n",
    "\t'level_configs': [\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 64, 'radius': 0.4},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 0,\n",
    "\t\t 'graph_scale': 1},\n",
    "\t\t{'graph_gen_kwargs': {'num_neighbors': 192, 'radius': 1.2},\n",
    "\t\t 'graph_gen_method': 'disjointed_rnn_local_graph_v3',\n",
    "\t\t 'graph_level': 1,\n",
    "\t\t 'graph_scale': 1}]\n",
    "}\n",
    "\n",
    "def configure_model(model_params_path, max_cls_classes=3, max_inst_classes=7, verbose=False):\n",
    "    a = time.time()\n",
    "    model = model1(num_classes=max_cls_classes, max_instance_no=max_inst_classes)\n",
    "    if os.path.isfile(model_params_path):\n",
    "        model.load_state_dict(torch.load(model_params_path))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"[ModelParamPathError] {model_params_path} does not exist\")\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), model_params_path)\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(f\"Model Setup Time (secs) : {b-a}\")\n",
    "    return model\n",
    "\n",
    "def classify_scan(model, scan_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns \n",
    "     - the x|y|z|cls|inst for all keypoints in the cloud as a Kx5 array\n",
    "     - and also as a dict\n",
    "    \"\"\"\n",
    "    a = time.time()\n",
    "    pointxyz, offset = pcloader(scan_path)\n",
    "    vertex_coord_list, keypoint_indices_list, edges_list = \\\n",
    "    gen_multi_level_local_graph_v3(pointxyz,0.6,graph_gen_kwargs['level_configs'])\n",
    "    last_layer_v = vertex_coord_list[-1]\n",
    "\n",
    "    ## conversions: type precision\n",
    "    vertex_coord_list = [p.astype(np.float32) for p in vertex_coord_list]\n",
    "    keypoint_indices_list = [e.astype(np.int32) for e in keypoint_indices_list]\n",
    "    edges_list = [e.astype(np.int32) for e in edges_list]\n",
    "\n",
    "    ## conversions: numpy array to tensor\n",
    "    vertex_coord_list = [torch.from_numpy(item) for item in vertex_coord_list]\n",
    "    keypoint_indices_list = [torch.from_numpy(item).long() for item in keypoint_indices_list]\n",
    "    edges_list = [torch.from_numpy(item).long() for item in edges_list]\n",
    "\n",
    "    ## Run graph through GNN model\n",
    "    batch = (vertex_coord_list, keypoint_indices_list, edges_list)\n",
    "    cls_seg, inst_seg = model(*batch)\n",
    "\n",
    "    ## Filter classification probabilities for the most probable\n",
    "    cls_preds = torch.argmax(cls_seg, dim=1)\n",
    "    inst_preds = torch.argmax(inst_seg, dim=1)\n",
    "    \n",
    "    ## expand the shape of the array\n",
    "    cls_preds = np.expand_dims(cls_preds, axis=1)\n",
    "    inst_preds = np.expand_dims(inst_preds, axis=1)\n",
    "\n",
    "    b = time.time()\n",
    "    if verbose:\n",
    "        print(\"Scan Inference Time (secs): \", b-a)\n",
    "        print()\n",
    "    \n",
    "    return np.hstack((last_layer_v, cls_preds, inst_preds)),\\\n",
    "            {'vertices': last_layer_v, 'cls_preds': cls_preds, 'inst_preds': inst_preds}\n",
    "\n",
    "def filter_out_background(scan_data):\n",
    "    non_bg_idx = ~np.logical_or(scan_data[:, 3] == 0, scan_data[:, 4] == 0)\n",
    "    non_bg = scan_data[non_bg_idx]\n",
    "    return non_bg, non_bg_idx\n",
    "\n",
    "def count_cluster_by_instance_prediction(scan_data, threshold_factor=0.5):\n",
    "    \"\"\"\n",
    "    Returns a cluster_count ie the number of clusters on the cls field/col that has at least a threshold number of members\n",
    "    Also: a dict of intermediary/final counts, types and thresholds\n",
    "    \"\"\"\n",
    "    inst, inst_count = np.unique(scan_data[:,4], return_counts=True)\n",
    "    inst_count_threshold = threshold_factor * np.mean(inst_count)\n",
    "    reduced_idx = np.where(inst_count > inst_count_threshold)\n",
    "    cluster_count = np.sum(inst_count > inst_count_threshold)\n",
    "    reduced = inst[reduced_idx]\n",
    "    \n",
    "    return cluster_count, {'orig_insts': inst,\n",
    "                           'orig_inst_ct': inst_count,\n",
    "                           'inst_ct_thresh': inst_count_threshold,\n",
    "                           'reduced_insts': reduced}\n",
    "\n",
    "def _plot(path, points, values, title=\"\", caption=\"\", overlay_points=None ):\n",
    "    \"\"\"\n",
    "    path - output path\n",
    "    points - matrix of x, y, etc cols\n",
    "    values - assigned value per point ie predicted/truth cls/inst\n",
    "    [optional] overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "    im = ax.scatter(points[:,0], points[:,1],s=0.25,c=values)\n",
    "    \n",
    "#     _im = None\n",
    "    if overlay_points:    \n",
    "        for pts in overlay_points:\n",
    "#             _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=2, c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "            _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=pts[\"ss\"], c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "#             ims.append(_im)\n",
    "            \n",
    "        \n",
    "\n",
    "    ax.set_xlabel(\"x [mm]\")\n",
    "    ax.set_ylabel(\"y [mm]\")\n",
    "# #     leg_els = im.legend_elements()\n",
    "# #     if _im:\n",
    "# #         leg_els += _im.legend_elements()\n",
    "# #         print(\"leg_els\")\n",
    "# #         print(leg_els)\n",
    "# #         print(\"_im.legend_elements()\")\n",
    "# #         print(_im.legend_elements())\n",
    "# #         print(\"im.legend_elements()\")\n",
    "# #         print(im.legend_elements())\n",
    "        \n",
    "    legend_ = ax.legend(*im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "# #     legend_ = ax.legend(*leg_els, bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "    ax.add_artist(legend_)\n",
    "# #     for _im in ims:\n",
    "# #         _legend = ax.legend(*_im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "# #         ax.add_artist(_legend)\n",
    "\n",
    "    ax.text(0.5, -0.5, caption, style='italic', \\\n",
    "        horizontalalignment='center', verticalalignment='top', transform=ax.transAxes)\n",
    "    axes=plt.gca()\n",
    "    axes.set_aspect(1)\n",
    "    if path:\n",
    "        plt.savefig(path, dpi=150)\n",
    "        plt.close()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show\n",
    "    \n",
    "def save_prediction_plots(non_bg_matrix, cls_tag=None, inst_tag=None, cls_col=3, inst_col=4, dir_path=\"./plots/\", verbose=False):\n",
    "    \"\"\"\n",
    "    non_bg_matrix -- x|y|z|cls|inst\n",
    "    // tag -- eg A_xycls_eps0_45_50\n",
    "    tag -- eg A_xycls\n",
    "    \"\"\"\n",
    "    \n",
    "    ## [RED FLAG] - what if dir_path comes through as None\n",
    "    f_tag = \"[save_prediction_plots]\"\n",
    "    f_msg = []\n",
    "    \n",
    "    if not (cls_tag and inst_tag):\n",
    "        print(\"[ERROR] No specfied cls_tag or inst_tag args. Plots not generated\")\n",
    "    else:\n",
    "        if cls_tag:\n",
    "            cls_output_path = dir_path+cls_tag+\".png\" if dir_path else None\n",
    "            _plot(cls_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, cls_col], title=cls_tag)\n",
    "            f_msg.append(cls_output_path)\n",
    "\n",
    "        if inst_tag:\n",
    "            inst_output_path = dir_path+inst_tag+\".png\" if dir_path else None\n",
    "            _plot(inst_output_path, non_bg_matrix[:, :2], non_bg_matrix[:, inst_col], title=inst_tag)\n",
    "            f_msg.append(inst_output_path)\n",
    "\n",
    "        if verbose:\n",
    "#             f_msg = \", \".join(f_msg)\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "    \n",
    "\n",
    "def run_preclustering(model, scan_path, sample_tag=\"\", dir_path=\"./plots\"):\n",
    "    \"\"\"\n",
    "    Runs the scan through the model\n",
    "    Filters out the background predictions\n",
    "    Plots cls and inst predictions post filtering and saves plots to file\n",
    "    Returns the Nx5 data of non-background points --> x|y|z|cls|inst\n",
    "    \"\"\"\n",
    "    # KX5 array --> x|y|z|cls|inst\n",
    "    scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "    \n",
    "    # non bg NX5 array --> x|y|z|cls|inst\n",
    "    scan_data, _ = filter_out_background(scan_data)\n",
    "    \n",
    "    ## [RED FLAG] - No use of the cluster_count or data\n",
    "    _, cluster_data = count_cluster_by_instance_prediction(scan_data)\n",
    "    \n",
    "    ## [RED FLAG] what if dir_path specified as None. Need to catch this cas\n",
    "    save_prediction_plots(scan_data, cls_tag=sample_tag+\"_xycls\", inst_tag=sample_tag+\"_xyinst\", dir_path=dir_path)\n",
    "    return scan_data, {\"0_scan\": scan_path, \"0_tag\":sample_tag}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8eac280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup Time (secs) : 0.06800007820129395\n",
      "Scan Inference Time (secs):  10.159850120544434\n",
      "\n",
      "scan_data.shape :\n",
      "(20751, 5)\n",
      "\n",
      "scan_data.shape :\n",
      "(2892, 5)\n",
      "\n",
      "cluster_count: \n",
      "6\n",
      "\n",
      "cluster_data: \n",
      "{'orig_insts': array([1., 2., 3., 4., 5., 6.]), 'orig_inst_ct': array([425, 380, 519, 416, 601, 551], dtype=int64), 'inst_ct_thresh': 241.0, 'reduced_insts': array([1., 2., 3., 4., 5., 6.])}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test Running a sample\n",
    "\n",
    "## Run Settings\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "\n",
    "## Instantiating model and running scan through for predictions\n",
    "model = configure_model(model_params_path, verbose=True)\n",
    "scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "print(\"scan_data.shape :\")\n",
    "print(scan_data.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "## Filter out background points\n",
    "scan_data, _ = filter_out_background(scan_data)\n",
    "print(\"scan_data.shape :\")\n",
    "print(scan_data.shape)\n",
    "print()\n",
    "\n",
    "## Clustering by the instance predictions\n",
    "cluster_count, cluster_data = count_cluster_by_instance_prediction(scan_data)\n",
    "print(\"cluster_count: \")\n",
    "print(cluster_count)\n",
    "print()\n",
    "print(\"cluster_data: \")\n",
    "print(cluster_data)\n",
    "print()\n",
    "\n",
    "# ## Plotting class predictions and instance predictions\n",
    "# save_prediction_plots(scan_data, cls_tag='egA_xycls', inst_tag='egA_xyinst')\n",
    "\n",
    "img_directory = \"C:/Users/KZTYLF/Documents/playground/GNN UIs/GNN InstanceSegmentation/Recreating Dataset/_img_seg/\"\n",
    "save_prediction_plots(scan_data, cls_tag='egA_xycls', inst_tag='egA_xyinst', dir_path=img_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d06408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-17.30000305   4.74999976 -15.38999987   0.           0.        ]\n",
      " [-52.75000763  18.94999981 -14.75499988   0.           0.        ]\n",
      " [ 29.23332977  -5.93333387   0.40666666   0.           0.        ]\n",
      " ...\n",
      " [ -3.1000061   27.79999924  -3.64499998   0.           0.        ]\n",
      " [ -3.20000458  27.89999962   0.76999998   0.           0.        ]\n",
      " [ -1.70000458  27.89999962  -1.04999995   0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(scan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55fd0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_xyz(scan_data):\n",
    "    ## Scaling the numerical data columns ie x, y, z cols, then appending the orig cols for return\n",
    "    \n",
    "    data_numerical = scan_data[:, :3]\n",
    "    X = StandardScaler()\n",
    "    scaled_num = X.fit_transform(data_numerical)\n",
    "    return np.hstack((scaled_num, scan_data))\n",
    "\n",
    "\n",
    "\n",
    "def save_cluster_plots(non_bg_matrix, values, title=None, dir_path=\"./plots/\", verbose=False):\n",
    "    \"\"\"\n",
    "   ### non_bg_matrix -- scaled_x|scaled_y|scaled_z|x|y|z|cls|inst\n",
    "    non_bg_matrix -- x|y|z|cls|inst\n",
    "    values -- cluster id\n",
    "    tag -- eg A_x_eps0.05_minpts30\n",
    "    \"\"\"\n",
    "    f_tag = \"[save_cluster_plots]\"\n",
    "    f_msg = \"\"\n",
    "    \n",
    "    if not title:\n",
    "        print(\"[ERROR] No specfied title args. Plot not generated\")\n",
    "    else:\n",
    "        cls_output_path = dir_path+title+\".png\"\n",
    "        _plot(cls_output_path, non_bg_matrix[:, :2], values, title=title)\n",
    "        f_msg = cls_output_path\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "            \n",
    "def processDBSCAN(data, sample_tag, cols_tag, eps, min_samples, plot_sample=False, verbose=False):\n",
    "    \"\"\"\n",
    "    data  -- scaled_x|scaled_y|scaled_z|x|y|z|cls|inst\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data[:, :1])\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "        \n",
    "#         if plot_sample:\n",
    "#             plt.scatter(scan_data[idx, 3], scan_data[idx, 4])\n",
    "    \n",
    "#     if plot_sample:\n",
    "#         plt.show()\n",
    "\n",
    "    \n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def processDBSCAN2(data, sample_tag, cols_tag, eps, min_samples, verbose=False):\n",
    "    \"\"\"\n",
    "    data  -- eg x|y\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "\n",
    "    \n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def processDBSCAN3_with_backups(data, instance_count, sample_tag, cols_tag, eps_seq, min_samples_seq, verbose=False):\n",
    "    assert type(eps_seq) == type(min_samples_seq), \"eps_seq and min_samples_seq must be of the same type, either a float or a list of float\"\n",
    "    assert len(eps_seq) == len(min_samples_seq), \"eps_seq and min_samples_seq must be of the same length\"\n",
    "    \n",
    "    expected_mean = data.shape[0]/instance_count\n",
    "    count_range = (0.75*expected_mean, 1.25*expected_mean)\n",
    "    \n",
    "#     pairs = zip(eps_seq, min_samples_seq)\n",
    "    \n",
    "    cluster_count = 0\n",
    "    pair_idx = 0\n",
    "    max_cluster_count = 0\n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    \n",
    "    while cluster_count != instance_count:\n",
    "        if pair_idx >= len(eps_seq):\n",
    "            break\n",
    "            \n",
    "        eps = eps_seq[pair_idx]\n",
    "        min_samples = min_samples_seq[pair_idx]\n",
    "        DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "        DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "        ## grouping and overriding DBSCAN_clusters and _cluster_count with clean version\n",
    "        DBSCAN_clusters, DBSCAN_cluster_counts = \\\n",
    "                pre_clean_cluster(DBSCAN_clusters, DBSCAN_cluster_counts, count_range, instance_count)\n",
    "\n",
    "        pair_idx+=1\n",
    "        cluster_count = len(DBSCAN_cluster_counts)\n",
    "        if cluster_count > max_cluster_count:\n",
    "            max_cluster_count = cluster_count\n",
    "            for cluster in DBSCAN_clusters:\n",
    "                idx = np.where(DBSCAN_result == cluster)\n",
    "                out[idx] = cluster\n",
    "            title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "        \n",
    "    return out, title, {\"cluster_ids\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def pre_clean_cluster(clusters, cluster_counts, count_range, instance_count):\n",
    "    \n",
    "    ## Cluster Size Check: Removing the noise and merged clusters\n",
    "    filtered_clusters = []\n",
    "    for c_id, c_count in zip(clusters, cluster_counts):\n",
    "        if c_id != -1:\n",
    "            if c_count >= count_range[0] and c_count <= count_range[1]:\n",
    "                filtered_clusters.append((int(c_id), int(c_count)))\n",
    "                \n",
    "    ## Cluster Size Check: Sorting into descending order of cluster size\n",
    "    filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    " \n",
    "    ## Cluster Size Check: Returning only expected_cluster_count or less number of clusters\n",
    "    ## Effectively relying on GNN instance predictions for number of welds detected, at least\n",
    "    if len(filtered_clusters) > instance_count:\n",
    "        filtered_clusters = filtered_clusters[:instance_count]\n",
    "    \n",
    "    print(\"filtered_clusters\")\n",
    "    print(filtered_clusters)\n",
    "    \n",
    "#     clusters, cluster_counts = [], [] if filtered_clusters == [] else zip(*filtered_clusters) \n",
    "    if filtered_clusters == []:\n",
    "        clusters, cluster_counts = [0],[0]\n",
    "    else:\n",
    "        clusters, cluster_counts = zip(*filtered_clusters) \n",
    "    print(\"clusters, cluster_counts\")\n",
    "    print(clusters, cluster_counts)\n",
    "    \n",
    "    return clusters, cluster_counts\n",
    "    \n",
    "def clean_cluster(scan_data, clusters):\n",
    "    ## Reconsituting scan_data without noise, merged or extra clusters; \n",
    "    ## Reorders into continuguous clusters; possible size reduction\n",
    "    shortlist_cluster_idx = [c_id for c_id, c_count in filtered_clusters]\n",
    "    scan_data_idx = np.isin(scan_data[:,-1], shortlist_cluster_idx)\n",
    "    scan_data = scan_data[scan_data_idx]\n",
    "    meta[\"3_final_count\"] = scan_data.shape[0]\n",
    "    \n",
    "    \n",
    "def run_DBSCAN_clustering(scan_data, sample_tag, cols_tag, eps, min_samples, dir_path):\n",
    "    scan_data = scale_xyz(scan_data)\n",
    "    out, title, _ = processDBSCAN(scan_data, sample_tag, cols_tag, eps, min_samples, verbose=True)\n",
    "    save_cluster_plots(scan_data[:, 3:], out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))\n",
    "    \n",
    "def run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    out, title, _ = processDBSCAN2(scan_data[:, :2], sample_tag, \"xy\", eps, min_samples, verbose=False)\n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))\n",
    "\n",
    "def run_DBSCAN_clustering_2D_with_backups(scan_data, eps_seq, min_samples_seq, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    instance_count, _ = count_cluster_by_instance_prediction(scan_data)\n",
    "    out, title, clusters = processDBSCAN3_with_backups(scan_data[:,:2], instance_count, sample_tag, \"xy\", eps_seq, min_samples_seq, verbose=False)\n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out)), clusters\n",
    "\n",
    "## POSTProcessing after clustering to remove the low numbers (clusters of noise) and the high numbers (merges)\n",
    "## Check for non-clusters (-1 tagged) and remove from the scan_data\n",
    "## For each cluster, if count greater than a min and less than a max\n",
    "## Return \"cleaned\" cluster index in sorted order\n",
    "\n",
    "def postcluster_grouping(scan_data, clusters=None):\n",
    "    \n",
    "    meta = {}\n",
    "    \n",
    "    shortlist_cluster_idx = clusters[\"cluster_ids\"] if clusters else None\n",
    "    if shortlist_cluster_idx:\n",
    "        meta[\"2_count_pre_reduction\"] = sum(clusters[\"cluster_counts\"])\n",
    "    \n",
    "    else:\n",
    "        ## Cluster Size Check: Setting cluster count bounds to weed out noise cluster and merged clusters\n",
    "        expected_cluster_count, _ = count_cluster_by_instance_prediction(scan_data)\n",
    "        expected_mean = scan_data.shape[0]/expected_cluster_count\n",
    "        count_range = (0.5*expected_mean, 1.5*expected_mean)\n",
    "        meta[\"1_orig_nonbg_count\"] = scan_data.shape[0]\n",
    "        meta[\"4_clustering_size_range\"] = count_range\n",
    "\n",
    "        ## Cluster Size Check: Removing the noise and merged clusters\n",
    "        clusters, cluster_counts = np.unique(scan_data[:, -1], return_counts=True)\n",
    "        filtered_clusters = [] \n",
    "        for c_id, c_count in zip(clusters, cluster_counts):\n",
    "            if c_id != -1:\n",
    "                if c_count >= count_range[0] and c_count <= count_range[1]:\n",
    "                    filtered_clusters.append((int(c_id), int(c_count)))\n",
    "\n",
    "        ## [ERROR] should be storing a copy\n",
    "        meta[\"5_filtered_cluster_presort\"] = filtered_clusters\n",
    "\n",
    "        ## Cluster Size Check: Sorting into descending order of cluster size\n",
    "        filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    "\n",
    "        ## [ERROR] should be storing a copy\n",
    "        meta[\"6_filtered_cluster_sorted\"] = filtered_clusters\n",
    "        meta[\"2_filtered_by_size_count\"] = sum([c_count for c_id, c_count in filtered_clusters])\n",
    "\n",
    "        ## Cluster Size Check: Returning only expected_cluster_count or less number of clusters\n",
    "        ## Effectively relying on GNN instance predictions for number of welds detected, at least\n",
    "        if len(filtered_clusters) > expected_cluster_count:\n",
    "            filtered_clusters = filtered_clusters[:expected_cluster_count]\n",
    "        meta[\"7_filtered_cluster_final\"] = filtered_clusters    \n",
    "\n",
    "        shortlist_cluster_idx = [c_id for c_id, c_count in filtered_clusters]\n",
    "    \n",
    "    ## Reconsituting scan_data without noise, merged or extra clusters; possible size reduction\n",
    "    scan_data_idx = np.isin(scan_data[:,-1], shortlist_cluster_idx)\n",
    "    scan_data = scan_data[scan_data_idx]\n",
    "    meta[\"3_final_count\"] = scan_data.shape[0]\n",
    "    \n",
    "    return scan_data, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45094672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2083eecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## CLUSTERING EXPERIMENTS\n",
    "\n",
    "# scan_path_options = [{\"scan_path\":\"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\", \"tag\": \"A\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1--2312014-NoResult-l-2023_06_02-7-56-18-48.ply\", \"tag\": \"B\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-10-231201626-Pass-2023_06_13-9-13-41-018.ply\", \"tag\": \"C\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-4-231201605-Pass-2023_06_12-10-46-11-096.ply\", \"tag\": \"D\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-1-231201596-Pass-2023_06_13-12-18-19-465.ply\", \"tag\": \"E\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-2-231201584-Pass-2023_06_09-9-19-43-815.ply\", \"tag\": \"F\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-5-231201583-Pass-2023_06_09-9-16-53-153.ply\", \"tag\": \"G\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-11-231201594-Pass-2023_06_13-12-08-56-692.ply\", \"tag\": \"H\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201598-Pass-2023_06_12-9-30-17-240.ply\", \"tag\": \"I\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201597-Fail-2023_06_13-12-21-34-579.ply\", \"tag\": \"J\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201599-Fail-2023_06_13-12-43-13-933.ply\", \"tag\": \"K\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201607-Fail-2023_06_13-13-44-20-591.ply\", \"tag\": \"L\"},\n",
    "#                     ]\n",
    "# eps_options = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4,\\\n",
    "#                0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "# min_samples_options = [3, 10, 30, 50, 75, 100, 150, 200, 250, 300, 350, 400]\n",
    "# dir_path = \"./plots1_/\"\n",
    "\n",
    "# # eps_options = [0.01, 0.02]\n",
    "# # min_samples_options = [3, 10]\n",
    "\n",
    "# scan_data = None\n",
    "# for scan in scan_path_options:\n",
    "#     scan_data = run_preclustering(model, scan[\"scan_path\"], scan[\"tag\"], dir_path=dir_path)\n",
    "\n",
    "#     for eps in eps_options:\n",
    "#         for min_samples in min_samples_options:\n",
    "#             run_DBSCAN_clustering(scan_data, scan[\"tag\"], \"scaled_x\", eps, min_samples, dir_path=dir_path)\n",
    "# print(\"Done\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5c6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fdf5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Testing the Clustering run\n",
    "\n",
    "# print(\"scan_data.shape: \")\n",
    "# print(scan_data.shape)\n",
    "# print()\n",
    "\n",
    "# scan_data = scale_xyz(scan_data)\n",
    "# print(\"scan_data.shape: \")\n",
    "# print(scan_data.shape)\n",
    "# print()\n",
    "\n",
    "# result, data = processDBSCAN(scan_data[:, :1], 'testA', 'xy', 0.02, 3)\n",
    "# print(\"result\")\n",
    "# print(result)\n",
    "# print()\n",
    "\n",
    "# print(\"data\")\n",
    "# print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afb4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "# scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "# scan_data, _ = filter_out_background(scan_data)\n",
    "# count_cluster_by_instance_prediction(scan_data, threshold_factor=0.5)\n",
    "# scan_data = scale_xyz(scan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab7341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### IDEA2\n",
    "# ## LOOPING THROUGH EACH INSTANCE SUBSET AND CLUSTERING SINGULAR HIGH DENSITY REGION\n",
    "\n",
    "# def filter_and_cluster(scan_data, inst_col_no, inst_id, eps, min_samples, verbose=False):\n",
    "# #     print(scan_data[inst_col_no])\n",
    "#     inst_subset_idx = np.where(scan_data[:, inst_col_no] == inst_id)\n",
    "# #     if verbose:\n",
    "# #         print(inst_subset_idx)\n",
    "#     inst_subset = scan_data[inst_subset_idx]\n",
    "#     if verbose:\n",
    "#         print(\"inst_subset.shape\")\n",
    "#         print(inst_subset.shape)\n",
    "        \n",
    "#     DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#     DBSCAN_result = DBSCAN_model.fit_predict(inst_subset[:, :1])\n",
    "#     DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "#     if verbose:\n",
    "#         print(\"DBSCAN_clusters\")\n",
    "#         print(DBSCAN_clusters)\n",
    "#         print()\n",
    "        \n",
    "#         print(\"DBSCAN_cluster_counts\")\n",
    "#         print(DBSCAN_cluster_counts)\n",
    "#         print()\n",
    "        \n",
    "#     for cluster in DBSCAN_clusters:\n",
    "#         idx = np.where(DBSCAN_result == cluster)\n",
    "#         plt.scatter(scan_data[idx, 0], scan_data[idx, 1])\n",
    "    \n",
    "#     plt.show\n",
    "#     return inst_subset\n",
    "\n",
    "\n",
    "# filter_and_cluster(scan_data, 7, 2, 0.01, 50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31e3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Testing DBSCAN Clustering on 2D ie  x and y Locs\n",
    "\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)\n",
    "\n",
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "\n",
    "\n",
    "# scan_data, _ = classify_scan(model, scan_path, verbose=True)\n",
    "# scan_data, _ = filter_out_background(scan_data)\n",
    "# _, cluster_data = count_cluster_by_instance_prediction(scan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c1b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 2.5\n",
    "# min_samples = 30\n",
    "# out, title, _ = processDBSCAN2(scan_data[:, :2], \"\", \"\", eps, min_samples, verbose=True)\n",
    "# print(\"title:\", title)\n",
    "# _plot(None, scan_data[:, :2], out, title=\"\", caption=\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179c787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb72bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_path_options = [{\"scan_path\":\"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\", \"tag\": \"A\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1--2312014-NoResult-l-2023_06_02-7-56-18-48.ply\", \"tag\": \"B\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-10-231201626-Pass-2023_06_13-9-13-41-018.ply\", \"tag\": \"C\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-4-231201605-Pass-2023_06_12-10-46-11-096.ply\", \"tag\": \"D\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-1-231201596-Pass-2023_06_13-12-18-19-465.ply\", \"tag\": \"E\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-2-231201584-Pass-2023_06_09-9-19-43-815.ply\", \"tag\": \"F\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-5-231201583-Pass-2023_06_09-9-16-53-153.ply\", \"tag\": \"G\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/RH-11-231201594-Pass-2023_06_13-12-08-56-692.ply\", \"tag\": \"H\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201598-Pass-2023_06_12-9-30-17-240.ply\", \"tag\": \"I\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201597-Fail-2023_06_13-12-21-34-579.ply\", \"tag\": \"J\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201599-Fail-2023_06_13-12-43-13-933.ply\", \"tag\": \"K\"},\n",
    "#                      {\"scan_path\":\"./_data/scans/LH-1-231201607-Fail-2023_06_13-13-44-20-591.ply\", \"tag\": \"L\"},\n",
    "#                     ]\n",
    "# eps_options = [1.0, 1.5, 2, 2.5, 3]\n",
    "# min_samples_options = [3, 10, 20, 30, 40, 50]\n",
    "# dir_path = \"./plots2_/\"\n",
    "\n",
    "\n",
    "# scan_data = None\n",
    "# for scan in scan_path_options:\n",
    "#     scan_data = run_preclustering(model, scan[\"scan_path\"], scan[\"tag\"], dir_path=dir_path)\n",
    "\n",
    "#     for eps in eps_options:\n",
    "#         for min_samples in min_samples_options:\n",
    "#             run_DBSCAN_clustering_2D(scan_data, scan[\"tag\"], \"xy\", eps, min_samples, dir_path=dir_path)\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b756f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "# eps = 2\n",
    "# min_samples = 20\n",
    "# sample_tag = \"T1\"\n",
    "# dir_path = \"./plots/\"\n",
    "# scan_data = run_preclustering(model, scan_path, sample_tag=sample_tag, dir_path=dir_path)\n",
    "# scan_data = run_DBSCAN_clustering_2D(scan_data, sample_tag, \"xy\", eps, min_samples, dir_path=dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c5b340",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# postcluster_grouping(scan_data)\n",
    "\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90222d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.random.randint(3, size=(10,2))\n",
    "# d = []\n",
    "# b = np.where(a[:,-1] == 2)[0].tolist()\n",
    "# d+=b\n",
    "# c = np.where(a[:,-1] == 0)[0].tolist()\n",
    "# d+=c\n",
    "\n",
    "# print(\"a\")\n",
    "# print(a)\n",
    "# print(\"b\")\n",
    "# print(b)\n",
    "# print(\"c\")\n",
    "# print(c)\n",
    "# print(\"d\")\n",
    "# print(d)\n",
    "\n",
    "# np.isin(a[:, 0], [1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72287c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1691a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing through a file itemizing a list of scans_path, label_path\n",
    "## Tag each as A-Z,AA-ZZ,AAA-ZZZ,etc\n",
    "## Parse Each and Plot\n",
    "## Run through the DBSCAN_2D and plot clustering\n",
    "## Run through the post_clustering_grouping\n",
    "## RUn through the post_clustering_voting\n",
    "## Plot final cls and inst, for demo sake\n",
    "## Compile Spreadsheet of plotting results\n",
    "## Compile meta data: prediction point count, clustering point count, filtered clustering point \n",
    "## compile weld data: centroid, x/y min/max, type --> ordered by x,\n",
    "##                    bincounts --> mode_x, mode_y\n",
    "##                    median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fb37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scan_label_file(file_path):\n",
    "    \"\"\"\n",
    "    Read file at spec'd file for line count and for scan and label path lists\n",
    "    \"\"\"\n",
    "    scan_list, label_list = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            scan_path, label_path = eval(line)\n",
    "            scan_list.append(scan_path)\n",
    "            label_list.append(label_path)\n",
    "    return len(scan_list), {\"scans\": scan_list, \"labels\": label_list}\n",
    "    \n",
    "\n",
    "# sample_file_path = \"./_data/selection.txt\"\n",
    "# count, data = parse_scan_label_file(sample_file_path)\n",
    "# print(\"parse count\", count)\n",
    "# print(\"\\nscans\")\n",
    "# print(data[\"scans\"])\n",
    "# print(\"\\nlabels\")\n",
    "# print(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ddc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tags_list(item_count):\n",
    "    \"\"\"\n",
    "    Generating tag list for X number of items up to max count of 26*27=702\n",
    "    level1: A to Z --> 1 to 26\n",
    "    level2: AA to ZZ --> 27 to 26*(26+1)\n",
    "    \"\"\"\n",
    "    chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    \n",
    "    ## Dictionary to capture each Level\n",
    "    n_levels = 1 if item_count <=26 else 2\n",
    "    level_els = {}\n",
    "    for i in range(1, n_levels+1):\n",
    "        level_els[i] = []\n",
    "\n",
    "    ## Incrementing up to item_count and creating a tag each time\n",
    "    idx = 0\n",
    "    while idx < item_count:\n",
    "        if idx < 26:\n",
    "            level_els[1].append(chars[idx])\n",
    "        elif idx < 26*27:\n",
    "            first = int(idx/26) - 1\n",
    "            second = idx % 26\n",
    "            level_els[2].append(level_els[1][first]+chars[second])\n",
    "        idx+=1\n",
    "    \n",
    "    ## Combining dictionary's levels into output list\n",
    "    _list = level_els[1] + level_els[2] if n_levels==2 else level_els[1]\n",
    "    \n",
    "    return _list\n",
    "\n",
    "# test_tags1 = gen_tags_list(26*27)\n",
    "# print(test_tags1[:3])\n",
    "# print(test_tags1[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3511157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postcluster_voting(scan_data):\n",
    "    \n",
    "    ## create new voting cols: cls and inst\n",
    "    n_rows, _ = scan_data.shape\n",
    "    scan_data = np.hstack((scan_data, np.zeros((n_rows, 1)), np.zeros((n_rows, 1)))) \n",
    "\n",
    "    ## expecting col5 to be the clustering result\n",
    "    clusters = np.unique(scan_data[:, 5])\n",
    "    for c_id in clusters:\n",
    "        scan_data_subset_idx = np.where(scan_data[:, 5] == c_id)\n",
    "        \n",
    "        ## majority votes\n",
    "        cls_vote = mode(np.ravel(scan_data[scan_data_subset_idx,3]), keepdims=False)[0]\n",
    "        inst_vote = mode(np.ravel(scan_data[scan_data_subset_idx,4]), keepdims=False)[0]\n",
    "        \n",
    "        ## push to data matrix\n",
    "        scan_data[scan_data_subset_idx, 6] = cls_vote\n",
    "        scan_data[scan_data_subset_idx, 7] = inst_vote\n",
    "\n",
    "    return scan_data\n",
    "\n",
    "# scan_data_test = postcluster_voting(scan_data)\n",
    "# df = pd.DataFrame(scan_data_test, columns=['x', 'y', 'z', 'cls_preds' ,'inst_pred', 'cluster', 'cls', 'inst'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086d92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial version\n",
    "def run_postclustering(scan_data, sample_tag=\"\", dir_path=\"./plots/\", clusters=None ):\n",
    "    scan_data, meta1 = postcluster_grouping(scan_data, clusters=None) ## ncols remains 6, nrows possibly reduced\n",
    "    scan_data = postcluster_voting(scan_data)  ## n_cols increased from 6 to 8\n",
    "    save_prediction_plots(scan_data, cls_tag=f\"{sample_tag}_xycls_clean\", inst_tag=f\"{sample_tag}_xyinst_clean\", cls_col=6, inst_col=7, dir_path=dir_path)\n",
    "    return scan_data, meta1\n",
    "\n",
    "# ## refactored version\n",
    "# def run_clustervoting(scan_data, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "#     scan_data = postcluster_voting(scan_data)  ## n_cols increased from 6 to 8\n",
    "#     save_prediction_plots(scan_data, cls_tag=f\"{sample_tag}_xycls_clean\", inst_tag=f\"{sample_tag}_xyinst_clean\", cls_col=6, inst_col=7, dir_path=dir_path)\n",
    "#     return scan_data, meta1\n",
    "\n",
    "def save_data_dictionary(data, dir_path=\"./plots/\"):\n",
    "    file_path = f\"{dir_path}log.txt\"\n",
    "    pairs = [(k, v) for k, v in data.items()]\n",
    "    pairs.sort()\n",
    "    with open(file_path, \"a+\") as f_out:\n",
    "        for k,v in pairs:\n",
    "            f_out.write(f\"{k} \\t {v}\\n\")\n",
    "        f_out.write(\"\\n\")\n",
    "    \n",
    "# scan_data_test, meta_test = run_postclustering(scan_data, \"testA\", dir_path=None)\n",
    "# save_data_dictionary(meta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc6ed417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29966dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps = 2\n",
    "# min_samples = 20\n",
    "# sample_tag = \"T1\"\n",
    "# dir_path = \"./plots/\"\n",
    "# scan_data = run_preclustering(model, scan_path, sample_tag=sample_tag, dir_path=dir_path)\n",
    "# scan_data = run_DBSCAN_clustering_2D(scan_data, sample_tag, \"xy\", eps, min_samples, dir_path=dir_path)\n",
    "# run_postclustering(scan_data, \"testA\", None)\n",
    "\n",
    "def run(model_params_path, file_path, dir_path, eps=2, min_samples=20):\n",
    "    \n",
    "    ## [TODO] Check path exists\n",
    "    \n",
    "    ## [TODO] Delete Log file if exist\n",
    "    \n",
    "    ## Generate model, load parameters\n",
    "    model = configure_model(model_params_path, verbose=True)\n",
    "    \n",
    "    ## Parse the spec'd file for count and scans' paths\n",
    "    file_count, file_data = parse_scan_label_file(file_path)\n",
    "    scans = file_data[\"scans\"]\n",
    "    \n",
    "    ## Generate sample tags for each scan\n",
    "    tags = gen_tags_list(file_count)\n",
    "    \n",
    "    ## Zip scans and tags and loop through each pair\n",
    "    for scan, tag in zip(scans, tags):\n",
    "        \n",
    "        ## preclustering: inference, filter out background, save plots\n",
    "        scan_data, metaA = run_preclustering(model, scan, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "        ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6]\n",
    "#         scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "        ## ALTERNATIVELY for exploration: cluster and plot diff combinations of eps and min_samples\n",
    "        if isinstance(eps, list) and isinstance(min_samples, list):\n",
    "            for e in eps:\n",
    "                for ms in min_samples:\n",
    "                    run_DBSCAN_clustering_2D(scan_data, e, ms, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "        elif isinstance(eps, list) and not isinstance(min_samples):\n",
    "            for e in eps:\n",
    "                run_DBSCAN_clustering_2D(scan_data, e, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "                \n",
    "        elif not isinstance(eps,list) and isinstance(min_samples, list):\n",
    "            for ms in min_samples:\n",
    "                run_DBSCAN_clustering_2D(scan_data, eps, ms, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "        else:\n",
    "            scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "            \n",
    "        \n",
    "            ## postclustering: filter out noise and merged data, limit cluster \n",
    "            ## count to count from instance predictions, majority votes and \n",
    "            ## add to scan_data [cols: 6->8], save plots\n",
    "            scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "\n",
    "            ## combine meta dictionaries and print to file\n",
    "            meta = metaA | metaB\n",
    "            save_data_dictionary(meta, dir_path=dir_path)\n",
    "\n",
    "            ## locationing: collect welds with mean/median/mode, type, id, and tag; save plot and csv\n",
    "            welds = run_locationing(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "    print(\"[Run]: Done!\")\n",
    "        \n",
    "def run2(model_params_path, file_path, dir_path, eps_seq=[2], min_samples_seq=[20]):\n",
    "    \n",
    "    ## [TODO] Check path exists\n",
    "    \n",
    "    ## [TODO] Delete Log file if exist\n",
    "    \n",
    "    ## Generate model, load parameters\n",
    "    model = configure_model(model_params_path, verbose=True)\n",
    "    \n",
    "    ## Parse the spec'd file for count and scans' paths\n",
    "    file_count, file_data = parse_scan_label_file(file_path)\n",
    "    scans = file_data[\"scans\"]\n",
    "    \n",
    "    ## Generate sample tags for each scan\n",
    "    tags = gen_tags_list(file_count)\n",
    "    \n",
    "    ## Zip scans and tags and loop through each pair\n",
    "    for scan, tag in zip(scans, tags):\n",
    "        \n",
    "        ## preclustering: inference, filter out background, save plots\n",
    "        scan_data, metaA = run_preclustering(model, scan, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "        ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6]\n",
    "        scan_data, clusters = \\\n",
    "                run_DBSCAN_clustering_2D_with_backups(scan_data, eps_seq, min_samples_seq, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "        \n",
    "        ## postclustering: filter out noise and merged data, limit cluster \n",
    "        ## count to count from instance predictions, majority votes and \n",
    "        ## add to scan_data [cols: 6->8], save plots\n",
    "        scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path, clusters=clusters)\n",
    "\n",
    "        ## combine meta dictionaries and print to file\n",
    "        meta = metaA | metaB\n",
    "        save_data_dictionary(meta, dir_path=dir_path)\n",
    "\n",
    "        ## locationing: collect welds with mean/median/mode, type, id, and tag; save plot and csv\n",
    "        welds = run_locationing(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "        \n",
    "    print(\"[Run]: Done!\")\n",
    "        \n",
    "## [TODO] need a prodcution version without the plotting and/or saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8717412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemize_welds(scan_data, cls_col, inst_col, sample_tag=\"\"):\n",
    "    \"\"\"\n",
    "    A list of dict per weld \n",
    "     - Instance id, and type\n",
    "     - min and max x and y\n",
    "     - Centroid by mean - x and y means\n",
    "     - Centroid by median - x and y medians\n",
    "     - Bin splits then highest count --> effectively the x and y modes  (bins to decrease ganularity of the widely sparsed floats)\n",
    "     \n",
    "    Output options: ==> Another Function\n",
    "     -- each weld to a record in a csv file\n",
    "     -- an imaged saved showing center estimations\n",
    "    \n",
    "    \"\"\"\n",
    "    ## A incrementing list of welds\n",
    "    welds = []\n",
    "    \n",
    "    ## Loop through each option in the inst_col ie each weld\n",
    "    for weld_inst in np.unique(scan_data[:, inst_col]):     \n",
    "        \n",
    "    ## For each weld identified, \n",
    "            \n",
    "        ## dict, label, and id \n",
    "        weld = {}\n",
    "        weld['tag'] = f\"{sample_tag}_weld{int(weld_inst)}\"\n",
    "        weld['id'] = int(weld_inst)\n",
    "\n",
    "        ## weld members\n",
    "        sub = scan_data[scan_data[:, inst_col] == weld_inst]\n",
    "        weld[\"sub\"] = sub\n",
    "\n",
    "        ## type\n",
    "        weld['type'] = \"[\" if sub[0, cls_col] == 1 else \"]\"\n",
    "\n",
    "        ## ranges\n",
    "        weld[\"x_min\"], weld[\"x_max\"] = np.min(sub[:, 0]), np.max(sub[:,0])\n",
    "        weld[\"y_min\"], weld[\"y_max\"] = np.min(sub[:, 1]), np.max(sub[:,1])\n",
    "\n",
    "        ## means\n",
    "        weld['x_mean'] = np.mean(sub[:,0])\n",
    "        weld['y_mean'] = np.mean(sub[:,1])\n",
    "\n",
    "        ## medians\n",
    "        weld['x_median'] = np.median(sub[:,0])\n",
    "        weld['y_median'] = np.median(sub[:,1])\n",
    "\n",
    "        ## modes\n",
    "        weld['x_modeA'], weld['x_modeB'] = modes(sub, 0, 0.2)\n",
    "        weld['y_modeA'], weld['y_modeB'] = modes(sub, 1, 0.2)\n",
    "\n",
    "        welds.append(weld)\n",
    "    \n",
    "    return welds\n",
    "            \n",
    "            \n",
    "def modes(arr, col, bin_step):\n",
    "    \"\"\"\n",
    "    bin_step determines resolution\n",
    "    \"\"\"\n",
    "    col_min, col_max = np.min(arr[:,col]), np.max(arr[:,col])\n",
    "    step = int(np.ceil((col_max - col_min)/bin_step)) ## ideally should recenter the cols instead of shrinking the divs\n",
    "    bin_bounds = np.linspace(col_min, col_max, step)\n",
    "    bin_splits = bin_bounds[1:-1]\n",
    "\n",
    "    splits = [np.where(np.logical_and(bin_bounds[i-1] <= arr[:,col], arr[:,col] <bin_bounds[i]))[0] for i in range(1, len(bin_bounds))]\n",
    "    split_counts = [len(split) for split in splits]\n",
    "    highest_count = max(split_counts)\n",
    "    \n",
    "    mode_by_mean = [np.mean(arr[splits[i], col]) for i, s in enumerate(split_counts) if s == highest_count]\n",
    "    mode_by_range_mid = [(bin_bounds[i] + bin_bounds[i+1])/2 for i, s in enumerate(split_counts) if s == highest_count]\n",
    "    \n",
    "    return mode_by_mean, mode_by_range_mid\n",
    "    \n",
    "def save_weld_location_plots(non_bg_matrix, values, welds, title=None, dir_path=\"./plots\", verbose=False):\n",
    "\n",
    "    f_tag = \"[save_weld_location_plots]\"\n",
    "    f_msg = \"\"\n",
    "    \n",
    "    if not title:\n",
    "        print(\"[ERROR] No specfied title args. Plot not generated\")\n",
    "    else:\n",
    "        ## weld_inst \n",
    "        ## --> x_/y_min/ max;\n",
    "        ## --> x_/y_mean;\n",
    "        ## --> x_/y_median;\n",
    "        ## --> x_/y_modeA;\n",
    "        ## --> x_/y_modeB;\n",
    "        \n",
    "        ## overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "        overlay_points, _ = parse_weld_centers(welds) \n",
    "        \n",
    "#         print()\n",
    "#         print(\"overlay_points\")\n",
    "#         print(overlay_points)\n",
    "        \n",
    "        \n",
    "        weld_loc_output_path = dir_path+title+\".png\"\n",
    "        _plot(weld_loc_output_path, non_bg_matrix[:, :2], values, title=title, overlay_points=overlay_points)\n",
    "        f_msg = weld_loc_output_path\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{f_tag}: Done --> {f_msg}\")\n",
    "        \n",
    "#         return f_msg \n",
    "\n",
    "def save_welds_csv(welds, dir_path=\"./plots/\", verbose=False):\n",
    "    filename = f\"{dir_path}log.csv\"\n",
    "    _keys = [\"tag\", \"id\", \"type\", \\\n",
    "             \"x_min\", \"x_max\", \\\n",
    "             \"y_min\", \"y_max\", \\\n",
    "             \"x_mean\", \"y_mean\", \\\n",
    "             \"x_median\", \"y_median\", \\\n",
    "             \"x_modeA\", \"y_modeA\", \\\n",
    "             \"x_modeB\", \"y_modeB\"]\n",
    "    \n",
    "    add_header = False\n",
    "    if not os.path.isfile(filename):\n",
    "        add_header = True\n",
    "        \n",
    "    with open(filename, 'a+') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        if add_header:\n",
    "            csvwriter.writerow(_keys)\n",
    "            add_header=False\n",
    "    \n",
    "        for weld in welds:\n",
    "            row = []\n",
    "            for k in _keys:\n",
    "                row.append(weld[k])\n",
    "            csvwriter.writerow(row)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[save_welds_csv]: Done --> {filename}\")\n",
    "        \n",
    "    pass\n",
    "\n",
    "def parse_weld_centers(welds_list):\n",
    "    \n",
    "    ## overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "    \n",
    "    means = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'+', \"label\":\"x_/y_mean\" }\n",
    "    medians = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'s', \"label\":\"x_/y_median\" }\n",
    "    modeAs = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'x', \"label\":\"x_/y_mode[mean]\" }\n",
    "    modeBs = {\"xs\":[], \"ys\":[], \"cs\":[], \"marker\":'*', \"label\":\"x_/y_mode[range_mid]\" }\n",
    "    \n",
    "    for weld in welds_list:\n",
    "        ## means\n",
    "        means[\"xs\"].append(weld[\"x_mean\"])\n",
    "        means[\"ys\"].append(weld[\"y_mean\"])\n",
    "        means[\"cs\"].append('black')\n",
    "        \n",
    "        ## medians\n",
    "        medians[\"xs\"].append(weld[\"x_median\"])\n",
    "        medians[\"ys\"].append(weld[\"y_median\"])\n",
    "        medians[\"cs\"].append('grey')\n",
    "        \n",
    "        ## modeAs\n",
    "        xs = weld[\"x_modeA\"]\n",
    "        ys = weld[\"y_modeA\"]\n",
    "        for x_mA in xs:\n",
    "            for y_mA in ys:\n",
    "                modeAs[\"xs\"].append(x_mA)\n",
    "                modeAs[\"ys\"].append(y_mA)\n",
    "                modeAs[\"cs\"].append('red')\n",
    "        \n",
    "        ## modeBs\n",
    "        xs = weld[\"x_modeB\"]\n",
    "        ys = weld[\"y_modeB\"]\n",
    "        for x_mB in xs:\n",
    "            for y_mB in ys:\n",
    "                modeBs[\"xs\"].append(x_mB)\n",
    "                modeBs[\"ys\"].append(y_mB)\n",
    "                modeBs[\"cs\"].append('orange')\n",
    "                \n",
    "                \n",
    "    return [means, medians, modeAs, modeBs]   \n",
    "\n",
    "\n",
    "def run_locationing(scan_data, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=sample_tag)\n",
    "    title = f\"{sample_tag}_xyinst_weld_locs\"\n",
    "    save_weld_location_plots(scan_data, scan_data[:,7], welds, title=title, dir_path=dir_path)\n",
    "    save_welds_csv(welds, dir_path=dir_path)\n",
    "    return welds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b7c298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup Time (secs) : 0.09599614143371582\n",
      "Scan Inference Time (secs):  9.68393087387085\n",
      "\n",
      "Scan Inference Time (secs):  9.631904363632202\n",
      "\n",
      "Scan Inference Time (secs):  10.195894241333008\n",
      "\n",
      "Scan Inference Time (secs):  10.716588020324707\n",
      "\n",
      "Scan Inference Time (secs):  9.997865438461304\n",
      "\n",
      "Scan Inference Time (secs):  10.084866762161255\n",
      "\n",
      "Scan Inference Time (secs):  10.643692970275879\n",
      "\n",
      "Scan Inference Time (secs):  11.065883874893188\n",
      "\n",
      "Scan Inference Time (secs):  10.098860025405884\n",
      "\n",
      "Scan Inference Time (secs):  10.005257368087769\n",
      "\n",
      "Scan Inference Time (secs):  10.447861194610596\n",
      "\n",
      "Scan Inference Time (secs):  11.028852701187134\n",
      "\n",
      "Scan Inference Time (secs):  12.03091287612915\n",
      "\n",
      "Scan Inference Time (secs):  11.640886783599854\n",
      "\n",
      "Scan Inference Time (secs):  10.948853969573975\n",
      "\n",
      "Scan Inference Time (secs):  10.764853954315186\n",
      "\n",
      "[Run]: Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## VERSION 1: Single eps and min_samples OR eps and min_sample permutation exploration\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "file_path = \"./_data/good_and_bad2.txt\"\n",
    "dir_path = \"./plots5_/\"\n",
    "eps = [2, 1.5, 1]\n",
    "min_samples = [20, 10]\n",
    "run(model_params_path, file_path, dir_path, eps=eps, min_samples=min_samples)\n",
    "\n",
    "# ## VERSION2: indexwise eps and min_samples combinations\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# file_path = \"./_data/good_and_bad.txt\"\n",
    "# dir_path = \"./plots4_/\"\n",
    "# eps_seq = [2, 1.5, 1]\n",
    "# min_samples_seq = [20, 20, 10]\n",
    "# run2(model_params_path, file_path, dir_path, eps_seq=eps_seq, min_samples_seq=min_samples_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1d6d61b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  9.871661901473999\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Denoising single instances\n",
    "\n",
    "# model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "# model = configure_model(model_params_path, verbose=True)\n",
    "\n",
    "# #easys\n",
    "# scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"  \n",
    "# sample_tag = \"_T_\"\n",
    "\n",
    "\n",
    "# #hards\n",
    "scan_path = './_data/scans/LH-6-231201595-Pass-2023_06_12-9-21-22-515.ply'\n",
    "sample_tag = '_H1_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-7-231201596-Fail-2023_06_13-12-19-11-036.ply'\n",
    "# sample_tag = '_H2_'\n",
    "\n",
    "# scan_path = './_data/scans/LH-6-231201598-Pass-2023_06_12-9-30-40-570.ply'\n",
    "# sample_tag = '_H3_'\n",
    "\n",
    "# scan_path = './_data/scans/LH-2-231201599-Pass-2023_06_12-9-34-34-210.ply'\n",
    "# sample_tag = '_H4_'\n",
    "\n",
    "# scan_path = './_data/scans/LH-2-231201595-Pass-2023_06_12-9-21-03-886.ply'\n",
    "# sample_tag = '_H5_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-9-231201591-Pass-2023_06_09-10-51-11-577.ply'\n",
    "# sample_tag = '_H6_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-2-231201593-Pass-2023_06_09-10-54-31-162.ply'\n",
    "# sample_tag = '_H7_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-4-231201590-Fail-2023_06_09-10-48-34-421.ply'\n",
    "# sample_tag = '_H8_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-7-231201616-Fail-2023_06_12-12-14-28-799.ply'\n",
    "# sample_tag = '_H9_'\n",
    "\n",
    "# scan_path = './_data/scans/RH-8-231201592-Pass-2023_06_09-10-53-26-003.ply'\n",
    "# sample_tag = '_H10_'\n",
    "\n",
    "dir_path = \"./plots_tests/\"\n",
    "\n",
    "scan_data = run_preclustering(model, scan_path, sample_tag=sample_tag, dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3702b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processInstancePredictions(scan_data, sample_tag=\"\", verbose=False): # x|y|z|cls|inst\n",
    "    out = np.zeros((scan_data.shape[0], 1))\n",
    "    instances = np.unique(scan_data[:,4])\n",
    "    DBSCAN_model = DBSCAN(eps=2, min_samples=20)\n",
    "    global_clusters, global_cluster_counts = [], []\n",
    "    for inst in instances:\n",
    "        subset_idx = np.where(scan_data[:, 4] == inst)[0]\n",
    "        if verbose:\n",
    "            print(\"subset_idx\")\n",
    "            print(subset_idx)\n",
    "        \n",
    "        temp = np.zeros((subset_idx.shape[0], 4))\n",
    "        temp[:,0] = subset_idx\n",
    "        temp[:, 1:3] = scan_data[subset_idx, 0:2]\n",
    "        temp[:,3] = DBSCAN_model.fit_predict(temp[:, 1:3])\n",
    "        clusters, cluster_counts = np.unique(temp[:,3], return_counts=True)\n",
    "        if verbose:\n",
    "            print('temp')\n",
    "            print(temp)\n",
    "            print(\"clusters, cluster_counts\")\n",
    "            print(clusters, cluster_counts)\n",
    "        \n",
    "        ## Removing the noise clusters\n",
    "        filtered_clusters = []\n",
    "        for c_id, c_count in zip(clusters, cluster_counts):\n",
    "            if c_id != -1:\n",
    "                filtered_clusters.append((int(c_id), int(c_count)))\n",
    "                \n",
    "        ## Sorting into descending order of cluster size\n",
    "        filtered_clusters.sort(reverse=True, key = lambda e: e[1])\n",
    "        largest_clust = filtered_clusters[0][0]\n",
    "        if verbose:\n",
    "            print(\"largest_clust\")\n",
    "            print(largest_clust)\n",
    "        \n",
    "        ## Reconstituting cluster columns\n",
    "        idx = temp[temp[:,3] == largest_clust, 0].astype(int)\n",
    "        if verbose:\n",
    "            print(\"idx\", idx)\n",
    "        out[idx] = inst\n",
    "        global_clusters.append(int(inst))\n",
    "        global_cluster_counts.append(len(idx))\n",
    "\n",
    "    ## Removing the misclassed points\n",
    "    \n",
    "    \n",
    "    title = f\"{sample_tag}_xy_per_pred_inst\"\n",
    "    return out, title, {\"cluster_vals\": global_clusters, \"cluster_counts\": global_cluster_counts}\n",
    "\n",
    "def processDBSCAN3D(data, sample_tag, cols_tag, eps, min_samples, verbose=False):\n",
    "    \"\"\"\n",
    "    Intended for clustering of 3D data where\n",
    "    data ==> the subset of columns to consider in the distance measure -> eg x|y|inst\n",
    "    sample_tag for tagging the scan to differentiate plots and print outs\n",
    "    cols_tag ==> as a descriptor on which data cols were combined\n",
    "    eps and min_samples for the DBSCAN args\n",
    "    \n",
    "    Returns\n",
    "    out --> col vec of the attributed clusters --> effectively a re-prediction of instances\n",
    "    title --> encoding a descriptions of arguments \n",
    "    a dict of cluster vals and cluster counts\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    title = f\"{sample_tag}_{cols_tag}_eps{eps}_min{min_samples}\"\n",
    "    if verbose:\n",
    "        print(\"title:\", title )\n",
    "    DBSCAN_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    DBSCAN_result = DBSCAN_model.fit_predict(data)\n",
    "    DBSCAN_clusters, DBSCAN_cluster_counts = np.unique(DBSCAN_result, return_counts=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"DBSCAN_clusters\")\n",
    "        print(DBSCAN_clusters)\n",
    "        print()\n",
    "\n",
    "        print(\"DBSCAN_cluster_counts\")\n",
    "        print(DBSCAN_cluster_counts)\n",
    "        print()\n",
    "    \n",
    "    n_rows, n_cols = data.shape\n",
    "    out = np.zeros((n_rows, 1))\n",
    "    for cluster in DBSCAN_clusters:\n",
    "        idx = np.where(DBSCAN_result == cluster)\n",
    "        out[idx] = cluster\n",
    "\n",
    "    \n",
    "    return out, title, {\"cluster_vals\": DBSCAN_clusters, \"cluster_counts\": DBSCAN_cluster_counts}\n",
    "\n",
    "def run_denoising_predictions(scan_data, sample_tag=\"\", dir_path=\"./plots/\", verbose=False): # x|y|z|cls|inst\n",
    "    out, title, clusters = processInstancePredictions(scan_data, sample_tag)\n",
    "    scan_data = np.hstack((scan_data, out))\n",
    "    print(clusters)\n",
    "    cluster_idx = clusters[\"cluster_vals\"]\n",
    "    scan_data = scan_data[np.isin(scan_data[:,-1],  cluster_idx)]\n",
    "    save_cluster_plots(scan_data, scan_data[:,-1], title=title, dir_path=dir_path )\n",
    "    return scan_data\n",
    "\n",
    "\n",
    "def run_DBSCAN_clustering_3D(scan_data, eps, min_samples, instance_weighting=1, sample_tag=\"\", dir_path=\"./plots/\"):\n",
    "    ## Scaling the separation in inst predictions, if necessary\n",
    "    scan_data[:,4] *= instance_weighting\n",
    "    \n",
    "    ## clustering to yeild a col vec of cluster labels\n",
    "    ## [poss RED FLAG] might have to rescale the col vector\n",
    "    out, title, _ = processDBSCAN3D(scan_data[:, [0,1,4]], sample_tag, f\"xyinst_w{instance_weighting}\", eps, min_samples)\n",
    "    \n",
    "    \n",
    "    save_cluster_plots(scan_data, out, title=title, dir_path=dir_path )\n",
    "    return np.hstack((scan_data, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d0907e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scan_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m min_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      5\u001b[0m instance_weighting \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 6\u001b[0m soln \u001b[38;5;241m=\u001b[39m run_DBSCAN_clustering_3D(\u001b[43mscan_data\u001b[49m[\u001b[38;5;241m0\u001b[39m], eps, min_samples, instance_weighting, sample_tag\u001b[38;5;241m=\u001b[39msample_tag, dir_path\u001b[38;5;241m=\u001b[39mdir_path)\n\u001b[0;32m      7\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m run_postclustering(soln, sample_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mw\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_weighting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, dir_path\u001b[38;5;241m=\u001b[39mdir_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scan_data' is not defined"
     ]
    }
   ],
   "source": [
    "# run_denoising_predictions(scan_data[0], sample_tag=sample_tag, dir_path=dir_path)\n",
    "\n",
    "eps = 2\n",
    "min_samples = 20\n",
    "instance_weighting = 2\n",
    "soln = run_DBSCAN_clustering_3D(scan_data[0], eps, min_samples, instance_weighting, sample_tag=sample_tag, dir_path=dir_path)\n",
    "_, _ = run_postclustering(soln, sample_tag=f\"{sample_tag}w{instance_weighting}_\", dir_path=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aad6506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative to Isolating the Weld Centers\n",
    "## Centroiding\n",
    "## ICP\n",
    "## Pattern Matching\n",
    "## Manual: straight edges --> offset\n",
    "## GNN?\n",
    "\n",
    "def run_locationing_by_icp():\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0988aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing ICP\n",
    "\n",
    "## 1. Create Template for source, 2D --> Desire the x,y and theta for each weld and weld are already \n",
    "## Need a good scan with multiple complete weld segments (emphasize quality) --> visualize\n",
    "## Combine those of like type by mean, median, mids, limits; Visualize the different options together for comparison\n",
    "## Offsets (x,y,theta) for combination to position the origin where desired\n",
    "## Alternatively offset each before combining\n",
    "\n",
    "\n",
    "def create_template(model, scan_path, tag=\"\", weld_selections=None, dir_path='./plots/'):\n",
    "    \n",
    "    ## preclustering: inference, filter out background, save plots\n",
    "    scan_data, metaA = run_preclustering(model, scan_path, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6] \n",
    "    print(\"create_template::scan_data.shape\")\n",
    "    print(scan_data.shape)\n",
    "    scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "    ## postclustering: filter out noise and merged data, limit cluster \n",
    "    ## count to count from instance predictions, majority votes and \n",
    "    ## add to scan_data [cols: 6->8], save plots\n",
    "    scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ### splitting for weld instances\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=tag)\n",
    "    \n",
    "    \n",
    "    ## collecting into type groups\n",
    "    \n",
    "    type1, type2 = [], []\n",
    "    if weld_selections:\n",
    "        if isinstance(weld_selections, list):\n",
    "            type1_insts = [int(sel) for sel in weld_selections]\n",
    "        else:\n",
    "            type1_insts = [int(weld_selections)]\n",
    "        type1 = [weld for weld in welds if weld[\"id\"] in type1_insts]\n",
    "    else:\n",
    "        for weld in welds:\n",
    "            if weld[\"type\"] == \"[\":\n",
    "                type1.append(weld)\n",
    "            else:\n",
    "                type2.append(weld)\n",
    "\n",
    "    ## counting for largest group, but starting with type1 for now\n",
    "    ## combining by subtracting the mean | median | mids\n",
    "    welds_at_means = []\n",
    "    welds_at_medians = []\n",
    "    welds_at_mids = []\n",
    "    welds_at_SW_corner = []\n",
    "    \n",
    "    for weld in type1:\n",
    "        \n",
    "        ## subtracting means\n",
    "        weld_sub_means = np.copy(weld[\"sub\"])\n",
    "        weld_sub_means[:, :2] -= np.array([weld[\"x_mean\"], weld[\"y_mean\"]])\n",
    "        welds_at_means.append(weld_sub_means)\n",
    "        \n",
    "        ## subtracting medians\n",
    "        weld_sub_medians = np.copy(weld[\"sub\"])\n",
    "        weld_sub_medians[:, :2] -= np.array([weld[\"x_median\"], weld[\"y_median\"]])\n",
    "        welds_at_medians.append(weld_sub_medians)\n",
    "        \n",
    "        ## subtracting mids\n",
    "        x_mid = (weld[\"x_max\"] + weld[\"x_min\"])/2\n",
    "        y_mid = (weld[\"y_max\"] + weld[\"y_min\"])/2\n",
    "        weld_sub_mids = np.copy(weld[\"sub\"])\n",
    "        weld_sub_mids[:, :2] -= np.array([x_mid, y_mid])\n",
    "        welds_at_mids.append(weld_sub_mids)\n",
    "        \n",
    "        ## subtracting SW corner\n",
    "        weld_sub_corner = np.copy(weld[\"sub\"])\n",
    "        weld_sub_corner[:, :2] -= np.array([weld[\"x_min\"], weld[\"y_min\"]])\n",
    "        welds_at_SW_corner.append(weld_sub_corner)\n",
    "        \n",
    "    template_at_mean = np.vstack(tuple(welds_at_means))\n",
    "    save_cluster_plots(template_at_mean, template_at_mean[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_means_then_merged\", dir_path=dir_path)\n",
    "        \n",
    "    template_at_median = np.vstack(tuple(welds_at_medians))\n",
    "    save_cluster_plots(template_at_median, template_at_median[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_medians_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "    template_at_mid = np.vstack(tuple(welds_at_mids))\n",
    "    save_cluster_plots(template_at_mid, template_at_mid[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_mids_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "    template_at_corner = np.vstack(tuple(welds_at_SW_corner))\n",
    "    save_cluster_plots(template_at_corner, template_at_corner[:, -1], \\\n",
    "                       title=tag+\"A_zeroed_at_corner_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "    templateA = {\"at_mean\": template_at_mean,\n",
    "                \"at_median\": template_at_median,\n",
    "                \"at_mid\": template_at_mid,\n",
    "                \"at_corner\": template_at_corner}\n",
    "    if type2:\n",
    "    \n",
    "        welds_at_means = []\n",
    "        welds_at_medians = []\n",
    "        welds_at_mids = []\n",
    "        welds_at_SW_corner = []\n",
    "\n",
    "        for weld in type2:\n",
    "\n",
    "            ## subtracting means\n",
    "            weld_sub_means = np.copy(weld[\"sub\"])\n",
    "            weld_sub_means[:, :2] -= np.array([weld[\"x_mean\"], weld[\"y_mean\"]])\n",
    "            welds_at_means.append(weld_sub_means)\n",
    "\n",
    "            ## subtracting medians\n",
    "            weld_sub_medians = np.copy(weld[\"sub\"])\n",
    "            weld_sub_medians[:, :2] -= np.array([weld[\"x_median\"], weld[\"y_median\"]])\n",
    "            welds_at_medians.append(weld_sub_medians)\n",
    "\n",
    "            ## subtracting mids\n",
    "            x_mid = (weld[\"x_max\"] + weld[\"x_min\"])/2\n",
    "            y_mid = (weld[\"y_max\"] + weld[\"y_min\"])/2\n",
    "            weld_sub_mids = np.copy(weld[\"sub\"])\n",
    "            weld_sub_mids[:, :2] -= np.array([x_mid, y_mid])\n",
    "            welds_at_mids.append(weld_sub_mids)\n",
    "\n",
    "            ## subtracting SW corner\n",
    "            weld_sub_corner = np.copy(weld[\"sub\"])\n",
    "            weld_sub_corner[:, :2] -= np.array([weld[\"x_min\"], weld[\"y_min\"]])\n",
    "            welds_at_SW_corner.append(weld_sub_corner)\n",
    "\n",
    "        template_at_mean = np.vstack(tuple(welds_at_means))\n",
    "        save_cluster_plots(template_at_mean, template_at_mean[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_means_then_merged\", dir_path=dir_path)\n",
    "\n",
    "        template_at_median = np.vstack(tuple(welds_at_medians))\n",
    "        save_cluster_plots(template_at_median, template_at_median[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_medians_then_merged\", dir_path=dir_path)\n",
    "\n",
    "        template_at_mid = np.vstack(tuple(welds_at_mids))\n",
    "        save_cluster_plots(template_at_mid, template_at_mid[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_mids_then_merged\", dir_path=dir_path)\n",
    "\n",
    "        template_at_corner = np.vstack(tuple(welds_at_SW_corner))\n",
    "        save_cluster_plots(template_at_corner, template_at_corner[:, -1], \\\n",
    "                           title=tag+\"B_zeroed_at_corner_then_merged\", dir_path=dir_path)\n",
    "    \n",
    "        \n",
    "        templateB = {\"at_mean\": template_at_mean,\n",
    "                \"at_median\": template_at_median,\n",
    "                \"at_mid\": template_at_mid,\n",
    "                \"at_corner\": template_at_corner}\n",
    "        \n",
    "        return templateA, templateB\n",
    "    \n",
    "#     TODO: DRAW THE ORIGIN CENTERS\n",
    "    \n",
    "    return templateA\n",
    "\n",
    "def save_template_to_file(template, path, weld_type=\"[\"):\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_template_from_file(path, weld_type=\"[\"):\n",
    "    \"\"\" Saved as Type-[ (default)\"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "def use_template(template, weld_type):\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "def offset_template(template, weld_type, offset=(0,0)):\n",
    "    \"\"\"\"\"\"\n",
    "    pass\n",
    "\n",
    "## 2. Segment out each weld\n",
    "\n",
    "## 3. Adapt Template for weld instance type\n",
    "\n",
    "## 3.5 Compute FPFH Features\n",
    "\n",
    "## 4. Global Registration (Mean | Median | Box Centers | RANSAC Global Registration | Fast Global Registration)\n",
    "\n",
    "## 5. Local Registration \n",
    "\n",
    "## EXTRA_1: Refactor weld instance from dictionary to class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a96912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0.exe: Initializing Model\n",
    "model_params_path = \"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\"\n",
    "\n",
    "model = configure_model(model_params_path)\n",
    "eps = 2\n",
    "min_samples = 20\n",
    "instance_weighting = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2cc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  9.920997142791748\n",
      "\n",
      "create_template::scan_data.shape\n",
      "(2922, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 1.exe: Creating Template\n",
    "scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "templates = create_template(model, scan_path, tag=\"TEMPLATE\", dir_path=\"./plots_tests_3/\")\n",
    "\n",
    "if len(templates) == 2:\n",
    "    weld_source_right = templates[0][\"at_mid\"][:, :2]\n",
    "else:\n",
    "    weld_source_right = templates[\"at_mid\"][:, :2]\n",
    "weld_source_left = weld_source_right * -1\n",
    "\n",
    "weld_source_left = np.hstack((weld_source_left, np.zeros((weld_source_left.shape[0], 1))))\n",
    "weld_source_right = np.hstack((weld_source_right, np.zeros((weld_source_right.shape[0], 1))))\n",
    "\n",
    "## Checking by visualizing\n",
    "save_cluster_plots(weld_source_left, weld_source_left[:, 2], \\\n",
    "                       title=\"left_facing_weld_template\", dir_path=\"./plots_tests_2/\")\n",
    "                   \n",
    "save_cluster_plots(weld_source_right, weld_source_right[:, 2], \\\n",
    "                       title=\"right_facing_weld_template\", dir_path=\"./plots_tests_2/\")\n",
    "\n",
    "templates_dict = {\n",
    "    \"[\": {\"xyz\": weld_source_right}, \n",
    "    \"]\": {\"xyz\": weld_source_left}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed81ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Segment out each weld\n",
    "## Generate a list of welds after processing a scan\n",
    "def get_weld_instances(model, scan_path, tag=\"\", dir_path=None):\n",
    "    \n",
    "    ## preclustering: inference, filter out background, save plots\n",
    "    scan_data, metaA = run_preclustering(model, scan_path, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ## clustering: DBSCAN, save_plots, add cluster col to scan_data [cols: 5->6] \n",
    "    print(\"get_weld_instances::scan_data.shape\")\n",
    "    print(scan_data.shape)\n",
    "    scan_data = run_DBSCAN_clustering_2D(scan_data, eps, min_samples, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "    ## postclustering: filter out noise and merged data, limit cluster \n",
    "    ## count to count from instance predictions, majority votes and \n",
    "    ## add to scan_data [cols: 6->8], save plots\n",
    "    scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ### splitting for weld instances\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=tag)\n",
    "    \n",
    "    \n",
    "    ## Visualizing each weld separately if a dir_path is spec'd\n",
    "    if dir_path:\n",
    "        for weld in welds:\n",
    "            save_cluster_plots(weld[\"sub\"], np.zeros((weld[\"sub\"].shape[0], 1)), \\\n",
    "                       title=weld[\"tag\"], dir_path=dir_path)\n",
    "            \n",
    "    return welds, scan_data\n",
    "\n",
    "def get_weld_instances_from_3D_clustering(model, scan_path, eps, min_samples, instance_weighting, tag=\"\", dir_path=None):\n",
    "    \n",
    "    ## preclustering: inference, filter out background, save plots\n",
    "    ## scan_data -->  Nx5 data of non-background points --> x|y|z|cls|inst\n",
    "    ## tags each predictions plot  [RED FLAG] if the output dir is specified as None\n",
    "    scan_data, metaA = run_preclustering(model, scan_path, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ## clustering: 3D DBSCAN clustering using the instance predictions as z axis for added separation\n",
    "    ##  then save_plots, add cluster col to scan_data [cols: 5->6] ==> x|y|z|cls|inst|clustering\n",
    "    \n",
    "#     print(\"get_weld_instances_from_3D_clustering::scan_data.shape\")\n",
    "#     print(scan_data.shape)\n",
    "    scan_data = run_DBSCAN_clustering_3D(scan_data, eps, min_samples, instance_weighting=instance_weighting, sample_tag=tag, dir_path=dir_path)\n",
    "\n",
    "    ## postclustering: \n",
    "    ## filter out noise and merged data\n",
    "    ## limit cluster count to count from instance predictions, eg if 4 welds predicted, use 4 biggest clusters\n",
    "    ## majority votes for type and instance according to prediction\n",
    "    ## add to scan_data [cols: 6->8], save plots\n",
    "    scan_data, metaB = run_postclustering(scan_data, sample_tag=tag, dir_path=dir_path)\n",
    "    \n",
    "    ### splitting for weld instances\n",
    "    welds = itemize_welds(scan_data, 6, 7, sample_tag=tag)\n",
    "    \n",
    "    \n",
    "    ## Visualizing each weld separately if a dir_path is spec'd\n",
    "    if dir_path:\n",
    "        for weld in welds:\n",
    "            save_cluster_plots(weld[\"sub\"], np.zeros((weld[\"sub\"].shape[0], 1)), \\\n",
    "                       title=weld[\"tag\"], dir_path=dir_path)\n",
    "            \n",
    "    return welds, scan_data\n",
    "    \n",
    "## 3. Match template to weld instance by type\n",
    "# templates_dict = {\"[\": weld_source_right, \"]\": weld_source_left}\n",
    "def get_matching_template(weld, templates_dict):\n",
    "    return templates_dict[weld[\"type\"]]\n",
    "\n",
    "## 3.5 Compute FPFH Features as part of weld instance\n",
    "## Transforming the points of cloud to projections on the same 0-plane\n",
    "def compute_FPFH_for_weld_points_2D(weld, voxel_size):\n",
    "    \"\"\"\n",
    "    Downsamples, by specified voxel size. Units: mm\n",
    "    Computes a 33-d vector for each downsampled point that describes the local geometry of the point\n",
    "    Augments the weld dict with \n",
    "     - - _down --> the xy0 per downsample\n",
    "     - - _fpfh --> the 33d vector per downsample\n",
    "    \"\"\"\n",
    "    ## Projecting points to the z=0 plane\n",
    "    xyz = np.zeros((weld[\"sub\"].shape[0], 3))\n",
    "    xyz[:,:2 ] = weld[\"sub\"][:,:2 ]\n",
    "    weld[\"xyz\"] = xyz\n",
    "    \n",
    "    ## Computing Downsampling and FastPointFeatureHistogram of weld\n",
    "    weld[\"down\"], weld[\"fpfh\"] = compute_FPFH(xyz, voxel_size)\n",
    "    \n",
    "    \n",
    "def compute_FPFH(xyz, voxel_size):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    \n",
    "    ## Downsampling by specified voxel-size, mm\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "    \n",
    "    ## Computing normals of downsampling\n",
    "    radius_normal = voxel_size*2\n",
    "    pcd_down.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "    \n",
    "    ## Computing Fast Point Feature Histograms\n",
    "    radius_feature = voxel_size*5\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(pcd_down, \n",
    "                                o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    \n",
    "    return pcd_down, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58842783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e991dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Global Registration (Mean | Median | Box Centers ie mids | RANSAC Global Registration | Fast Global Registration)\n",
    "## Getting and Initial Transfrom to feed into and initialize the local registration\n",
    "## Mean, Median, Mids are already precalculated as entries in the welds' dictionaries \n",
    "##   via x_mean, y_mean; x_median, y_median; x_min, x_max, y_min, y_max\n",
    "## Functions for RANdom SAmpling Concensus and Fast Global Registration by FPFH \n",
    "##   to return a 4x4 transfrom that roughly moves the re-zeroed source to the given weld instance \n",
    "\n",
    "# def global_reg_by_RANSAC(template_down, weld_down, template_fpfh, weld_fpfh, voxel_size):\n",
    "def global_reg_by_RANSAC(template, weld, voxel_size):\n",
    "    tmpl_down, tmpl_fpfh = template[\"down\"], template[\"fpfh\"]\n",
    "    weld_down, weld_fpfh = weld[\"down\"], weld[\"fpfh\"]\n",
    "    \n",
    "    dist_threshold = voxel_size*1.5\n",
    "    \n",
    "    ## Args: source, target, source_feature, target_feature, mutual_filter, \n",
    "    ##     max_correspondence_dist, estimation, ransac_n ie sampling size, \n",
    "    ##     correspondence_checkers ie pruning algos, convergence_criteria\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        tmpl_down, weld_down, tmpl_fpfh, weld_fpfh, True, \n",
    "        dist_threshold, o3d.pipelines.registration.TransformationEstimationPointToPoint(False), 3, \n",
    "        [o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.9),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                dist_threshold)], o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999))\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "\n",
    "def global_reg_by_FGR(template, weld, voxel_size):\n",
    "    tmpl_down, tmpl_fpfh = template[\"down\"], template[\"fpfh\"]\n",
    "    weld_down, weld_fpfh = weld[\"down\"], weld[\"fpfh\"]\n",
    "    \n",
    "    dist_threshold = voxel_size*1.5\n",
    "    \n",
    "    ## Args: source, target, source_feature, target_feature, fgr_options\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "                tmpl_down, weld_down, tmpl_fpfh, weld_fpfh, \n",
    "                o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "                    maximum_correspondence_distance=dist_threshold))\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "        \n",
    "def global_reg_by_mean(template, weld):\n",
    "    transform = np.identity(4)\n",
    "    transform[:3, -1] = np.array([weld[\"x_mean\"], weld[\"y_mean\"], 0])\n",
    "    return transform, None, None, None\n",
    "\n",
    "def global_reg_by_median(template, weld):\n",
    "    transform = np.identity(4)\n",
    "    transform[:3, -1] = np.array([weld[\"x_median\"], weld[\"y_median\"], 0])\n",
    "    return transform, None, None, None\n",
    "    \n",
    "def global_reg_by_mid(template, weld):\n",
    "    transform = np.identity(4)\n",
    "    x_mid = (weld[\"x_min\"] + weld[\"x_max\"])/2\n",
    "    y_mid = (weld[\"y_min\"] + weld[\"y_max\"])/2\n",
    "    transform[:3, -1] = np.array([x_mid, y_mid, 0])\n",
    "    return transform, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ec8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Local Registration (Point-to-Point ICP vs Point-to-Plane ICP)\n",
    "## With an initial transform guess, refine guess until within stopping criteria\n",
    "## The refined transform is to be deconstructed for the finer x and y and yaw potentially \n",
    "##  of the center of each weld, which are to collected for output to a temp file\n",
    "## To be verified by the GNN UI\n",
    "\n",
    "def point_to_point_ICP(template_xyz, weld_xyz, init_transform, voxel_size):\n",
    "    dist_threshold = voxel_size*0.4\n",
    "    \n",
    "    ## Args: source, target, max_correspondence_distance, init_transform, \n",
    "    ##     estimation method, convergence criteria\n",
    "    result = o3d.pipelines.registration.registration_icp(template_xyz, weld_xyz, dist_threshold, init_transform,\n",
    "                                                        o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "\n",
    "def point_to_plane_ICP(template_xyz, weld_xyz, init_transform, voxel_size):\n",
    "    dist_threshold = voxel_size*0.4\n",
    "    \n",
    "    ## Args: source, target, max_correspondence_distance, init_transform, \n",
    "    ##     estimation method, convergence criteria\n",
    "    result = o3d.pipelines.registration.registration_icp(template_xyz, weld_xyz, dist_threshold, init_transform,\n",
    "                                                        o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "    \n",
    "    return result.transformation, result.fitness, result.inlier_rmse, result.correspondence_set\n",
    "\n",
    "def transform_to_xyztheta(transform):\n",
    "    \n",
    "    ## X, y, theta from transform\n",
    "    x = transform[0, -1]\n",
    "    y = transform[1, -1]\n",
    "    z = transform[2, -1]\n",
    "    theta = np.arccos(transform[0,0])\n",
    "    \n",
    "    return x,y,z,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a4c57ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  11.063999891281128\n",
      "\n",
      "get_weld_instances::scan_data.shape\n",
      "(2888, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2.exe: Collecting welds to loop through\n",
    "scan_path = \"./_data/scans/LH-3-231201600-Pass-2023_06_12-9-38-37-588.ply\"\n",
    "welds, _ = get_weld_instances(model, scan_path, tag=\"TEST\", dir_path=\"./plots_tests_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b122273f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weld ID:  1\n",
      "Weld type:  [\n",
      "Weld tag:  TEST_weld1\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.5301837270341208 0.19672737004831298\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.5021872265966754 0.20083992012417337\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (-75.45001602172852, 12.424999376758933, 0.0, 0.0)\n",
      "-- -- FGR:  (-76.59411868778508, 13.496627766971184, -0.0, 0.18501002694000118)\n",
      "-- -- mean:  (-76.70888387974229, 11.858549511060076, 0.0, 0.0)\n",
      "-- -- median:  (-76.70000457763672, 11.549999713897705, 0.0, 0.0)\n",
      "-- -- mid:  (-76.40000915527344, 11.87499962002039, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.05653170359052712 0.05142112301461699\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.04583651642475172 0.05400742973371009\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.051098731406921276\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05357055503310873\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.0748663101604278 0.05305645324772217\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (-75.43996660924583, 12.439386945059965, 0.0, 0.0006688827382727644)\n",
      "-- -- FGR:  (-76.58842978697113, 13.49385440422537, 0.0, 0.18405940504368032)\n",
      "-- -- mean:  (-76.63408824246494, 11.862620168082591, 0.0, 0.00035699788005462397)\n",
      "-- -- median:  (-76.6989157690891, 11.552046547860346, 0.0, 0.0010146407944573057)\n",
      "-- -- mid:  (-76.42277214183724, 11.889519636161547, 0.0, 0.0011980810462876052)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.05653170359052712 0.05142112301461696\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.04583651642475172 0.05400742973371015\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05109873140692078\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05357055503310855\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.0748663101604278 0.0530564532477236\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (-75.43996660924583, 12.439386945059967, 0.0, 0.0006688827382727644)\n",
      "-- -- FGR:  (-76.58842978697115, 13.493854404225365, 0.0, 0.1840594050436797)\n",
      "-- -- mean:  (-76.63408824246497, 11.862620168082593, 0.0, 0.00035699788005462397)\n",
      "-- -- median:  (-76.69891576908908, 11.552046547860344, 0.0, 0.0010146407944573057)\n",
      "-- -- mid:  (-76.42277214183726, 11.889519636161545, 0.0, 0.0011980810458242712)\n",
      "\n",
      "Weld ID:  2\n",
      "Weld type:  [\n",
      "Weld tag:  TEST_weld2\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.600174978127734 0.1944567610210616\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.657917760279965 0.19885350645826075\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (16.824483766794657, 12.574719799390973, 0.0, 0.08410540625655188)\n",
      "-- -- FGR:  (16.973724961590932, 13.061283494972642, -0.0, 0.0)\n",
      "-- -- mean:  (16.671554852907477, 12.78057075621417, 0.0, 0.0)\n",
      "-- -- median:  (16.49999237060547, 12.337499976158142, 0.0, 0.0)\n",
      "-- -- mid:  (17.149993896484375, 12.874999612569809, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.052614641488350246\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07944996180290298 0.050208857942080326\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.06340718105423988 0.04958382041375862\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061115355233002294 0.054983793934947574\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.04355504448922343\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (16.849469432221117, 12.556750864174917, 0.0, 0.07714087613185858)\n",
      "-- -- FGR:  (16.969657553014414, 12.97026654409567, 0.0, 0.0036855921794863994)\n",
      "-- -- mean:  (16.692367315112786, 12.7606167122272, 0.0, 0.0005644868118809546)\n",
      "-- -- median:  (16.52232450411378, 12.338674700908584, 0.0, 0.0021623135594923157)\n",
      "-- -- mid:  (17.14011695922101, 12.951869118614445, 0.0, 8.487226064488343e-05)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.052614641488349614\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07944996180290298 0.05020885794208041\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.06340718105423988 0.04958382041375935\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.061115355233002294 0.05498379393494659\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.043555044489222525\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (16.84946943222112, 12.556750864174917, 0.0, 0.07714087613186145)\n",
      "-- -- FGR:  (16.969657553014418, 12.97026654409567, 0.0, 0.0036855921794863994)\n",
      "-- -- mean:  (16.692367315112776, 12.760616712227202, 0.0, 0.0005644868118809546)\n",
      "-- -- median:  (16.52232450411378, 12.338674700908582, 0.0, 0.0021623135594409713)\n",
      "-- -- mid:  (17.140116959221018, 12.951869118614448, 0.0, 8.487226326110456e-05)\n",
      "\n",
      "Weld ID:  3\n",
      "Weld type:  [\n",
      "Weld tag:  TEST_weld3\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.6124234470691163 0.19542132982949448\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.7069116360454943 0.19575472730534949\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (27.80436679392755, 11.705985116905547, 0.0, 0.03131120281064451)\n",
      "-- -- FGR:  (28.631504433617344, 11.312664834777326, -0.0, 0.0)\n",
      "-- -- mean:  (28.34323322847954, 10.96906514373809, 0.0, 0.0)\n",
      "-- -- median:  (28.199996948242188, 9.349999904632568, 0.0, 0.0)\n",
      "-- -- mid:  (28.50999221801758, 11.774999616667628, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.06799083269671505 0.05706802681493617\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.05394384749990139\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08097784568372804 0.054013419707839676\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.060351413292589765 0.048809877785348864\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.054298270084972525\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (27.82497310417815, 11.698031663571522, 0.0, 0.03098750249256388)\n",
      "-- -- FGR:  (28.682332926983545, 11.338269367699226, 0.0, 0.003517458400574025)\n",
      "-- -- mean:  (28.335598633382915, 10.981644700195433, 0.0, 0.003292993718726247)\n",
      "-- -- median:  (28.1449951455654, 9.34716859024786, 0.0, 0.0008451531269451214)\n",
      "-- -- mid:  (28.470659621154848, 11.714265462657147, 0.0, 0.008242266344548364)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.06799083269671505 0.057068026814935724\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.07028265851795264 0.053943847499901346\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08097784568372804 0.05401341970783947\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.060351413292589765 0.0488098777853489\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.05429827008497165\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (27.82497310417816, 11.69803166357152, 0.0, 0.030987502492571047)\n",
      "-- -- FGR:  (28.68233292698355, 11.338269367699231, 0.0, 0.003517458400542462)\n",
      "-- -- mean:  (28.335598633382922, 10.981644700195428, 0.0, 0.003292993718726247)\n",
      "-- -- median:  (28.144995145565403, 9.347168590247858, 0.0, 0.000845153125894213)\n",
      "-- -- mid:  (28.470659621154844, 11.714265462657146, 0.0, 0.008242266344561834)\n",
      "\n",
      "Weld ID:  4\n",
      "Weld type:  ]\n",
      "Weld tag:  TEST_weld4\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.5701754385964912 0.19711524715052586\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.5166666666666667 0.19541327494607993\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (71.67499351501465, 13.625000268220901, 0.0, 0.0)\n",
      "-- -- FGR:  (71.20431826477912, 11.697374964755028, -0.0, 3.0099966977802444)\n",
      "-- -- mean:  (70.96786276086586, 12.194696806943892, 0.0, 0.0)\n",
      "-- -- median:  (71.09999338785808, 11.75, 0.0, 0.0)\n",
      "-- -- mid:  (70.64999389648438, 12.583333214124043, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.054239877769289534 0.05447898044243861\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.05547527460527819\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.09014514896867838 0.05215179761573007\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.07104660045836517 0.05186404339114506\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.08326967150496563 0.05472166278894629\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (71.63742372452569, 13.585887943137637, 0.0, 0.0009124890012384704)\n",
      "-- -- FGR:  (71.2027642076742, 11.70310773962994, 0.0, 3.0100695987564063)\n",
      "-- -- mean:  (70.98964435716057, 12.188150122187245, 0.0, 0.0029450297122656736)\n",
      "-- -- median:  (71.03628551341774, 11.768254256651236, 0.0, 0.00047213977667327723)\n",
      "-- -- mid:  (70.64437483252466, 12.58036940018812, 0.0, 0.000134721399279547)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.054239877769289534 0.05447898044243791\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.061879297173414824 0.055475274605278284\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.09014514896867838 0.05215179761573036\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.07104660045836517 0.051864043391145226\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.08326967150496563 0.05472166278894613\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (71.63742372452569, 13.585887943137639, 0.0, 0.0009124890017251495)\n",
      "-- -- FGR:  (71.20276420767422, 11.703107739629942, 0.0, 3.0100695987564063)\n",
      "-- -- mean:  (70.9896443571606, 12.188150122187245, 0.0, 0.002945029712227975)\n",
      "-- -- median:  (71.03628551341778, 11.768254256651234, 0.0, 0.00047213977784901285)\n",
      "-- -- mid:  (70.64437483252468, 12.580369400188117, 0.0, 0.000134721399279547)\n",
      "\n",
      "Weld ID:  5\n",
      "Weld type:  ]\n",
      "Weld tag:  TEST_weld5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.6236842105263158 0.186049182945404\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.5447368421052632 0.1942277448735041\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (82.25587864390668, 12.872363823858846, 0.0, 0.23955024844256692)\n",
      "-- -- FGR:  (82.31270094346942, 15.762220305855932, -0.0, 3.1075408738070633)\n",
      "-- -- mean:  (82.30348323382707, 14.210766498392717, 0.0, 0.0)\n",
      "-- -- median:  (82.44999694824219, 14.47499966621399, 0.0, 0.0)\n",
      "-- -- mid:  (81.87499364217123, 14.149999856948853, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.05618690162437299\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.06417112299465241 0.052814018116671364\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08403361344537816 0.05562431414638576\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.09472880061115355 0.05446143087022958\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.10236822001527884 0.054711404415762305\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (82.22157309608335, 12.864001009479368, 0.0, 0.23192201929794887)\n",
      "-- -- FGR:  (82.31837413810189, 15.669245638107292, 0.0, 3.111927098476774)\n",
      "-- -- mean:  (82.29202583722454, 14.209322029732808, 0.0, 0.0014139411819621035)\n",
      "-- -- median:  (82.41699957600899, 14.384563372233718, 0.0, 0.0025974131461868736)\n",
      "-- -- mid:  (81.83958117037615, 14.1860332322698, 0.0, 0.0006773802274928631)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.09702062643239114 0.056186901624374355\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.06417112299465241 0.052814018116671725\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.08403361344537816 0.055624314146385814\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.09472880061115355 0.05446143087023\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.10236822001527884 0.054711404415762686\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (82.22157309608338, 12.864001009479365, 0.0, 0.2319220192979479)\n",
      "-- -- FGR:  (82.31837413810192, 15.669245638107292, 0.0, 3.111927098476774)\n",
      "-- -- mean:  (82.29202583722454, 14.209322029732812, 0.0, 0.0014139411819621035)\n",
      "-- -- median:  (82.41699957600898, 14.384563372233714, 0.0, 0.0025974131458876694)\n",
      "-- -- mid:  (81.83958117037614, 14.1860332322698, 0.0, 0.0006773802266733654)\n",
      "\n",
      "Weld ID:  6\n",
      "Weld type:  ]\n",
      "Weld tag:  TEST_weld6\n",
      "\n",
      "Global Registration\n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.6412280701754386 0.19218865308466643\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.6938596491228071 0.19328465254457688\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (92.53526896720311, 15.618580888585516, 0.0, 0.008261546666379679)\n",
      "-- -- FGR:  (92.47648903524241, 14.154685745482137, -0.0, 0.0)\n",
      "-- -- mean:  (92.7885320432354, 14.57595016570143, 0.0, 0.0)\n",
      "-- -- median:  (93.29999542236328, 15.25, 0.0, 0.0)\n",
      "-- -- mid:  (92.19166056315103, 13.949999928474426, 0.0, 0.0)\n",
      "\n",
      "Local Registration: Point 2 Point \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.0718105423987777 0.050338712047594915\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.08938120702826585 0.05088631292693545\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.07868601986249045 0.055870921328772895\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.05576776165011459 0.057238398227506225\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.054988682094888594\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (92.53681601847202, 15.598588116384985, 0.0, 0.005750083264880892)\n",
      "-- -- FGR:  (92.44292923828375, 14.180012164465744, 0.0, 0.0010932542779134146)\n",
      "-- -- mean:  (92.81893836284716, 14.579020290573085, 0.0, 0.0032352800821497303)\n",
      "-- -- median:  (93.29155606662519, 15.235808504655145, 0.0, 0.00010435138389263829)\n",
      "-- -- mid:  (92.19970258518362, 13.966947640421056, 0.0, 0.0014368700623996313)\n",
      "\n",
      "Local Registration: Point 2 Plane \n",
      "-- RANSAC\n",
      "-- -- fitness, inlier_rmse:  0.0718105423987777 0.05033871204759545\n",
      "-- FGR\n",
      "-- -- fitness, inlier_rmse:  0.08938120702826585 0.05088631292693499\n",
      "-- mean\n",
      "-- -- fitness, inlier_rmse:  0.07868601986249045 0.0558709213287744\n",
      "-- median\n",
      "-- -- fitness, inlier_rmse:  0.05576776165011459 0.05723839822750642\n",
      "-- mid\n",
      "-- -- fitness, inlier_rmse:  0.09090909090909091 0.054988682094888774\n",
      "\n",
      "-- X|Y|Z|theta\n",
      "-- -- RANSAC:  (92.53681601847204, 15.598588116384986, 0.0, 0.005750083264842275)\n",
      "-- -- FGR:  (92.44292923828368, 14.180012164465753, 0.0, 0.0010932542779134146)\n",
      "-- -- mean:  (92.81893836284715, 14.579020290573085, 0.0, 0.003235280081978149)\n",
      "-- -- median:  (93.29155606662519, 15.235808504655152, 0.0, 0.00010435138602049319)\n",
      "-- -- mid:  (92.19970258518366, 13.966947640421054, 0.0, 0.0014368700623996313)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3,4,5.exe : Looping through collected welds\n",
    "\n",
    "voxel_size = 0.2\n",
    "\n",
    "for idx, weld in enumerate(welds, start=1):\n",
    "    print(\"Weld ID: \", weld[\"id\"])\n",
    "    print(\"Weld type: \", weld[\"type\"])\n",
    "    print(\"Weld tag: \", weld[\"tag\"]) \n",
    "    print()\n",
    "    \n",
    "    ## 3. Adapt Template for weld instance type\n",
    "    template = get_matching_template(weld, templates_dict)\n",
    "    \n",
    "    ## 3.5 Compute FPFH Features\n",
    "    ## For weld\n",
    "    compute_FPFH_for_weld_points_2D(weld, voxel_size)\n",
    "    ## For Template\n",
    "    template_down, template_fpfh = compute_FPFH(template[\"xyz\"], voxel_size)\n",
    "    template[\"down\"], template[\"fpfh\"] = template_down, template_fpfh\n",
    "    \n",
    "    ## 4. Global Registration (Mean | Median | Box Centers | RANSAC Global Registration | Fast Global Registration)\n",
    "    T_RANSAC, fitness_RANSAC, inlier_rmse_RANSAC, _ = global_reg_by_RANSAC(template, weld, voxel_size)\n",
    "    T_FGR, fitness_FGR, inlier_rmse_FGR, _ = global_reg_by_FGR(template, weld, voxel_size)\n",
    "    T_mean, _, _, _ = global_reg_by_mean(template, weld)\n",
    "    T_median, _, _, _ = global_reg_by_median(template, weld)\n",
    "    T_mid, _, _, _ = global_reg_by_mid(template, weld)\n",
    "    \n",
    "    print(\"Global Registration\")\n",
    "    print(\"-- RANSAC\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", fitness_RANSAC, inlier_rmse_RANSAC)\n",
    "    print(\"-- FGR\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", fitness_FGR, inlier_rmse_FGR)\n",
    "    print()\n",
    "    print(\"-- X|Y|Z|theta\")\n",
    "    print(\"-- -- RANSAC: \", transform_to_xyztheta(T_RANSAC))\n",
    "    print(\"-- -- FGR: \", transform_to_xyztheta(T_FGR))\n",
    "    print(\"-- -- mean: \", transform_to_xyztheta(T_mean))\n",
    "    print(\"-- -- median: \", transform_to_xyztheta(T_median))\n",
    "    print(\"-- -- mid: \", transform_to_xyztheta(T_mid))\n",
    "    print()\n",
    "    \n",
    "    ## 5. Local Registration: Refining\n",
    "    \n",
    "    ## Represent as Point Cloud\n",
    "    \n",
    "    weld_xyz = o3d.geometry.PointCloud()\n",
    "    weld_xyz.points = o3d.utility.Vector3dVector(weld[\"xyz\"])\n",
    "    \n",
    "    template_xyz = o3d.geometry.PointCloud()\n",
    "    template_xyz.points = o3d.utility.Vector3dVector(template[\"xyz\"])\n",
    "    \n",
    "    ## Point to Point ICP using T_RANSAC, T_FGR, T_mean, F_median, F_mid respectively as Initial Transforms \n",
    "    \n",
    "    f_T_p2p_RANSAC, f_fitness_p2p_RANSAC, f_inlier_rmse_p2p_RANSAC, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_RANSAC, voxel_size)\n",
    "    \n",
    "    f_T_p2p_FGR, f_fitness_p2p_FGR, f_inlier_rmse_p2p_FGR, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_FGR, voxel_size)\n",
    "    \n",
    "    f_T_p2p_mean, f_fitness_p2p_mean, f_inlier_rmse_p2p_mean, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mean, voxel_size)\n",
    "    \n",
    "    f_T_p2p_median, f_fitness_p2p_median, f_inlier_rmse_p2p_median, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_median, voxel_size)\n",
    "    \n",
    "    f_T_p2p_mid, f_fitness_p2p_mid, f_inlier_rmse_p2p_mid, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mid, voxel_size)\n",
    "    \n",
    "    print(\"Local Registration: Point 2 Point \")\n",
    "    print(\"-- RANSAC\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_RANSAC, f_inlier_rmse_p2p_RANSAC)\n",
    "    print(\"-- FGR\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_FGR, f_inlier_rmse_p2p_FGR)\n",
    "    print(\"-- mean\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_mean, f_inlier_rmse_p2p_mean)\n",
    "    print(\"-- median\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_median, f_inlier_rmse_p2p_median)\n",
    "    print(\"-- mid\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2p_mid, f_inlier_rmse_p2p_mid)\n",
    "    print()\n",
    "    print(\"-- X|Y|Z|theta\")\n",
    "    print(\"-- -- RANSAC: \", transform_to_xyztheta(f_T_p2p_RANSAC))\n",
    "    print(\"-- -- FGR: \", transform_to_xyztheta(f_T_p2p_FGR))\n",
    "    print(\"-- -- mean: \", transform_to_xyztheta(f_T_p2p_mean))\n",
    "    print(\"-- -- median: \", transform_to_xyztheta(f_T_p2p_median))\n",
    "    print(\"-- -- mid: \", transform_to_xyztheta(f_T_p2p_mid))\n",
    "    print()\n",
    "    \n",
    "    ## Point to Point ICP using T_RANSAC, T_FGR, T_mean, F_median, F_mid respectively as Initial Transforms \n",
    "    \n",
    "    f_T_p2pl_RANSAC, f_fitness_p2pl_RANSAC, f_inlier_rmse_p2pl_RANSAC, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_RANSAC, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_FGR, f_fitness_p2pl_FGR, f_inlier_rmse_p2pl_FGR, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_FGR, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_mean, f_fitness_p2pl_mean, f_inlier_rmse_p2pl_mean, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mean, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_median, f_fitness_p2pl_median, f_inlier_rmse_p2pl_median, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_median, voxel_size)\n",
    "    \n",
    "    f_T_p2pl_mid, f_fitness_p2pl_mid, f_inlier_rmse_p2pl_mid, _ = point_to_point_ICP(\n",
    "        template_xyz, weld_xyz, T_mid, voxel_size)\n",
    "    \n",
    "    print(\"Local Registration: Point 2 Plane \")\n",
    "    print(\"-- RANSAC\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_RANSAC, f_inlier_rmse_p2pl_RANSAC)\n",
    "    print(\"-- FGR\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_FGR, f_inlier_rmse_p2pl_FGR)\n",
    "    print(\"-- mean\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_mean, f_inlier_rmse_p2pl_mean)\n",
    "    print(\"-- median\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_median, f_inlier_rmse_p2pl_median)\n",
    "    print(\"-- mid\")\n",
    "    print(\"-- -- fitness, inlier_rmse: \", f_fitness_p2pl_mid, f_inlier_rmse_p2pl_mid)\n",
    "    print()\n",
    "    print(\"-- X|Y|Z|theta\")\n",
    "    print(\"-- -- RANSAC: \", transform_to_xyztheta(f_T_p2pl_RANSAC))\n",
    "    print(\"-- -- FGR: \", transform_to_xyztheta(f_T_p2pl_FGR))\n",
    "    print(\"-- -- mean: \", transform_to_xyztheta(f_T_p2pl_mean))\n",
    "    print(\"-- -- median: \", transform_to_xyztheta(f_T_p2pl_median))\n",
    "    print(\"-- -- mid: \", transform_to_xyztheta(f_T_p2pl_mid))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "805a57c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'at_mean': array([[ 0.47552498, -4.87945503,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 2.92552574, -4.22945493,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.72552116, -9.029455  ,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 3.48632034,  8.00074502,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 4.13631805,  8.25074534,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [-0.96368042, 11.30074521,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]]),\n",
       "  'at_median': array([[ 0.54999924, -4.30000019,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.        , -3.6500001 ,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.79999542, -8.45000017,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 3.59999847, 10.4000001 ,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 4.24999619, 10.65000041,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [-0.85000229, 13.70000029,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]]),\n",
       "  'at_mid': array([[ 0.25000381, -5.05000029,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 2.70000458, -4.40000019,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 3.5       , -9.20000026,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 3.35000229,  7.27499994,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 4.        ,  7.52500026,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [-1.09999847, 10.57500013,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]]),\n",
       "  'at_corner': array([[ 4.35000229,  6.55      ,  3.59000003, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 6.80000305,  7.20000009,  2.55249989, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 7.59999847,  2.40000002,  2.56166661, ...,  5.        ,\n",
       "           1.        ,  1.        ],\n",
       "         ...,\n",
       "         [ 7.35000229, 18.80000019,  2.00249994, ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 8.        , 19.05000051,  1.8466666 , ...,  2.        ,\n",
       "           1.        ,  3.        ],\n",
       "         [ 2.90000153, 22.10000039,  1.83249992, ...,  2.        ,\n",
       "           1.        ,  3.        ]])},\n",
       " {'at_mean': array([[-1.99669032,  5.67663618,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.30331273, -5.47336392,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 0.85331578, -2.07336414,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [-2.17007287, 10.02996473,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.97006829, 10.17996435,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.52007135, 10.37996416,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]]),\n",
       "  'at_median': array([[-2.30000305,  6.4000001 ,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.        , -4.75      ,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 0.55000305, -1.35000022,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [-2.65000153,  9.40000057,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-1.44999695,  9.55000019,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-1.        ,  9.75      ,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]]),\n",
       "  'at_mid': array([[-1.67499924,  5.20000029,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.62500381, -5.94999981,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 1.17500687, -2.55000003,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [-1.72500229, 10.55000067,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.52499771, 10.70000029,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [-0.07500076, 10.9000001 ,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]]),\n",
       "  'at_corner': array([[ 2.34999847, 16.60000038,  1.48499995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 5.65000153,  5.45000029,  1.42999995, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         [ 5.20000458,  8.85000006,  1.22166663, ...,  1.        ,\n",
       "           2.        ,  4.        ],\n",
       "         ...,\n",
       "         [ 2.34999847, 22.45000076,  1.27499998, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [ 3.55000305, 22.60000038,  1.38749996, ...,  4.        ,\n",
       "           2.        ,  6.        ],\n",
       "         [ 4.        , 22.80000019,  1.19499999, ...,  4.        ,\n",
       "           2.        ,  6.        ]])})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ddc4252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup Time (secs) : 0.06300210952758789\n",
      "./_data/labels/RH-5-231201627-Pass-2023_06_13-9-19-08-357.txt\n",
      "./_data/labels/LH-5-231201608-Pass-2023_06_12-10-56-33-911.txt\n",
      "./_data/labels/LH-5-231201605-Pass-2023_06_12-10-46-15-717.txt\n",
      "./_data/labels/LH-5-231201596-Pass-2023_06_12-9-22-54-270.txt\n",
      "./_data/labels/RH-10-231201591-Pass-2023_06_13-11-42-25-737.txt\n",
      "./_data/labels/RH-7-231201627-Fail-2023_06_13-9-19-17-681.txt\n",
      "./_data/labels/RH-9-231201590-Pass-2023_06_13-11-39-05-753.txt\n",
      "./_data/labels/RH-7-231201626-Fail-2023_06_13-9-13-27-075.txt\n",
      "./_data/labels/LH-3-231201605-Pass-2023_06_12-10-46-06-374.txt\n",
      "./_data/labels/LH-1--2312014-NoResult-l-2023_06_02-7-56-18-48.txt\n",
      "./_data/labels/RH-6-231201597-Pass-2023_06_13-12-22-18-478.txt\n",
      "./_data/labels/LH-2-231201596-Pass-2023_06_12-9-22-40-303.txt\n",
      "./_data/labels/LH-6-231201595-Pass-2023_06_12-9-21-22-515.txt\n",
      "./_data/labels/LH-7-231201595-Pass-2023_06_12-9-21-27-188.txt\n",
      "./_data/labels/RH-6-231201590-Pass-2023_06_13-11-38-41-053.txt\n",
      "./_data/labels/RH-8-231201598-Pass-2023_06_13-12-39-49-771.txt\n",
      "./_data/labels/LH-3-231201600-Pass-2023_06_12-9-38-37-588.txt\n",
      "./_data/labels/LH-4-231201595-Pass-2023_06_12-9-21-13-183.txt\n",
      "./_data/labels/LH-3-231201598-Pass-2023_06_12-9-30-26-590.txt\n",
      "./_data/labels/RH-8-231201592-Pass-2023_06_13-11-45-02-878.txt\n",
      "./_data/labels/LH-6-231201596-Pass-2023_06_12-9-22-58-919.txt\n",
      "./_data/labels/RH-3-231201626-Pass-2023_06_13-9-13-08-435.txt\n",
      "./_data/labels/LH-5-231201604-Pass-2023_06_12-10-01-59-188.txt\n",
      "./_data/labels/RH-10-231201626-Pass-2023_06_13-9-13-41-018.txt\n",
      "./_data/labels/RH-10-231201592-Pass-2023_06_13-11-45-19-145.txt\n",
      "./_data/labels/LH-1-231201598-Pass-2023_06_12-9-30-17-240.txt\n",
      "./_data/labels/LH-6-231201600-Pass-2023_06_12-9-38-51-617.txt\n",
      "./_data/labels/LH-3-231201597-Pass-2023_06_12-9-27-19-422.txt\n",
      "./_data/labels/RH-7-231201596-Fail-2023_06_13-12-19-11-036.txt\n",
      "./_data/labels/LH-6-231201598-Pass-2023_06_12-9-30-40-570.txt\n",
      "./_data/labels/RH-4-231201595-Pass-2023_06_13-12-14-53-295.txt\n",
      "./_data/labels/RH-2-231201591-Pass-2023_06_13-11-41-20-814.txt\n",
      "./_data/labels/RH-11-231201594-Pass-2023_06_13-12-08-56-692.txt\n",
      "./_data/labels/LH-4-231201597-Pass-2023_06_12-9-27-24-065.txt\n",
      "./_data/labels/LH-4-231201605-Pass-2023_06_12-10-46-11-096.txt\n",
      "./_data/labels/LH-3-231201599-Pass-2023_06_12-9-34-38-894.txt\n",
      "./_data/labels/LH-2-231201599-Pass-2023_06_12-9-34-34-210.txt\n",
      "./_data/labels/RH-3-231201595-Pass-2023_06_13-12-14-44-634.txt\n",
      "./_data/labels/RH-9-231201594-Fail-2023_06_13-12-08-40-959.txt\n",
      "./_data/labels/LH-3-231201596-Pass-2023_06_12-9-22-44-921.txt\n",
      "./_data/labels/RH-9-231201592-Fail-2023_06_13-11-45-10-478.txt\n",
      "./_data/labels/LH-9-231201605-Fail-2023_06_12-10-46-34-327.txt\n",
      "./_data/labels/LH-4-231201603-Pass-2023_06_12-9-58-00-685.txt\n",
      "./_data/labels/RH-9-231201588-Fail-2023_06_13-10-19-58-456.txt\n",
      "./_data/labels/LH-3-231201595-Pass-2023_06_12-9-21-08-519.txt\n",
      "./_data/labels/RH-10-231201596-Pass-2023_06_13-12-19-39-257.txt\n",
      "./_data/labels/RH-1-231201627-Pass-2023_06_13-9-18-49-630.txt\n",
      "./_data/labels/RH-7-231201590-Fail-2023_06_13-11-38-49-513.txt\n",
      "./_data/labels/RH-3-231201627-Pass-2023_06_13-9-18-59-073.txt\n",
      "./_data/labels/RH-5-231201595-Pass-2023_06_13-12-15-01-555.txt\n",
      "./_data/labels/RH-3-231201599-Pass-2023_06_13-12-43-30-651.txt\n",
      "./_data/labels/LH-8-231201603-Fail-2023_06_12-9-58-19-339.txt\n",
      "./_data/labels/LH-5-231201603-Fail-2023_06_12-9-58-05-361.txt\n",
      "./_data/labels/LH-6-231201602-Pass-2023_06_12-9-53-25-942.txt\n",
      "./_data/labels/RH-5-231201626-Pass-2023_06_13-9-13-17-760.txt\n",
      "./_data/labels/RH-9-231201596-Fail-2023_06_13-12-19-28-965.txt\n",
      "./_data/labels/LH-8-231201601-Pass-2023_06_12-9-43-14-007.txt\n",
      "./_data/labels/RH-1-231201596-Pass-2023_06_13-12-18-19-465.txt\n",
      "./_data/labels/LH-7-231201604-Pass-2023_06_12-10-02-08-485.txt\n",
      "./_data/labels/LH-2-231201595-Pass-2023_06_12-9-21-03-886.txt\n",
      "./_data/labels/RH-9-231201591-Pass-2023_06_09-10-51-11-577.txt\n",
      "./_data/labels/RH-5-231201585-Pass-2023_06_09-9-22-35-051.txt\n",
      "./_data/labels/RH-2-231201593-Pass-2023_06_09-10-54-31-162.txt\n",
      "./_data/labels/RH-1-231201588-Fail-2023_06_09-10-44-27-895.txt\n",
      "./_data/labels/RH-11-231201251-Pass-2023_06_09-9-15-24-431.txt\n",
      "./_data/labels/RH-4-231201590-Fail-2023_06_09-10-48-34-421.txt\n",
      "./_data/labels/RH-8-231201585-Pass-2023_06_09-9-22-49-024.txt\n",
      "./_data/labels/RH-5-231201587-Fail-2023_06_09-10-42-44-797.txt\n",
      "./_data/labels/RH-6-231201625-Pass-2023_06_12-14-17-48-219.txt\n",
      "./_data/labels/RH-8-231201624-Pass-2023_06_12-14-14-41-990.txt\n",
      "./_data/labels/RH-10-231201625-Pass-2023_06_12-14-18-06-818.txt\n",
      "./_data/labels/RH-9-231201583-Fail-2023_06_09-9-17-11-772.txt\n",
      "./_data/labels/RH-6-231201250-Pass-2023_06_09-9-09-12-098.txt\n",
      "./_data/labels/RH-1-231201612-Pass-2023_06_12-11-45-07-296.txt\n",
      "./_data/labels/RH-5-231201584-Pass-2023_06_09-9-19-57-787.txt\n",
      "./_data/labels/RH-2-231201584-Pass-2023_06_09-9-19-43-815.txt\n",
      "./_data/labels/RH-9-231201251-Fail-2023_06_09-9-15-15-123.txt\n",
      "./_data/labels/LH-1-231201597-Fail-2023_06_13-12-21-34-579.txt\n",
      "./_data/labels/RH-7-231201616-Fail-2023_06_12-12-14-28-799.txt\n",
      "./_data/labels/RH-2-231201588-Pass-2023_06_09-10-44-32-644.txt\n",
      "./_data/labels/RH-4-231201613-Pass-2023_06_12-11-49-07-871.txt\n",
      "./_data/labels/RH-1-231201251-Pass-2023_06_09-9-14-37-782.txt\n",
      "./_data/labels/RH-3-231201584-Pass-2023_06_09-9-19-48-487.txt\n",
      "./_data/labels/RH-7-231201585-Fail-2023_06_09-9-22-44-389.txt\n",
      "./_data/labels/RH-8-231201592-Pass-2023_06_09-10-53-26-003.txt\n",
      "./_data/labels/RH-10-231201249-Pass-2023_06_09-8-04-18-518.txt\n",
      "./_data/labels/RH-3-231201250-Pass-2023_06_09-9-08-58-142.txt\n",
      "./_data/labels/RH-5-231201251-Pass-2023_06_09-9-14-56-512.txt\n",
      "./_data/labels/RH-7-231201623-Fail-2023_06_12-14-11-19-597.txt\n",
      "./_data/labels/RH-10-231201584-Pass-2023_06_09-9-20-21-084.txt\n",
      "./_data/labels/RH-3-231201613-Fail-2023_06_12-11-49-03-245.txt\n",
      "./_data/labels/RH-4-231201622-Pass-2023_06_12-14-07-08-024.txt\n",
      "./_data/labels/RH-5-231201622-Fail-2023_06_12-14-07-12-706.txt\n",
      "./_data/labels/RH-5-231201583-Pass-2023_06_09-9-16-53-153.txt\n",
      "./_data/labels/RH-6-231201624-Pass-2023_06_12-14-14-32-709.txt\n",
      "./_data/labels/LH-1-231201599-Fail-2023_06_13-12-43-13-933.txt\n",
      "./_data/labels/RH-1-231201619-Pass-2023_06_12-12-21-51-539.txt\n",
      "./_data/labels/RH-10-231201622-Pass-2023_06_12-14-07-35-975.txt\n",
      "./_data/labels/RH-9-231201621-Fail-2023_06_12-13-55-00-115.txt\n",
      "./_data/labels/RH-5-231201625-Pass-2023_06_12-14-17-43-530.txt\n",
      "./_data/labels/RH-11-231201593-Pass-2023_06_09-10-55-13-000.txt\n",
      "./_data/labels/LH-6-231201603-Fail-2023_06_13-13-30-45-710.txt\n",
      "./_data/labels/LH-1-231201607-Fail-2023_06_13-13-44-20-591.txt\n",
      "./_data/labels/LH-5-231201597-Fail-2023_06_13-12-22-08-360.txt\n",
      "./_data/labels/RH-1-231201594-Fail-2023_06_09-10-57-27-505.txt\n",
      "./_data/labels/LH-5-231201604-Fail-2023_06_13-13-34-23-901.txt\n",
      "./_data/labels/RH-9-231201612-Fail-2023_06_12-11-45-44-597.txt\n",
      "./_data/labels/RH-7-231201621-Fail-2023_06_12-13-54-50-814.txt\n",
      "./_data/labels/LH-10-231201595-Fail-2023_06_13-12-15-48-417.txt\n",
      "./_data/labels/RH-5-231201592-Fail-2023_06_09-10-53-12-049.txt\n",
      "./_data/labels/RH-8-231201587-Pass-2023_06_09-10-42-58-781.txt\n",
      "./_data/labels/RH-8-231201251-Pass-2023_06_09-9-15-10-481.txt\n",
      "./_data/labels/RH-3-231201621-Pass-2023_06_12-13-54-32-191.txt\n",
      "./_data/labels/LH-5-231201595-Fail-2023_06_13-12-15-01-556.txt\n",
      "./_data/labels/LH-5-231201607-Fail-2023_06_13-13-44-55-051.txt\n",
      "./_data/labels/RH-7-231201617-Fail-2023_06_12-12-18-26-677.txt\n",
      "./_data/labels/RH-5-231201589-Fail-2023_06_09-10-46-42-686.txt\n",
      "./_data/labels/LH-9-231201600-Fail-2023_06_13-13-12-23-510.txt\n",
      "./_data/labels/RH-4-231201251-Pass-2023_06_09-9-14-51-848.txt\n",
      "./_data/labels/LH-5-231201601-Fail-2023_06_13-13-16-14-093.txt\n",
      "./_data/labels/RH-9-231201584-Fail-2023_06_09-9-20-16-429.txt\n",
      "Scan Inference Time (secs):  10.475001335144043\n",
      "\n",
      "Scan Inference Time (secs):  9.798996925354004\n",
      "\n",
      "Scan Inference Time (secs):  9.884998798370361\n",
      "\n",
      "Scan Inference Time (secs):  9.982001066207886\n",
      "\n",
      "Scan Inference Time (secs):  9.232001304626465\n",
      "\n",
      "Scan Inference Time (secs):  10.527001142501831\n",
      "\n",
      "Scan Inference Time (secs):  9.88999891281128\n",
      "\n",
      "Scan Inference Time (secs):  10.317000150680542\n",
      "\n",
      "Scan Inference Time (secs):  10.154000043869019\n",
      "\n",
      "Scan Inference Time (secs):  9.466998815536499\n",
      "\n",
      "Scan Inference Time (secs):  10.487965106964111\n",
      "\n",
      "Scan Inference Time (secs):  9.65596604347229\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Inference Time (secs):  10.854000568389893\n",
      "\n",
      "Scan Inference Time (secs):  10.655966758728027\n",
      "\n",
      "Scan Inference Time (secs):  10.746996402740479\n",
      "\n",
      "Scan Inference Time (secs):  10.331965446472168\n",
      "\n",
      "Scan Inference Time (secs):  10.192006587982178\n",
      "\n",
      "Scan Inference Time (secs):  9.876966714859009\n",
      "\n",
      "Scan Inference Time (secs):  11.01636004447937\n",
      "\n",
      "Scan Inference Time (secs):  10.5619637966156\n",
      "\n",
      "Scan Inference Time (secs):  9.856967449188232\n",
      "\n",
      "Scan Inference Time (secs):  10.147966861724854\n",
      "\n",
      "Scan Inference Time (secs):  9.749000072479248\n",
      "\n",
      "Scan Inference Time (secs):  9.10599946975708\n",
      "\n",
      "Scan Inference Time (secs):  9.246001482009888\n",
      "\n",
      "Scan Inference Time (secs):  10.288000583648682\n",
      "\n",
      "Scan Inference Time (secs):  9.990999698638916\n",
      "\n",
      "Scan Inference Time (secs):  10.008998155593872\n",
      "\n",
      "Scan Inference Time (secs):  10.208003997802734\n",
      "\n",
      "Scan Inference Time (secs):  10.600000619888306\n",
      "\n",
      "Scan Inference Time (secs):  10.321123361587524\n",
      "\n",
      "Scan Inference Time (secs):  9.92400074005127\n",
      "\n",
      "Scan Inference Time (secs):  9.918861389160156\n",
      "\n",
      "Scan Inference Time (secs):  9.917969465255737\n",
      "\n",
      "Scan Inference Time (secs):  9.986965656280518\n",
      "\n",
      "Scan Inference Time (secs):  10.097967386245728\n",
      "\n",
      "Scan Inference Time (secs):  9.96396541595459\n",
      "\n",
      "Scan Inference Time (secs):  10.088966369628906\n",
      "\n",
      "Scan Inference Time (secs):  9.841967582702637\n",
      "\n",
      "Scan Inference Time (secs):  10.742003917694092\n",
      "\n",
      "Scan Inference Time (secs):  10.099001407623291\n",
      "\n",
      "Scan Inference Time (secs):  10.605999231338501\n",
      "\n",
      "Scan Inference Time (secs):  9.901000022888184\n",
      "\n",
      "Scan Inference Time (secs):  9.749001502990723\n",
      "\n",
      "Scan Inference Time (secs):  10.343036651611328\n",
      "\n",
      "Scan Inference Time (secs):  9.885840654373169\n",
      "\n",
      "Scan Inference Time (secs):  10.050964117050171\n",
      "\n",
      "Scan Inference Time (secs):  9.92400050163269\n",
      "\n",
      "Scan Inference Time (secs):  10.198999881744385\n",
      "\n",
      "Scan Inference Time (secs):  10.88999605178833\n",
      "\n",
      "Scan Inference Time (secs):  10.198964834213257\n",
      "\n",
      "Scan Inference Time (secs):  10.551012754440308\n",
      "\n",
      "Scan Inference Time (secs):  10.459003925323486\n",
      "\n",
      "Scan Inference Time (secs):  9.713003635406494\n",
      "\n",
      "Scan Inference Time (secs):  10.228987455368042\n",
      "\n",
      "Scan Inference Time (secs):  10.230998992919922\n",
      "\n",
      "Scan Inference Time (secs):  10.556968927383423\n",
      "\n",
      "Scan Inference Time (secs):  9.931999206542969\n",
      "\n",
      "Scan Inference Time (secs):  10.56299901008606\n",
      "\n",
      "Scan Inference Time (secs):  9.882999181747437\n",
      "\n",
      "Scan Inference Time (secs):  10.76499605178833\n",
      "\n",
      "Scan Inference Time (secs):  10.48099970817566\n",
      "\n",
      "Scan Inference Time (secs):  11.58299970626831\n",
      "\n",
      "Scan Inference Time (secs):  10.99899959564209\n",
      "\n",
      "Scan Inference Time (secs):  9.973999500274658\n",
      "\n",
      "Scan Inference Time (secs):  11.042999505996704\n",
      "\n",
      "Scan Inference Time (secs):  10.115997791290283\n",
      "\n",
      "Scan Inference Time (secs):  10.41099214553833\n",
      "\n",
      "Scan Inference Time (secs):  10.478001117706299\n",
      "\n",
      "Scan Inference Time (secs):  10.27783489227295\n",
      "\n",
      "Scan Inference Time (secs):  9.320963621139526\n",
      "\n",
      "Scan Inference Time (secs):  10.520965337753296\n",
      "\n",
      "Scan Inference Time (secs):  10.373999834060669\n",
      "\n",
      "Scan Inference Time (secs):  10.525994777679443\n",
      "\n",
      "Scan Inference Time (secs):  10.280999898910522\n",
      "\n",
      "Scan Inference Time (secs):  9.594033241271973\n",
      "\n",
      "Scan Inference Time (secs):  11.177997589111328\n",
      "\n",
      "Scan Inference Time (secs):  12.85343885421753\n",
      "\n",
      "Scan Inference Time (secs):  10.448996782302856\n",
      "\n",
      "Scan Inference Time (secs):  11.688966512680054\n",
      "\n",
      "Scan Inference Time (secs):  10.792965650558472\n",
      "\n",
      "Scan Inference Time (secs):  10.261965990066528\n",
      "\n",
      "Scan Inference Time (secs):  10.228962898254395\n",
      "\n",
      "Scan Inference Time (secs):  10.305999279022217\n",
      "\n",
      "Scan Inference Time (secs):  10.544959545135498\n",
      "\n",
      "Scan Inference Time (secs):  9.182960987091064\n",
      "\n",
      "Scan Inference Time (secs):  9.993968725204468\n",
      "\n",
      "Scan Inference Time (secs):  10.309033870697021\n",
      "\n",
      "Scan Inference Time (secs):  10.580001831054688\n",
      "\n",
      "Scan Inference Time (secs):  9.114964246749878\n",
      "\n",
      "Scan Inference Time (secs):  10.19396710395813\n",
      "\n",
      "Scan Inference Time (secs):  10.281998872756958\n",
      "\n",
      "Scan Inference Time (secs):  10.410998344421387\n",
      "\n",
      "Scan Inference Time (secs):  10.497655630111694\n",
      "\n",
      "Scan Inference Time (secs):  10.345967292785645\n",
      "\n",
      "Scan Inference Time (secs):  12.185964584350586\n",
      "\n",
      "Scan Inference Time (secs):  11.253000259399414\n",
      "\n",
      "Scan Inference Time (secs):  10.606997966766357\n",
      "\n",
      "Scan Inference Time (secs):  10.415968418121338\n",
      "\n",
      "Scan Inference Time (secs):  10.844000101089478\n",
      "\n",
      "Scan Inference Time (secs):  11.579939603805542\n",
      "\n",
      "Scan Inference Time (secs):  11.056000471115112\n",
      "\n",
      "Scan Inference Time (secs):  13.017661809921265\n",
      "\n",
      "Scan Inference Time (secs):  11.705966234207153\n",
      "\n",
      "Scan Inference Time (secs):  10.816970109939575\n",
      "\n",
      "Scan Inference Time (secs):  11.15199613571167\n",
      "\n",
      "Scan Inference Time (secs):  10.685730934143066\n",
      "\n",
      "Scan Inference Time (secs):  10.722953081130981\n",
      "\n",
      "Scan Inference Time (secs):  13.209000587463379\n",
      "\n",
      "Scan Inference Time (secs):  12.45699667930603\n",
      "\n",
      "Scan Inference Time (secs):  10.679000616073608\n",
      "\n",
      "Scan Inference Time (secs):  10.513004302978516\n",
      "\n",
      "Scan Inference Time (secs):  10.562000036239624\n",
      "\n",
      "Scan Inference Time (secs):  11.094999551773071\n",
      "\n",
      "Scan Inference Time (secs):  10.265998601913452\n",
      "\n",
      "Scan Inference Time (secs):  11.418033838272095\n",
      "\n",
      "Scan Inference Time (secs):  11.946959257125854\n",
      "\n",
      "Scan Inference Time (secs):  10.144964694976807\n",
      "\n",
      "Scan Inference Time (secs):  10.437999486923218\n",
      "\n",
      "Scan Inference Time (secs):  10.740999460220337\n",
      "\n",
      "Scan Inference Time (secs):  11.123001337051392\n",
      "\n",
      "[Run]: Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_3Dclustering_ICP(model_params_path, file_path, dir_path, eps, min_samples, inst_weight, icp_templates, voxel_size):\n",
    "    \n",
    "    ## [TODO] Check path exists\n",
    "    \n",
    "    ## [TODO] Delete Log file if exist\n",
    "    \n",
    "    ## Generate model, load parameters\n",
    "    model = configure_model(model_params_path, verbose=True)\n",
    "    \n",
    "    ## Parse the spec'd file for count and scans' paths\n",
    "    file_count, file_data = parse_scan_label_file(file_path)\n",
    "    scans = file_data[\"scans\"]\n",
    "    \n",
    "    ## Need to unpack the weld position label files to add to the csv\n",
    "    ## FOR NOW: Different CSV\n",
    "    labels = file_data[\"labels\"]\n",
    "    with open(dir_path+\"labels.csv\", 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        \n",
    "        for l_pth in labels:\n",
    "            print(l_pth)\n",
    "            with open(l_pth, 'r') as labelfile:\n",
    "                csvreader = csv.reader(labelfile)\n",
    "                for row in csvreader:\n",
    "                    csvwriter.writerow(row)\n",
    "            csvwriter.writerow([])\n",
    "\n",
    "    ## Generate sample tags for each scan\n",
    "    tags = gen_tags_list(file_count)\n",
    "\n",
    "    \n",
    "    with open(dir_path+\"results.csv\", 'w', newline='') as resultsfile:\n",
    "        resultswriter = csv.writer(resultsfile)\n",
    "        resultswriter.writerow([\"tag\", \"id\", \"type\", \\\n",
    "                                \"\", \"mean_x\", \"mean_y\", \"mean_z\", \"mean_theta\", \\\n",
    "                                \"\", \"median_x\", \"median_y\", \"median_z\", \"median_theta\", \\\n",
    "                                \"\", \"mid_x\", \"mid_y\", \"mid_z\", \"mid_theta\", \\\n",
    "                                \"\", \"RANSAC_fitness\", \"RANSAC_RMSE\", \"RANSAC_x\", \"RANSAC_y\", \"RANSAC_z\", \"RANSAC_theta\", \\\n",
    "                                \"\", \"FGR_fitness\", \"FGR_RMSE\", \"FGR_x\", \"FGR_y\", \"FGR_z\", \"FGR_theta\", \\\n",
    "                                \"\", \"p2p_fitness_mean\", \"p2p_rsme_mean\", \"p2p_x_mean\", \"p2p_y_mean\", \"p2p_z_mean\", \"p2p_theta_mean\", \\\n",
    "                                \"\", \"p2p_fitness_median\", \"p2p_rsme_median\", \"p2p_x_median\", \"p2p_y_median\", \"p2p_z_median\", \"p2p_theta_median\", \\\n",
    "                                \"\", \"p2p_fitness_mid\", \"p2p_rsme_mid\", \"p2p_x_mid\", \"p2p_y_mid\", \"p2p_z_mid\", \"p2p_theta_mid\", \\\n",
    "                                \"\", \"p2p_fitness_RANSAC\", \"p2p_rsme_RANSAC\", \"p2p_x_RANSAC\", \"p2p_y_RANSAC\", \"p2p_z_RANSAC\", \"p2p_theta_RANSAC\", \\\n",
    "                                \"\", \"p2p_fitness_FGR\", \"p2p_rsme_FGR\", \"p2p_x_FGR\", \"p2p_y_FGR\", \"p2p_z_FGR\", \"p2p_theta_FGR\", \\\n",
    "                                \n",
    "                                \"\", \"p2pl_fitness_mean\", \"p2pl_rsme_mean\", \"p2pl_x_mean\", \"p2pl_y_mean\", \"p2pl_z_mean\", \"p2pl_theta_mean\", \\\n",
    "                                \"\", \"p2pl_fitness_median\", \"p2pl_rsme_median\", \"p2pl_x_median\", \"p2pl_y_median\", \"p2pl_z_median\", \"p2pl_theta_median\", \\\n",
    "                                \"\", \"p2pl_fitness_mid\", \"p2pl_rsme_mid\", \"p2pl_x_mid\", \"p2pl_y_mid\", \"p2pl_z_mid\", \"p2pl_theta_mid\", \\\n",
    "                                \"\", \"p2pl_fitness_RANSAC\", \"p2pl_rsme_RANSAC\", \"p2pl_x_RANSAC\", \"p2pl_y_RANSAC\", \"p2pl_z_RANSAC\", \"p2pl_theta_RANSAC\", \\\n",
    "                                \"\", \"p2pl_fitness_FGR\", \"p2pl_rsme_FGR\", \"p2pl_x_FGR\", \"p2pl_y_FGR\", \"p2pl_z_FGR\", \"p2pl_theta_FGR\" \\\n",
    "                               ])\n",
    "\n",
    "        ## Zip scans and tags and loop through each pair\n",
    "        for scan, tag in zip(scans, tags):\n",
    "\n",
    "            ## For plotting the initial and final weld locs for each point-to-point process\n",
    "            overlay_mean_pt2pt = {\"xs\": [], \"ys\": [], \"ss\": [], \"cs\": [], \"marker\": 's', \"label\":\"mean_pt2pt\" }\n",
    "            overlay_median_pt2pt = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'o', \"label\":\"median_pt2pt\" }\n",
    "            overlay_mid_pt2pt = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'P', \"label\":\"mid_pt2pt\" }\n",
    "            overlay_RANSAC_pt2pt = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'X', \"label\":\"RANSAC_pt2pt\" }\n",
    "            overlay_FGR_pt2pt = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'D', \"label\":\"FGR_pt2pt\" }\n",
    "\n",
    "            ## For plotting the initial and final weld locs for each point-to-plane process\n",
    "            overlay_mean_pt2pl = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 's', \"label\":\"mean_pt2pl\" }\n",
    "            overlay_median_pt2pl = {\"xs\": [], \"ys\": [], \"ss\": [], \"cs\": [], \"marker\": 'o', \"label\":\"median_pt2pl\" }\n",
    "            overlay_mid_pt2pl = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'P', \"label\":\"mid_pt2pl\" }\n",
    "            overlay_RANSAC_pt2pl = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'X', \"label\":\"RANSAC_pt2pl\" }\n",
    "            overlay_FGR_pt2pl = {\"xs\": [], \"ys\": [], \"ss\": [],  \"cs\": [], \"marker\": 'D', \"label\":\"FGR_pt2pl\" }\n",
    "\n",
    "            ## For aggregating all finals per icp method into single plot\n",
    "            overlay_all_pt2pt = []\n",
    "            overlay_all_pt2pl = []\n",
    "\n",
    "            ## Isolating and Looping through the detected welds\n",
    "\n",
    "            ### welds: a list of dicts representing each welds with its min/max/mean/median/modes/etc\n",
    "            ### scan_data --> removing too small and too large clusters --> x|y|z|cls_preds|inst_preds|clustering|cls_clean|inst_clean\n",
    "            welds, scan_data = get_weld_instances_from_3D_clustering(model, scan, eps, min_samples, inst_weight, tag=tag, dir_path=dir_path+\"clustering/\")\n",
    "#             welds, _ = get_weld_instances(model, scan, tag=tag, dir_path=dir_path+\"clustering/\")\n",
    "            for idx, weld in enumerate(welds):\n",
    "                \n",
    "#                 if idx == 0:\n",
    "#                     print(weld)\n",
    "                    \n",
    "                ## Building up the csv record\n",
    "                record = [weld[\"tag\"], weld[\"id\"], weld[\"type\"]]\n",
    "\n",
    "                ## 3. Get equivalent template for weld type\n",
    "                template = get_matching_template(weld, icp_templates)\n",
    "\n",
    "                ## 3.5 Compute FPFH Features --> needed for fast global registration\n",
    "                ## Added to weld dict\n",
    "                ## For weld\n",
    "                compute_FPFH_for_weld_points_2D(weld, voxel_size)\n",
    "                ## For Template\n",
    "                if \"down\" not in template.keys() or \"fpfh\" not in template.keys():\n",
    "                    template_down, template_fpfh = compute_FPFH(template[\"xyz\"], voxel_size)\n",
    "                    template[\"down\"], template[\"fpfh\"] = template_down, template_fpfh ## [possible RED FLAG] -- verify the templates dict is updated globally and not recomputed al the tiem\n",
    "                    print(\"Added downsampling and fpfh entries to template dict\")\n",
    "\n",
    "#                 if idx == 0:\n",
    "#                     print(weld)\n",
    "                    \n",
    "        ##### CONTINUE FROM HERE INCLUDING CHECKING THAT WE ARE WORKING IN THE XY[z=0] PROJECTION SPACE\n",
    "        #### OR ADD volume to the TEMPLATE [POSSIBLE RED FLAG]  template and scan dimensions should match ie either 3d or projection onto 2d\n",
    "                ## 4. Global Registration (Mean | Median | Box Centers | RANSAC Global Registration | Fast Global Registration)\n",
    "                T_RANSAC, fitness_RANSAC, inlier_rmse_RANSAC, _ = global_reg_by_RANSAC(template, weld, voxel_size)\n",
    "                T_FGR, fitness_FGR, inlier_rmse_FGR, _ = global_reg_by_FGR(template, weld, voxel_size)\n",
    "                T_mean, _, _, _ = global_reg_by_mean(template, weld)\n",
    "                T_median, _, _, _ = global_reg_by_median(template, weld)\n",
    "                T_mid, _, _, _ = global_reg_by_mid(template, weld)\n",
    "\n",
    "                record.extend([\"\", *transform_to_xyztheta(T_mean)])\n",
    "                record.extend([\"\", *transform_to_xyztheta(T_median)])\n",
    "                record.extend([\"\", *transform_to_xyztheta(T_mid)])\n",
    "                record.extend([\"\", fitness_RANSAC, inlier_rmse_RANSAC, *transform_to_xyztheta(T_RANSAC)])\n",
    "                record.extend([\"\", fitness_FGR, inlier_rmse_FGR, *transform_to_xyztheta(T_mean)])\n",
    "                \n",
    "\n",
    "                 ## 5. Local Registration: Refining\n",
    "\n",
    "                ## Represent as Point Cloud\n",
    "\n",
    "                weld_xyz = o3d.geometry.PointCloud()\n",
    "                weld_xyz.points = o3d.utility.Vector3dVector(weld[\"xyz\"])\n",
    "\n",
    "                template_xyz = o3d.geometry.PointCloud()\n",
    "                template_xyz.points = o3d.utility.Vector3dVector(template[\"xyz\"])\n",
    "\n",
    "                ## Point to Point ICP using T_RANSAC, T_FGR, T_mean, F_median, F_mid respectively as Initial Transforms \n",
    "\n",
    "                f_T_p2p_RANSAC, f_fitness_p2p_RANSAC, f_inlier_rmse_p2p_RANSAC, _ = point_to_point_ICP(\n",
    "                    template_xyz, weld_xyz, T_RANSAC, voxel_size)\n",
    "\n",
    "                f_T_p2p_FGR, f_fitness_p2p_FGR, f_inlier_rmse_p2p_FGR, _ = point_to_point_ICP(\n",
    "                    template_xyz, weld_xyz, T_FGR, voxel_size)\n",
    "\n",
    "                f_T_p2p_mean, f_fitness_p2p_mean, f_inlier_rmse_p2p_mean, _ = point_to_point_ICP(\n",
    "                    template_xyz, weld_xyz, T_mean, voxel_size)\n",
    "\n",
    "                f_T_p2p_median, f_fitness_p2p_median, f_inlier_rmse_p2p_median, _ = point_to_point_ICP(\n",
    "                    template_xyz, weld_xyz, T_median, voxel_size)\n",
    "\n",
    "                f_T_p2p_mid, f_fitness_p2p_mid, f_inlier_rmse_p2p_mid, _ = point_to_point_ICP(\n",
    "                    template_xyz, weld_xyz, T_mid, voxel_size)\n",
    "\n",
    "                record.extend([\"\", f_fitness_p2p_mean, f_inlier_rmse_p2p_mean,  *transform_to_xyztheta(f_T_p2p_mean)])\n",
    "                record.extend([\"\", f_fitness_p2p_median, f_inlier_rmse_p2p_median,  *transform_to_xyztheta(f_T_p2p_median)])\n",
    "                record.extend([\"\", f_fitness_p2p_mid, f_inlier_rmse_p2p_mid,  *transform_to_xyztheta(f_T_p2p_mid)])\n",
    "                record.extend([\"\", f_fitness_p2p_RANSAC, f_inlier_rmse_p2p_RANSAC,  *transform_to_xyztheta(f_T_p2p_RANSAC)])\n",
    "                record.extend([\"\", f_fitness_p2p_FGR, f_inlier_rmse_p2p_FGR,  *transform_to_xyztheta(f_T_p2p_FGR)])\n",
    "\n",
    "                ## storing the initial and final weld locs for each point-to-point process for plotting\n",
    "                ## initials\n",
    "                overlay_mean_pt2pt[\"xs\"].append(transform_to_xyztheta(T_mean)[0])\n",
    "                overlay_mean_pt2pt[\"ys\"].append(transform_to_xyztheta(T_mean)[1])\n",
    "                overlay_mean_pt2pt[\"cs\"].append('grey')\n",
    "                overlay_mean_pt2pt[\"ss\"].append(20)\n",
    "\n",
    "                overlay_median_pt2pt[\"xs\"].append(transform_to_xyztheta(T_median)[0])\n",
    "                overlay_median_pt2pt[\"ys\"].append(transform_to_xyztheta(T_median)[1])\n",
    "                overlay_median_pt2pt[\"cs\"].append('grey')\n",
    "                overlay_median_pt2pt[\"ss\"].append(20)\n",
    "\n",
    "                overlay_mid_pt2pt[\"xs\"].append(transform_to_xyztheta(T_mid)[0])\n",
    "                overlay_mid_pt2pt[\"ys\"].append(transform_to_xyztheta(T_mid)[1])\n",
    "                overlay_mid_pt2pt[\"cs\"].append('grey')\n",
    "                overlay_mid_pt2pt[\"ss\"].append(20)\n",
    "\n",
    "                overlay_RANSAC_pt2pt[\"xs\"].append(transform_to_xyztheta(T_RANSAC)[0])\n",
    "                overlay_RANSAC_pt2pt[\"ys\"].append(transform_to_xyztheta(T_RANSAC)[1])\n",
    "                overlay_RANSAC_pt2pt[\"cs\"].append('grey')\n",
    "                overlay_RANSAC_pt2pt[\"ss\"].append(20)\n",
    "\n",
    "                overlay_FGR_pt2pt[\"xs\"].append(transform_to_xyztheta(T_FGR)[0])\n",
    "                overlay_FGR_pt2pt[\"ys\"].append(transform_to_xyztheta(T_FGR)[1])\n",
    "                overlay_FGR_pt2pt[\"cs\"].append('grey')\n",
    "                overlay_FGR_pt2pt[\"ss\"].append(20)\n",
    "\n",
    "                ## finals\n",
    "                overlay_mean_pt2pt[\"xs\"].append(transform_to_xyztheta(f_T_p2p_mean)[0])\n",
    "                overlay_mean_pt2pt[\"ys\"].append(transform_to_xyztheta(f_T_p2p_mean)[1])\n",
    "                overlay_mean_pt2pt[\"cs\"].append('black')\n",
    "                overlay_mean_pt2pt[\"ss\"].append(2)\n",
    "\n",
    "                overlay_median_pt2pt[\"xs\"].append(transform_to_xyztheta(f_T_p2p_median)[0])\n",
    "                overlay_median_pt2pt[\"ys\"].append(transform_to_xyztheta(f_T_p2p_median)[1])\n",
    "                overlay_median_pt2pt[\"cs\"].append('black')\n",
    "                overlay_median_pt2pt[\"ss\"].append(2)\n",
    "\n",
    "                overlay_mid_pt2pt[\"xs\"].append(transform_to_xyztheta(f_T_p2p_mid)[0])\n",
    "                overlay_mid_pt2pt[\"ys\"].append(transform_to_xyztheta(f_T_p2p_mid)[1])\n",
    "                overlay_mid_pt2pt[\"cs\"].append('black')\n",
    "                overlay_mid_pt2pt[\"ss\"].append(2)\n",
    "\n",
    "                overlay_RANSAC_pt2pt[\"xs\"].append(transform_to_xyztheta(f_T_p2p_RANSAC)[0])\n",
    "                overlay_RANSAC_pt2pt[\"ys\"].append(transform_to_xyztheta(f_T_p2p_RANSAC)[1])\n",
    "                overlay_RANSAC_pt2pt[\"cs\"].append('black')\n",
    "                overlay_RANSAC_pt2pt[\"ss\"].append(2)\n",
    "\n",
    "                overlay_FGR_pt2pt[\"xs\"].append(transform_to_xyztheta(f_T_p2p_FGR)[0])\n",
    "                overlay_FGR_pt2pt[\"ys\"].append(transform_to_xyztheta(f_T_p2p_FGR)[1])\n",
    "                overlay_FGR_pt2pt[\"cs\"].append('black')\n",
    "                overlay_FGR_pt2pt[\"ss\"].append(2)\n",
    "\n",
    "\n",
    "                 ## Point to Point ICP using T_RANSAC, T_FGR, T_mean, F_median, F_mid respectively as Initial Transforms \n",
    "\n",
    "                f_T_p2pl_RANSAC, f_fitness_p2pl_RANSAC, f_inlier_rmse_p2pl_RANSAC, _ = point_to_plane_ICP(\n",
    "                    template_xyz, weld_xyz, T_RANSAC, voxel_size)\n",
    "\n",
    "                f_T_p2pl_FGR, f_fitness_p2pl_FGR, f_inlier_rmse_p2pl_FGR, _ = point_to_plane_ICP(\n",
    "                    template_xyz, weld_xyz, T_FGR, voxel_size)\n",
    "\n",
    "                f_T_p2pl_mean, f_fitness_p2pl_mean, f_inlier_rmse_p2pl_mean, _ = point_to_plane_ICP(\n",
    "                    template_xyz, weld_xyz, T_mean, voxel_size)\n",
    "\n",
    "                f_T_p2pl_median, f_fitness_p2pl_median, f_inlier_rmse_p2pl_median, _ = point_to_plane_ICP(\n",
    "                    template_xyz, weld_xyz, T_median, voxel_size)\n",
    "\n",
    "                f_T_p2pl_mid, f_fitness_p2pl_mid, f_inlier_rmse_p2pl_mid, _ = point_to_plane_ICP(\n",
    "                    template_xyz, weld_xyz, T_mid, voxel_size)\n",
    "\n",
    "                record.extend([\"\", f_fitness_p2pl_mean, f_inlier_rmse_p2pl_mean,  *transform_to_xyztheta(f_T_p2pl_mean)])\n",
    "                record.extend([\"\", f_fitness_p2pl_median, f_inlier_rmse_p2pl_median,  *transform_to_xyztheta(f_T_p2pl_median)])\n",
    "                record.extend([\"\", f_fitness_p2pl_mid, f_inlier_rmse_p2pl_mid,  *transform_to_xyztheta(f_T_p2pl_mid)])\n",
    "                record.extend([\"\", f_fitness_p2pl_RANSAC, f_inlier_rmse_p2pl_RANSAC,  *transform_to_xyztheta(f_T_p2pl_RANSAC)])\n",
    "                record.extend([\"\", f_fitness_p2pl_FGR, f_inlier_rmse_p2pl_FGR,  *transform_to_xyztheta(f_T_p2pl_FGR)])\n",
    "                resultswriter.writerow(record)\n",
    "\n",
    "                # storing the initial and final weld locs for each point-to-plane process for plotting\n",
    "                ## initials\n",
    "                overlay_mean_pt2pl[\"xs\"].append(transform_to_xyztheta(T_mean)[0])\n",
    "                overlay_mean_pt2pl[\"ys\"].append(transform_to_xyztheta(T_mean)[1])\n",
    "                overlay_mean_pt2pl[\"cs\"].append('orange')\n",
    "                overlay_mean_pt2pl[\"ss\"].append(20)\n",
    "\n",
    "                overlay_median_pt2pl[\"xs\"].append(transform_to_xyztheta(T_median)[0])\n",
    "                overlay_median_pt2pl[\"ys\"].append(transform_to_xyztheta(T_median)[1])\n",
    "                overlay_median_pt2pl[\"cs\"].append('orange')\n",
    "                overlay_median_pt2pl[\"ss\"].append(20)\n",
    "\n",
    "                overlay_mid_pt2pl[\"xs\"].append(transform_to_xyztheta(T_mid)[0])\n",
    "                overlay_mid_pt2pl[\"ys\"].append(transform_to_xyztheta(T_mid)[1])\n",
    "                overlay_mid_pt2pl[\"cs\"].append('orange')\n",
    "                overlay_mid_pt2pl[\"ss\"].append(20)\n",
    "\n",
    "                overlay_RANSAC_pt2pl[\"xs\"].append(transform_to_xyztheta(T_RANSAC)[0])\n",
    "                overlay_RANSAC_pt2pl[\"ys\"].append(transform_to_xyztheta(T_RANSAC)[1])\n",
    "                overlay_RANSAC_pt2pl[\"cs\"].append('orange')\n",
    "                overlay_RANSAC_pt2pl[\"ss\"].append(20)\n",
    "\n",
    "                overlay_FGR_pt2pl[\"xs\"].append(transform_to_xyztheta(T_FGR)[0])\n",
    "                overlay_FGR_pt2pl[\"ys\"].append(transform_to_xyztheta(T_FGR)[1])\n",
    "                overlay_FGR_pt2pl[\"cs\"].append('orange')\n",
    "                overlay_FGR_pt2pl[\"ss\"].append(20)\n",
    "\n",
    "                ## finals\n",
    "                overlay_mean_pt2pl[\"xs\"].append(transform_to_xyztheta(f_T_p2pl_mean)[0])\n",
    "                overlay_mean_pt2pl[\"ys\"].append(transform_to_xyztheta(f_T_p2pl_mean)[1])\n",
    "                overlay_mean_pt2pl[\"cs\"].append('red')\n",
    "                overlay_mean_pt2pl[\"ss\"].append(2)\n",
    "\n",
    "                overlay_median_pt2pl[\"xs\"].append(transform_to_xyztheta(f_T_p2pl_median)[0])\n",
    "                overlay_median_pt2pl[\"ys\"].append(transform_to_xyztheta(f_T_p2pl_median)[1])\n",
    "                overlay_median_pt2pl[\"cs\"].append('red')\n",
    "                overlay_median_pt2pl[\"ss\"].append(2)\n",
    "\n",
    "                overlay_mid_pt2pl[\"xs\"].append(transform_to_xyztheta(f_T_p2pl_mid)[0])\n",
    "                overlay_mid_pt2pl[\"ys\"].append(transform_to_xyztheta(f_T_p2pl_mid)[1])\n",
    "                overlay_mid_pt2pl[\"cs\"].append('red')\n",
    "                overlay_mid_pt2pl[\"ss\"].append(2)\n",
    "\n",
    "                overlay_RANSAC_pt2pl[\"xs\"].append(transform_to_xyztheta(f_T_p2pl_RANSAC)[0])\n",
    "                overlay_RANSAC_pt2pl[\"ys\"].append(transform_to_xyztheta(f_T_p2pl_RANSAC)[1])\n",
    "                overlay_RANSAC_pt2pl[\"cs\"].append('red')\n",
    "                overlay_RANSAC_pt2pl[\"ss\"].append(2)\n",
    "\n",
    "                overlay_FGR_pt2pl[\"xs\"].append(transform_to_xyztheta(f_T_p2pl_FGR)[0])\n",
    "                overlay_FGR_pt2pl[\"ys\"].append(transform_to_xyztheta(f_T_p2pl_FGR)[1])\n",
    "                overlay_FGR_pt2pl[\"cs\"].append('red')\n",
    "                overlay_FGR_pt2pl[\"ss\"].append(2)\n",
    "\n",
    "            resultswriter.writerow([])\n",
    "\n",
    "            ## Plotting for point-to-point ICP\n",
    "            overlay_mean_pt2pt = [overlay_mean_pt2pt]\n",
    "            _plot(dir_path+\"pt2pt/mean_init/\"+tag+\"_from_T_mean.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pt_from_T_mean\", overlay_points=overlay_mean_pt2pt)\n",
    "\n",
    "            overlay_median_pt2pt = [overlay_median_pt2pt]\n",
    "            _plot(dir_path+\"pt2pt/median_init/\"+tag+\"_from_T_median.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pt_from_T_median\", overlay_points=overlay_median_pt2pt)\n",
    "\n",
    "            overlay_mid_pt2pt = [overlay_mid_pt2pt]\n",
    "            _plot(dir_path+\"pt2pt/mid_init/\"+tag+\"_from_T_mid.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pt_from_T_mid\", overlay_points=overlay_mid_pt2pt)\n",
    "\n",
    "            overlay_RANSAC_pt2pt = [overlay_RANSAC_pt2pt]\n",
    "            _plot(dir_path+\"pt2pt/ransac_init/\"+tag+\"_from_T_RANSAC.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pt_from_T_RANSAC\", overlay_points=overlay_RANSAC_pt2pt)\n",
    "\n",
    "            overlay_FGR_pt2pt = [overlay_FGR_pt2pt]\n",
    "            _plot(dir_path+\"pt2pt/fgr_init/\"+tag+\"_from_T_FGR.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pt_from_T_FGR\", overlay_points=overlay_FGR_pt2pt)\n",
    "\n",
    "\n",
    "            ## Plotting for point-to-plane ICP\n",
    "            overlay_mean_pt2pl = [overlay_mean_pt2pl]\n",
    "            _plot(dir_path+\"pt2pl/mean_init/\"+tag+\"_from_T_mean.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pl_from_T_mean\", overlay_points=overlay_mean_pt2pl)\n",
    "\n",
    "            overlay_median_pt2pl = [overlay_median_pt2pl]\n",
    "            _plot(dir_path+\"pt2pl/median_init/\"+tag+\"_from_T_median.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pl_from_T_median\", overlay_points=overlay_median_pt2pl)\n",
    "\n",
    "            overlay_mid_pt2pl = [overlay_mid_pt2pl]\n",
    "            _plot(dir_path+\"pt2pl/mid_init/\"+tag+\"_from_T_mid.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pl_from_T_mid\", overlay_points=overlay_mid_pt2pl)\n",
    "\n",
    "            overlay_RANSAC_pt2pl = [overlay_RANSAC_pt2pl]\n",
    "            _plot(dir_path+\"pt2pl/ransac_init/\"+tag+\"_from_T_RANSAC.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pl_from_T_RANSAC\", overlay_points=overlay_RANSAC_pt2pl)\n",
    "\n",
    "            overlay_FGR_pt2pl = [overlay_FGR_pt2pl]\n",
    "            _plot(dir_path+\"pt2pl/fgr_init/\"+tag+\"_from_T_FGR.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pl_from_T_FGR\", overlay_points=overlay_FGR_pt2pl)\n",
    "\n",
    "\n",
    "\n",
    "            ## Plotting all finals per icp method into single plot\n",
    "            overlay_all_pt2pt = overlay_mean_pt2pt + overlay_median_pt2pt + overlay_mid_pt2pt + overlay_RANSAC_pt2pt + overlay_FGR_pt2pt\n",
    "            _plot(dir_path+\"pt2pt/\"+tag+\"_pt2pt.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pt_all\", overlay_points=overlay_all_pt2pt)\n",
    "\n",
    "            overlay_all_pt2pl = overlay_mean_pt2pl + overlay_median_pt2pl + overlay_mid_pt2pl + overlay_RANSAC_pt2pl + overlay_FGR_pt2pl\n",
    "            _plot(dir_path+\"pt2pl/\"+tag+\"_pt2pl.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_pt2pl_all\", overlay_points=overlay_all_pt2pl)\n",
    "\n",
    "\n",
    "            ## Plotting all finals per initial T into single plot\n",
    "            overlay_from_mean = overlay_mean_pt2pt + overlay_mean_pt2pl\n",
    "            _plot(dir_path+\"mean_init/\"+tag+\"_from_T_mean_pt2pt_and_ptpl.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_from_T_mean_pt2pt_and_pt2pl\", overlay_points=overlay_from_mean)\n",
    "\n",
    "            overlay_from_median = overlay_median_pt2pt + overlay_median_pt2pl\n",
    "            _plot(dir_path+\"median_init/\"+tag+\"_from_T_median_pt2pt_and_ptpl.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_from_T_median_pt2pt_and_pt2pl\", overlay_points=overlay_from_median)\n",
    "\n",
    "            overlay_from_mid = overlay_mid_pt2pt + overlay_mid_pt2pl\n",
    "            _plot(dir_path+\"mid_init/\"+tag+\"_from_T_mid_pt2pt_and_ptpl.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_from_T_mid_pt2pt_and_pt2pl\", overlay_points=overlay_from_mid)\n",
    "\n",
    "            overlay_from_RANSAC = overlay_RANSAC_pt2pt + overlay_RANSAC_pt2pl\n",
    "            _plot(dir_path+\"RANSAC_init/\"+tag+\"_from_T_RANSAC_pt2pt_and_ptpl.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_from_T_RANSAC_pt2pt_and_pt2pl\", overlay_points=overlay_from_RANSAC)\n",
    "\n",
    "            overlay_from_FGR = overlay_FGR_pt2pt + overlay_FGR_pt2pl\n",
    "            _plot(dir_path+\"FGR_init/\"+tag+\"_from_T_FGR_pt2pt_and_ptpl.png\", scan_data[:, :2], scan_data[:, 7], tag+\"_from_T_FGR_pt2pt_and_pt2pl\", overlay_points=overlay_from_FGR)\n",
    "\n",
    "#             print(\"overlay_mean_pt2pl\")        \n",
    "#             print(overlay_mean_pt2pl)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_median_pt2pl\")\n",
    "#             print(overlay_median_pt2pl)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_mid_pt2pl\")\n",
    "#             print(overlay_mid_pt2pl)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_RANSAC_pt2pl\")\n",
    "#             print(overlay_RANSAC_pt2pl)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_FGR_pt2pl\")\n",
    "#             print(overlay_FGR_pt2pl)\n",
    "#             print()\n",
    "            \n",
    "\n",
    "#             print(\"overlay_mean_pt2pt\")\n",
    "#             print(overlay_mean_pt2pt)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_median_pt2pt\")\n",
    "#             print(overlay_median_pt2pt)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_mid_pt2pt\")\n",
    "#             print(overlay_mid_pt2pt)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_RANSAC_pt2pt\")\n",
    "#             print(overlay_RANSAC_pt2pt)\n",
    "#             print()\n",
    "            \n",
    "#             print(\"overlay_FGR_pt2pt\")\n",
    "#             print(overlay_FGR_pt2pt)\n",
    "#             print()\n",
    "            \n",
    "        \n",
    "    print(\"[Run]: Done!\")\n",
    "    \n",
    "#     ## Plots for each scan\n",
    "# #     For each of the different icp with different initial Ts (show initial and finals)\n",
    "# #     Show each of the different icps show all 5 finals markered differently\n",
    "# #     For each of the starting points, show the two (for comparing how each icp reacts to same init)\n",
    "\n",
    "    ## CSV write\n",
    "# run_3Dclustering_ICP(model_params_path, file_path, dir_path, eps, min_samples, inst_weight, icp_templates, voxel_size)\n",
    "run_3Dclustering_ICP(\"./_model/train1-fix3/2023_06_22_17_01_13/params_epoch497_for_min_test_loss.pt\",\n",
    "#                     \"./testset_testing.txt\",\n",
    "#                     \"./_data/selection_testing.txt\",\n",
    "                    \"./_data/test.txt\",\n",
    "                    \"./function_check/\",\n",
    "                    2,\n",
    "                    20,\n",
    "                    1,\n",
    "                    templates_dict,\n",
    "                    0.2)\n",
    "\n",
    "## NEXT THINGS TO TRY\n",
    "# - volumetric templates\n",
    "# - different parameters on icps\n",
    "# - compare to labels and other analysis\n",
    "# - generate the output file for the inspection\n",
    "# - new UI to viz predictions\n",
    "# - paramteric template\n",
    "# - Is Theta in radians or degrees\n",
    "# No evidence of the registration working; might have to change the registration parameters and run exhaustive tests\n",
    "# - FGR might not be working on the 2D he\n",
    "# - SOME WELDS NOT DETECTED POST CLUSTERING: \n",
    "#### - L1,2\n",
    "#### - M3,4\n",
    "#### - U3,4\n",
    "#### - AB5,6\n",
    "#### - AC2,3\n",
    "#### - AD3,4\n",
    "#### - AK1,2\n",
    "#### - AN5,6\n",
    "#### - BB3,4\n",
    "#### - BH1,2\n",
    "\n",
    "#### - BI3,4\n",
    "#### - BK2\n",
    "#### - BN5,6\n",
    "#### - CA4,5,6\n",
    "#### - CB2\n",
    "#### - CG3,4\n",
    "#### - CX5,6\n",
    "#### - CZ3,4\n",
    "#### - DB3,4\n",
    "#### - DE4,5,6\n",
    "\n",
    "#### - DF3,4\n",
    "#### - DJ3,4\n",
    "#### - DK3,4\n",
    "#### - DL4,5,6\n",
    "#### - DM3,4\n",
    "#### - DP3,4\n",
    "#### - \n",
    "#### - \n",
    "#### - \n",
    "#### - C:\\Users\\KZTYLF\\Downloads\\PlantData\\PlantFloorpanScan_0419\\Pocket1_Right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd6ab33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'two', 'three')\n"
     ]
    }
   ],
   "source": [
    "a = (\"one\", \"two\", \"three\")\n",
    "b = (1, 2, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c4d512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('one', 'two', 'three', 1, 2, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6089739c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gen_weld_templates_by_params():\n",
    "    x_min, x_max = -4.0, 4.0\n",
    "    y_min, y_max = -12.0, 12.0\n",
    "    z = 0\n",
    "    resolution = 0.2\n",
    "    stem_x_min, stem_x_max = 1.8, 3.2\n",
    "    stem_y_min, stem_y_max = -8.0, 8.0\n",
    "    top_center_y = 7.5\n",
    "    bottom_center_y = -7.5\n",
    "    radius = 3.20\n",
    "    \n",
    "    def dist(pt1, pt2):\n",
    "        x1, y1 = pt1\n",
    "        x2, y2 = pt2\n",
    "        dx = x2-x1\n",
    "        dy = y2-y1\n",
    "        return np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "    points2 = []\n",
    "    \n",
    "    for x in np.arange(x_min, x_max, resolution):\n",
    "        for y in np.arange(y_min, y_max, resolution):\n",
    "            if dist((x,y), (0, top_center_y)) <= radius:\n",
    "                points2.append((x,y,z))\n",
    "            elif dist((x,y), (0, bottom_center_y)) <= radius:\n",
    "                points2.append((x,y,z))\n",
    "            elif stem_x_min<=x<=stem_x_max and stem_y_min<=y<=stem_y_max:\n",
    "                points2.append((x,y,z))\n",
    "\n",
    "    points1 = [(-x, y, z) for x, y, z in points2]\n",
    "    return {\n",
    "            \"[\": {\"xyz\": points1},\n",
    "            \"]\": {\"xyz\": points2}\n",
    "           }\n",
    "\n",
    "parametric_templates = gen_weld_templates_by_params()\n",
    "\n",
    "\n",
    "    \n",
    "def gen_volumetric_weld_templates_by_params():\n",
    "    x_min, x_max = -4.0, 4.0\n",
    "    y_min, y_max = -12.0, 12.0\n",
    "    z_min, z_max = -1, 1\n",
    "    resolution = 0.3\n",
    "    stem_x_min, stem_x_max = 1.8, 3.2\n",
    "    stem_y_min, stem_y_max = -8.0, 8.0\n",
    "    top_center_y = 7.5\n",
    "    bottom_center_y = -7.5\n",
    "    radius = 3.20\n",
    "    \n",
    "    def dist(pt1, pt2):\n",
    "        x1, y1 = pt1\n",
    "        x2, y2 = pt2\n",
    "        dx = x2-x1\n",
    "        dy = y2-y1\n",
    "        return np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "    points2 = []\n",
    "    \n",
    "    for z in np.arange(z_min, z_max, resolution):\n",
    "        for x in np.arange(x_min, x_max, resolution):\n",
    "            for y in np.arange(y_min, y_max, resolution):\n",
    "                if dist((x,y), (0, top_center_y)) <= radius:\n",
    "                    points2.append((x,y,z))\n",
    "                elif dist((x,y), (0, bottom_center_y)) <= radius:\n",
    "                    points2.append((x,y,z))\n",
    "                elif stem_x_min<=x<=stem_x_max and stem_y_min<=y<=stem_y_max:\n",
    "                    points2.append((x,y,z))\n",
    "\n",
    "    points1 = [(-x, y, z) for x, y, z in points2]\n",
    "    return {\n",
    "        \"[\": {\"xyz\": points1},\n",
    "        \"]\": {\"xyz\": points2}\n",
    "       }\n",
    "\n",
    "volumetric_templates = gen_volumetric_weld_templates_by_params()\n",
    "\n",
    "def plot_welds(templates, cls=\"[\"):\n",
    "    points = templates[cls]\n",
    "    dims = len(points[0])\n",
    "    xs = []\n",
    "    ys = []\n",
    "    if dims == 2:\n",
    "        for x, y in points:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    elif dims == 3:\n",
    "        for x, y, z in points:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    ax = plt.scatter(xs, ys, s=0.1)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "#     add_staple_patch(ax, 0, 0, 0, 2)\n",
    "\n",
    "# plot_welds(parametric_templates, \"]\")\n",
    "plot_welds(volumetric_templates, \"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "935c35ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['orange', 'red'], dtype='<U6'), array([0, 1, 0, 1, 0, 1, 0, 1], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def _plot_testing(overlay_points):\n",
    "#     \"\"\"\n",
    "#     path - output path\n",
    "#     points - matrix of x, y, etc cols\n",
    "#     values - assigned value per point ie predicted/truth cls/inst\n",
    "#     [optional] overlay_points = [{\"xs\":[], \"ys\":[], \"cs\":_, \"marker\":_, \"label\"=_}, ...]\n",
    "#     \"\"\"\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111)\n",
    "# #     ax.set_title(title)\n",
    "#     im = ax.scatter([-90, -80, -70, -60, -50, -40, -30, -20, -10, 0,10,20,30,40,50,60,70,80,90], [-90, -80, -70, -60, -50, -40, -30, -20, -10, 0,10,20,30,40,50,60,70,80,90],s=0.25,c='grey')\n",
    "    \n",
    "# #     _im = None\n",
    "#     if overlay_points:    \n",
    "#         for pts in overlay_points:\n",
    "# #             _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=2, c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "#             _im = ax.scatter(pts[\"xs\"], pts[\"ys\"], s=pts[\"ss\"], c=pts[\"cs\"], marker=pts[\"marker\"])\n",
    "#             print(np.unique(pts[\"cs\"], return_inverse=True))\n",
    "# #             print(pts[])\n",
    "# #             ims.append(_im)\n",
    "            \n",
    "        \n",
    "\n",
    "#     ax.set_xlabel(\"x [mm]\")\n",
    "#     ax.set_ylabel(\"y [mm]\")\n",
    "# #     leg_els = im.legend_elements()\n",
    "# #     if _im:\n",
    "# #         leg_els += _im.legend_elements()\n",
    "# #         print(\"leg_els\")\n",
    "# #         print(leg_els)\n",
    "# #         print(\"_im.legend_elements()\")\n",
    "# #         print(_im.legend_elements())\n",
    "# #         print(\"im.legend_elements()\")\n",
    "# #         print(im.legend_elements())\n",
    "        \n",
    "# #     legend_ = ax.legend(*im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "# #     legend_ = ax.legend(*leg_els, bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "# #     ax.add_artist(legend_)\n",
    "# #     for _im in ims:\n",
    "# #         _legend = ax.legend(*_im.legend_elements(), bbox_to_anchor=(1.1, 1), loc=\"upper right\")\n",
    "# #         ax.add_artist(_legend)\n",
    "\n",
    "# #     ax.text(0.5, -0.5, caption, style='italic', \\\n",
    "# #         horizontalalignment='center', verticalalignment='top', transform=ax.transAxes)\n",
    "#     axes=plt.gca()\n",
    "#     axes.set_aspect(1)\n",
    "# #     if path:\n",
    "# #         plt.savefig(path, dpi=150)\n",
    "# #         plt.close()\n",
    "# #         plt.cla()\n",
    "# #         plt.clf()\n",
    "# #     else:\n",
    "# #         plt.show\n",
    "#     plt.savefig(\"./function_check/test1.png\", dpi=150)\n",
    "#     plt.close()\n",
    "#     plt.cla()\n",
    "#     plt.clf()\n",
    "#     plt.show\n",
    "\n",
    "# overlay_mean_pt2pl = [{\\\n",
    "#   'xs': [-80.4080814534818, -80.46891292783077, 12.690050598167003, 12.675091368473577, 76.12924800589066, 76.05683683241976, 87.61278849506438, 87.6343603478098],\\\n",
    "#   'ys': [-1.4104994201207997, -1.4201994437698928, 0.5671794606719639, 0.5690492565029803, 1.0426517949247163, 1.0340038231892827, 1.6833320733939061, 1.675376535498944],\\\n",
    "#   'ss': [20, 2, 20, 2, 20, 2, 20, 2],\\\n",
    "#   'cs': ['orange', 'red', 'orange', 'red', 'orange', 'red', 'orange', 'red'],\\\n",
    "#   'marker': 's',\\\n",
    "#   'label': 'mean_pt2pl'}]\n",
    "\n",
    "# _plot_testing(overlay_mean_pt2pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fc305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
