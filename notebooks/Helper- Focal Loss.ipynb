{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a8db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes in a number of labels and predictions and computes the focal loss\n",
    "## Inputs: \n",
    "#  - labels as N x 1 integer categories\n",
    "#  - logits as N x num_of_classes represting class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914d997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1805bb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0746, 0.7369, 0.0055, 0.0845, 0.0985],\n",
       "        [0.0861, 0.1624, 0.0933, 0.3295, 0.3288],\n",
       "        [0.0468, 0.5452, 0.0685, 0.2918, 0.0477]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test Inputs\n",
    "\n",
    "Labels = torch.Tensor([1,0,4])\n",
    "Logits_pre_softmax  = torch.Tensor([[-0.0431,  2.2473, -2.6419,  0.0818,  0.2348], \n",
    "                                    [-0.7288, -0.0937, -0.6486,  0.6135,  0.6113], \n",
    "                                    [-1.3231,  1.1315, -0.9422,  0.5064, -1.3047]])\n",
    "Logits = torch.nn.functional.softmax(Logits_pre_softmax, dim=1)\n",
    "Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "affa64d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Version Result, res1:\n",
      "tensor([0.0211, 2.0485, 2.7595])\n"
     ]
    }
   ],
   "source": [
    "## Current Version of Function\n",
    "def focal_loss_softmax(labels, logits, alpha=0.5, gamma=2):\n",
    "    \"\"\"\n",
    "     github.com/tensorflow/models/blob/master/\\\n",
    "         research/object_detection/core/losses.py\n",
    "     Computer focal loss for binary classification\n",
    "     Args:\n",
    "       labels: A int32 tensor of shape [batch_size]. N x 1\n",
    "       logits: A float32 tensor of shape [batch_size]. N x C\n",
    "       alpha: A scalar for focal loss alpha hyper-parameter.\n",
    "       If positive samples number > negtive samples number,\n",
    "       alpha < 0.5 and vice versa.\n",
    "       gamma: A scalar for focal loss gamma hyper-parameter.\n",
    "     Returns:\n",
    "       A tensor of the same shape as `labels`\n",
    "     \"\"\"\n",
    "    # print(\"from focal_loss_softmax\")\n",
    "    # probs = softmax(logits, dim=1)\n",
    "    probs = logits\n",
    "    labels = torch.nn.functional.one_hot(labels.squeeze().long(), num_classes=probs.shape[1])\n",
    "    \n",
    "    print(f\"logits shape: {logits.shape}\")\n",
    "    print(\"logits(softmax):\")\n",
    "    print(logits)\n",
    "    print()\n",
    "    \n",
    "    print(f\"labels shape: {labels.shape}\")\n",
    "    print(\"labels:\")\n",
    "    print(labels)\n",
    "    print()\n",
    "\n",
    "    alphas = 1/labels.sum(dim=0)\n",
    "    print(f\"alphas_pre_correction: {alphas}\")\n",
    "#     alphas[torch.isinf(alphas)] = torch.sum(alphas[torch.logical_not(torch.isinf(alphas))])\n",
    "    alphas[torch.isinf(alphas)] = 0\n",
    "    print(f\"alphas shape: {alphas.shape}\")\n",
    "    print(f\"alphas_post_correction: {alphas}\")\n",
    "    print()\n",
    "\n",
    "    modulating = torch.pow(1-probs, gamma)\n",
    "    print(f\"modulation(gamma={gamma}):\")\n",
    "    print(modulating)\n",
    "    print()\n",
    "    \n",
    "    pt = torch.sum(probs * labels, dim=1)\n",
    "    focal_term = (1-pt)**gamma\n",
    "    print(\"pt:\")\n",
    "    print(pt)\n",
    "    print(\"focal_term:\")\n",
    "    print(focal_term)\n",
    "    print()\n",
    "    \n",
    "    neg_log = -torch.log(probs)  # == log_p\n",
    "    print(\"neg_log:\")\n",
    "    print(neg_log)\n",
    "    print(\"cross_entropy\")\n",
    "    print(neg_log*labels)\n",
    "    loss = torch.sum(neg_log*labels*alphas, dim=1)\n",
    "    print(\"loss\")\n",
    "    print(loss)\n",
    "    print()\n",
    "    \n",
    "    correction = focal_term*loss\n",
    "    print(\"correction:\")\n",
    "    print(correction)\n",
    "    print()\n",
    "    \n",
    "    focal_cross_entropy = alphas * modulating * neg_log\n",
    "    # print(f\"focal_cross_entropy.shape: {focal_cross_entropy.shape}\")\n",
    "    # print(f\"focal_cross_entropy: {focal_cross_entropy}\")\n",
    "    return focal_cross_entropy\n",
    "\n",
    "res1 = focal_loss_softmax(Labels, Logits, gamma=2)\n",
    "print(\"Current Version Result, res1:\")\n",
    "print(res1)\n",
    "tensor([0.0211, 2.0485, 2.7595])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37227990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[-2.5958, -0.3054, -5.1946, -2.4708, -2.3179],\n",
      "        [-2.4526, -1.8175, -2.3724, -1.1103, -1.1125],\n",
      "        [-3.0613, -0.6067, -2.6804, -1.2318, -3.0429]])\n",
      "B:\n",
      "tensor([[-2.5958, -0.3054, -5.1946, -2.4708, -2.3178],\n",
      "        [-2.4526, -1.8175, -2.3724, -1.1103, -1.1125],\n",
      "        [-3.0613, -0.6067, -2.6804, -1.2318, -3.0429]])\n"
     ]
    }
   ],
   "source": [
    "## Quick Check on if softmax and torch.log gives same as log_softmax\n",
    "A = torch.log(torch.nn.functional.softmax(Logits_pre_softmax, dim=1))\n",
    "B = torch.nn.functional.log_softmax(Logits_pre_softmax, dim=1)\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "print(\"B:\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9e42b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Running the stock focal_loss\n",
    "from typing import Optional, Sequence\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v!r}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        \n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        print(\"log_p\")\n",
    "        print(log_p)\n",
    "        print(\"y\")\n",
    "        print(y)\n",
    "        y = y.type(torch.LongTensor)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cpu',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36d188ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "tensor([[0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "\n",
      "alphas\n",
      "tensor([1., 1., 3., 3., 1.])\n",
      "\n",
      "log_p\n",
      "tensor([[-2.5958, -0.3054, -5.1946, -2.4708, -2.3178],\n",
      "        [-2.4526, -1.8175, -2.3724, -1.1103, -1.1125],\n",
      "        [-3.0613, -0.6067, -2.6804, -1.2318, -3.0429]])\n",
      "y\n",
      "tensor([1., 0., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0211, 2.0485, 2.7595])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.nn.functional.one_hot(Labels.squeeze().long(), num_classes=Logits.shape[1])\n",
    "print(\"labels\")\n",
    "print(labels)\n",
    "print()\n",
    "\n",
    "alphas = 1/labels.sum(dim=0)\n",
    "alphas[torch.isinf(alphas)] = torch.sum(alphas[torch.logical_not(torch.isinf(alphas))])\n",
    "print(\"alphas\")\n",
    "print(alphas)\n",
    "print()\n",
    "\n",
    "FL = focal_loss(alpha=alphas, gamma=2, reduction='none')\n",
    "FL(Logits_pre_softmax, Labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ccd0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nll_loss = torch.nn.NLLLoss(\n",
    "#             weight=alphas, reduction='none')\n",
    "test_nll_loss = torch.nn.NLLLoss(\n",
    "            reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b384e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3054, 2.4526, 3.0429])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = torch.Tensor([[-2.5958, -0.3054, -5.1946, -2.4708, -2.3178],\n",
    "        [-2.4526, -1.8175, -2.3724, -1.1103, -1.1125],\n",
    "        [-3.0613, -0.6067, -2.6804, -1.2318, -3.0429]])\n",
    "by = torch.Tensor([1, 0, 4])\n",
    "by = by.type(torch.LongTensor)\n",
    "test_nll_loss(ax, by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b55a85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Ru's version\n",
    "def focal_loss_sigmoid(labels, logits, alpha=0.5, gamma=2):\n",
    "    \"\"\"\n",
    "     github.com/tensorflow/models/blob/master/\\\n",
    "         research/object_detection/core/losses.py\n",
    "     Computer focal loss for binary classification\n",
    "     Args:\n",
    "       labels: A int32 tensor of shape [batch_size]. N x 1\n",
    "       logits: A float32 tensor of shape [batch_size]. N x C\n",
    "       alpha: A scalar for focal loss alpha hyper-parameter.\n",
    "       If positive samples number > negtive samples number,\n",
    "       alpha < 0.5 and vice versa.\n",
    "       gamma: A scalar for focal loss gamma hyper-parameter.\n",
    "     Returns:\n",
    "       A tensor of the same shape as `labels`\n",
    "     \"\"\"\n",
    "     \n",
    "    prob = logits.sigmoid()    #### [CORRECTION] Should be softmax since multi-class classification; assuming the focal cross works this way\n",
    "    print(\"prob(sigmoids):\")\n",
    "    print(prob)\n",
    "    print()\n",
    "    \n",
    "    labels = torch.nn.functional.one_hot(labels.squeeze().long(), num_classes=prob.shape[1])\n",
    "\n",
    "    cross_ent = torch.clamp(logits, min=0) - logits * labels + torch.log(1+torch.exp(-torch.abs(logits)))\n",
    "    print(\"cross_ent:\")\n",
    "    print(cross_ent)\n",
    "    print()\n",
    "    \n",
    "    prob_t = (labels*prob) + (1-labels) * (1-prob)\n",
    "    print(\"prob_t:\")\n",
    "    print(prob_t)\n",
    "    print()\n",
    "    \n",
    "    modulating = torch.pow(1-prob_t, gamma)\n",
    "    print(\"modulating:\")\n",
    "    print(modulating)\n",
    "    print()\n",
    "    \n",
    "    alpha_weight = (labels*alpha)+(1-labels)*(1-alpha)\n",
    "    print(\"alpha_weight:\")\n",
    "    print(alpha_weight)\n",
    "    print()\n",
    "\n",
    "    focal_cross_entropy = modulating * alpha_weight * cross_ent\n",
    "    print(\"focal_cross_entropy:\")\n",
    "    print(focal_cross_entropy)\n",
    "    print()\n",
    "\n",
    "    return focal_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d56f65dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob(sigmoids):\n",
      "tensor([[0.4892, 0.9044, 0.0665, 0.5204, 0.5584],\n",
      "        [0.3255, 0.4766, 0.3433, 0.6487, 0.6482],\n",
      "        [0.2103, 0.7561, 0.2805, 0.6240, 0.2134]])\n",
      "\n",
      "cross_ent:\n",
      "tensor([[0.6718, 0.1005, 0.0688, 0.7349, 0.8174],\n",
      "        [1.1225, 0.6474, 0.4205, 1.0462, 1.0448],\n",
      "        [0.2361, 1.4111, 0.3291, 0.9781, 1.5447]])\n",
      "\n",
      "prob_t:\n",
      "tensor([[0.5108, 0.9044, 0.9335, 0.4796, 0.4416],\n",
      "        [0.3255, 0.5234, 0.6567, 0.3513, 0.3518],\n",
      "        [0.7897, 0.2439, 0.7195, 0.3760, 0.2134]])\n",
      "\n",
      "modulating:\n",
      "tensor([[0.2393, 0.0091, 0.0044, 0.2709, 0.3118],\n",
      "        [0.4550, 0.2271, 0.1179, 0.4209, 0.4202],\n",
      "        [0.0442, 0.5717, 0.0787, 0.3893, 0.6188]])\n",
      "\n",
      "alpha_weight:\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])\n",
      "\n",
      "focal_cross_entropy:\n",
      "tensor([[8.0399e-02, 4.5892e-04, 1.5209e-04, 9.9524e-02, 1.2746e-01],\n",
      "        [2.5538e-01, 7.3525e-02, 2.4782e-02, 2.2016e-01, 2.1952e-01],\n",
      "        [5.2212e-03, 4.0336e-01, 1.2944e-02, 1.9039e-01, 4.7791e-01]])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8.0399e-02, 4.5892e-04, 1.5209e-04, 9.9524e-02, 1.2746e-01],\n",
       "        [2.5538e-01, 7.3525e-02, 2.4782e-02, 2.2016e-01, 2.1952e-01],\n",
       "        [5.2212e-03, 4.0336e-01, 1.2944e-02, 1.9039e-01, 4.7791e-01]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss_sigmoid(Labels, Logits_pre_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef37c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
